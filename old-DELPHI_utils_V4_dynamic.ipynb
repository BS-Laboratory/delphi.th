{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Hamza Tazi Bouardi (htazi@mit.edu), Michael L. Li (mlli@mit.edu), Omar Skali Lami (oskali@mit.edu)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "from copy import deepcopy\n",
    "from itertools import compress\n",
    "import import_ipynb\n",
    "#from DELPHI_params_V4 import MAPPING_STATE_CODE_TO_STATE_NAME, future_policies\n",
    "from matplotlib import pyplot as plt\n",
    "from logging import Logger\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_increasing(sequence: list) -> list:\n",
    "    \"\"\"\n",
    "    Used to force the Confidence Intervals generated for DELPHI to be always increasing\n",
    "    :param sequence: list, sequence of values\n",
    "    :return: list, forcefully increasing sequence of values\n",
    "    \"\"\"\n",
    "    for i in range(len(sequence)):\n",
    "        sequence[i] = max(sequence[i], sequence[max(i-1, 0)])\n",
    "    return sequence\n",
    "\n",
    "def get_bounds_params_from_pastparams(\n",
    "        optimizer: str, parameter_list: list, dict_default_reinit_parameters: dict, percentage_drift_lower_bound: float,\n",
    "        default_lower_bound: float, dict_default_reinit_lower_bounds: dict, percentage_drift_upper_bound: float,\n",
    "        default_upper_bound: float, dict_default_reinit_upper_bounds: dict,\n",
    "        percentage_drift_lower_bound_annealing: float, default_lower_bound_annealing: float,\n",
    "        percentage_drift_upper_bound_annealing: float, default_upper_bound_annealing: float,\n",
    "        default_lower_bound_t_jump: float, default_upper_bound_t_jump: float, default_lower_bound_std_normal: float,\n",
    "        default_upper_bound_std_normal: float,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates the lower and upper bounds of the past parameters used as warm starts for the optimization process\n",
    "    to predict with DELPHI: the output depends on the optimizer used (annealing or other, i.e. tnc or trust-constr)\n",
    "    :param optimizer: optimizer used to obtain the DELPHI predictions\n",
    "    :param parameter_list: list of all past parameter values for which we want to create bounds\n",
    "    :param dict_default_reinit_parameters: dictionary with default values in case of reinitialization of parameters\n",
    "    :param percentage_drift_lower_bound: percentage of drift allowed for the lower bound\n",
    "    :param default_lower_bound: default lower bound value\n",
    "    :param dict_default_reinit_lower_bounds: dictionary with lower bounds in case of reinitialization of parameters\n",
    "    :param percentage_drift_upper_bound: percentage of drift allowed for the upper bound\n",
    "    :param default_upper_bound: default upper bound value\n",
    "    :param dict_default_reinit_upper_bounds: dictionary with upper bounds in case of reinitialization of parameters\n",
    "    :param percentage_drift_lower_bound_annealing: percentage of drift allowed for the lower bound under annealing\n",
    "    :param default_lower_bound_annealing: default lower bound value under annealing\n",
    "    :param percentage_drift_upper_bound_annealing: percentage of drift allowed for the upper bound under annealing\n",
    "    :param default_upper_bound_annealing: default upper bound value under annealing\n",
    "    :param default_lower_bound_jump: default lower bound value for the jump parameter\n",
    "    :param default_upper_bound_jump: default upper bound value for the jump parameter\n",
    "    :param default_lower_bound_std_normal: default lower bound value for the normal standard deviation parameter\n",
    "    :param default_upper_bound_std_normal: default upper bound value for the normal standard deviation parameter\n",
    "    :return: a list of bounds for all the optimized parameters based on the optimizer and pre-fixed parameters\n",
    "    \"\"\"\n",
    "    if optimizer in [\"tnc\", \"trust-constr\"]:\n",
    "        # Allowing a drift for parameters\n",
    "        alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = parameter_list\n",
    "        parameter_list = [\n",
    "            max(alpha, dict_default_reinit_parameters[\"alpha\"]),\n",
    "            days,\n",
    "            max(r_s, dict_default_reinit_parameters[\"r_s\"]),\n",
    "            max(min(r_dth, 1), dict_default_reinit_parameters[\"r_dth\"]),\n",
    "            max(min(p_dth, 1), dict_default_reinit_parameters[\"p_dth\"]),\n",
    "            max(r_dthdecay, dict_default_reinit_parameters[\"r_dthdecay\"]),\n",
    "            max(k1, dict_default_reinit_parameters[\"k1\"]),\n",
    "            max(k2, dict_default_reinit_parameters[\"k2\"]),\n",
    "            max(jump, dict_default_reinit_parameters[\"jump\"]),\n",
    "            max(t_jump, dict_default_reinit_parameters[\"t_jump\"]),\n",
    "            max(std_normal, dict_default_reinit_parameters[\"std_normal\"]),\n",
    "            max(k3, dict_default_reinit_parameters[\"k3\"]),\n",
    "        ]\n",
    "        param_list_lower = [x - max(percentage_drift_lower_bound * abs(x), default_lower_bound) for x in parameter_list]\n",
    "        (\n",
    "            alpha_lower, days_lower, r_s_lower, r_dth_lower, p_dth_lower, r_dthdecay_lower,\n",
    "            k1_lower, k2_lower, jump_lower, t_jump_lower, std_normal_lower, k3_lower\n",
    "        ) = param_list_lower\n",
    "        param_list_lower = [\n",
    "            max(alpha_lower, dict_default_reinit_lower_bounds[\"alpha\"]),\n",
    "            days_lower,\n",
    "            max(r_s_lower, dict_default_reinit_lower_bounds[\"r_s\"]),\n",
    "            max(min(r_dth_lower, 1), dict_default_reinit_lower_bounds[\"r_dth\"]),\n",
    "            max(min(p_dth_lower, 1), dict_default_reinit_lower_bounds[\"p_dth\"]),\n",
    "            max(r_dthdecay_lower, dict_default_reinit_lower_bounds[\"r_dthdecay\"]),\n",
    "            max(k1_lower, dict_default_reinit_lower_bounds[\"k1\"]),\n",
    "            max(k2_lower, dict_default_reinit_lower_bounds[\"k2\"]),\n",
    "            max(jump_lower, dict_default_reinit_lower_bounds[\"jump\"]),\n",
    "            max(t_jump_lower, dict_default_reinit_lower_bounds[\"t_jump\"]),\n",
    "            max(std_normal_lower, dict_default_reinit_lower_bounds[\"std_normal\"]),\n",
    "            max(k3_lower, dict_default_reinit_lower_bounds[\"k3\"]),\n",
    "        ]\n",
    "        param_list_upper = [\n",
    "            x + max(percentage_drift_upper_bound * abs(x), default_upper_bound) for x in parameter_list\n",
    "        ]\n",
    "        (\n",
    "            alpha_upper, days_upper, r_s_upper, r_dth_upper, p_dth_upper, r_dthdecay_upper,\n",
    "            k1_upper, k2_upper, jump_upper, t_jump_upper, std_normal_upper, k3_upper\n",
    "        ) = param_list_upper\n",
    "        param_list_upper = [\n",
    "            max(alpha_upper, dict_default_reinit_upper_bounds[\"alpha\"]),\n",
    "            days_upper,\n",
    "            max(r_s_upper, dict_default_reinit_upper_bounds[\"r_s\"]),\n",
    "            max(min(r_dth_upper, 1), dict_default_reinit_upper_bounds[\"r_dth\"]),\n",
    "            max(min(p_dth_upper, 1), dict_default_reinit_upper_bounds[\"p_dth\"]),\n",
    "            max(r_dthdecay_upper, dict_default_reinit_upper_bounds[\"r_dthdecay\"]),\n",
    "            max(k1_upper, dict_default_reinit_upper_bounds[\"k1\"]),\n",
    "            max(k2_upper, dict_default_reinit_upper_bounds[\"k2\"]),\n",
    "            max(jump_upper, dict_default_reinit_upper_bounds[\"jump\"]),\n",
    "            max(t_jump_upper, dict_default_reinit_upper_bounds[\"t_jump\"]),\n",
    "            max(std_normal_upper, dict_default_reinit_upper_bounds[\"std_normal\"]),\n",
    "            max(k3_upper, dict_default_reinit_upper_bounds[\"k3\"]),\n",
    "        ]\n",
    "    elif optimizer == \"annealing\":  # Annealing procedure for global optimization\n",
    "        alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = parameter_list\n",
    "        parameter_list = [\n",
    "            max(alpha, dict_default_reinit_parameters[\"alpha\"]),\n",
    "            days,\n",
    "            max(r_s, dict_default_reinit_parameters[\"r_s\"]),\n",
    "            max(min(r_dth, 1), dict_default_reinit_parameters[\"r_dth\"]),\n",
    "            max(min(p_dth, 1), dict_default_reinit_parameters[\"p_dth\"]),\n",
    "            max(r_dthdecay, dict_default_reinit_parameters[\"r_dthdecay\"]),\n",
    "            max(k1, dict_default_reinit_parameters[\"k1\"]),\n",
    "            max(k2, dict_default_reinit_parameters[\"k2\"]),\n",
    "            max(jump, dict_default_reinit_parameters[\"jump\"]),\n",
    "            max(t_jump, dict_default_reinit_parameters[\"t_jump\"]),\n",
    "            max(std_normal, dict_default_reinit_parameters[\"std_normal\"]),\n",
    "            max(k3, dict_default_reinit_parameters[\"k3\"]),\n",
    "        ]\n",
    "        param_list_lower = [\n",
    "            x - max(percentage_drift_lower_bound_annealing * abs(x), default_lower_bound_annealing) for x in\n",
    "            parameter_list\n",
    "        ]\n",
    "        (\n",
    "            alpha_lower, days_lower, r_s_lower, r_dth_lower, p_dth_lower, r_dthdecay_lower,\n",
    "            k1_lower, k2_lower, jump_lower, t_jump_lower, std_normal_lower, k3_lower\n",
    "        ) = param_list_lower\n",
    "        param_list_lower = [\n",
    "            max(alpha_lower, dict_default_reinit_lower_bounds[\"alpha\"]),\n",
    "            days_lower,\n",
    "            max(r_s_lower, dict_default_reinit_lower_bounds[\"r_s\"]),\n",
    "            max(min(r_dth_lower, 1), dict_default_reinit_lower_bounds[\"r_dth\"]),\n",
    "            max(min(p_dth_lower, 1), dict_default_reinit_lower_bounds[\"p_dth\"]),\n",
    "            max(r_dthdecay_lower, dict_default_reinit_lower_bounds[\"r_dthdecay\"]),\n",
    "            max(k1_lower, dict_default_reinit_lower_bounds[\"k1\"]),\n",
    "            max(k2_lower, dict_default_reinit_lower_bounds[\"k2\"]),\n",
    "            max(jump_lower, dict_default_reinit_lower_bounds[\"jump\"]),\n",
    "            max(t_jump_lower, dict_default_reinit_lower_bounds[\"t_jump\"]),\n",
    "            max(std_normal_lower, dict_default_reinit_lower_bounds[\"std_normal\"]),\n",
    "            max(k3_lower, dict_default_reinit_lower_bounds[\"k3\"]),\n",
    "        ]\n",
    "        param_list_upper = [\n",
    "            x + max(percentage_drift_upper_bound_annealing * abs(x), default_upper_bound_annealing) for x in\n",
    "            parameter_list\n",
    "        ]\n",
    "        (\n",
    "            alpha_upper, days_upper, r_s_upper, r_dth_upper, p_dth_upper, r_dthdecay_upper,\n",
    "            k1_upper, k2_upper, jump_upper, t_jump_upper, std_normal_upper, k3_upper\n",
    "        ) = param_list_upper\n",
    "        param_list_upper = [\n",
    "            max(alpha_upper, dict_default_reinit_upper_bounds[\"alpha\"]),\n",
    "            days_upper,\n",
    "            max(r_s_upper, dict_default_reinit_upper_bounds[\"r_s\"]),\n",
    "            max(min(r_dth_upper, 1), dict_default_reinit_upper_bounds[\"r_dth\"]),\n",
    "            max(min(p_dth_upper, 1), dict_default_reinit_upper_bounds[\"p_dth\"]),\n",
    "            max(r_dthdecay_upper, dict_default_reinit_upper_bounds[\"r_dthdecay\"]),\n",
    "            max(k1_upper, dict_default_reinit_upper_bounds[\"k1\"]),\n",
    "            max(k2_upper, dict_default_reinit_upper_bounds[\"k2\"]),\n",
    "            max(jump_upper, dict_default_reinit_upper_bounds[\"jump\"]),\n",
    "            max(t_jump_upper, dict_default_reinit_upper_bounds[\"t_jump\"]),\n",
    "            max(std_normal_upper, dict_default_reinit_upper_bounds[\"std_normal\"]),\n",
    "            max(k3_upper, dict_default_reinit_upper_bounds[\"k3\"]),\n",
    "        ]\n",
    "        param_list_lower[9] = default_lower_bound_t_jump  # jump lower bound\n",
    "        param_list_upper[9] = default_upper_bound_t_jump  # jump upper bound\n",
    "#        param_list_lower[10] = default_lower_bound_std_normal  # std_normal lower bound\n",
    "#        param_list_upper[10] = default_upper_bound_std_normal  # std_normal upper bound\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {optimizer} not supported in this implementation so can't generate bounds\")\n",
    "\n",
    "    bounds_params = [(lower, upper) for lower, upper in zip(param_list_lower, param_list_upper)]\n",
    "    return bounds_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_dates_us_policies(raw_date: str) -> Union[float, datetime]:\n",
    "#     \"\"\"\n",
    "#     Converts dates from the dataframe with raw policies implemented in the US\n",
    "#     :param raw_date: a certain date string in a raw format\n",
    "#     :return: a datetime in the right format for the final policy dataframe\n",
    "#     \"\"\"\n",
    "#     if raw_date == \"Not implemented\":\n",
    "#         return np.nan\n",
    "#     else:\n",
    "#         x_long = raw_date + \"20\"\n",
    "#         return pd.to_datetime(x_long, format=\"%d-%b-%Y\")\n",
    "\n",
    "\n",
    "# def check_us_policy_data_consistency(policies: list, df_policy_raw_us: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Checks consistency of the policy data in the US retrieved e.g. from IHME by verifying that if there is an end date\n",
    "#     there must also be a start date for the policy implemented\n",
    "#     :param policies: list of policies under consideration\n",
    "#     :param df_policy_raw_us: slightly processed dataframe with policies implemented in the US\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     for policy in policies:\n",
    "#         assert (\n",
    "#             len(\n",
    "#                 df_policy_raw_us.loc[\n",
    "#                     (df_policy_raw_us[f\"{policy}_start_date\"].isnull())\n",
    "#                     & (~df_policy_raw_us[f\"{policy}_end_date\"].isnull()),\n",
    "#                     :,\n",
    "#                 ]\n",
    "#             )\n",
    "#             == 0\n",
    "#         ), f\"Problem in data, policy {policy} has no start date but has an end date\"\n",
    "\n",
    "\n",
    "# def create_intermediary_policy_features_us(\n",
    "#     df_policy_raw_us: pd.DataFrame, dict_state_to_policy_dates: dict, policies: list\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Processes the IHME policy data in the US to create the right intermediary features with the right names\n",
    "#     :param df_policy_raw_us: raw dataframe with policies implemented in the US\n",
    "#     :param dict_state_to_policy_dates: dictionary of the format {state: {policy: [start_date, end_date]}}\n",
    "#     :param policies: list of policies under consideration\n",
    "#     :return: an intermediary dataframe with processed columns containing binary variables as to whether or not a\n",
    "#     policy is implemented in a given state at a given date\n",
    "#     \"\"\"\n",
    "#     list_df_concat = []\n",
    "#     n_dates = (datetime.now() - datetime(2020, 3, 1)).days + 1\n",
    "#     date_range = [datetime(2020, 3, 1) + timedelta(days=i) for i in range(n_dates)]\n",
    "#     for location in df_policy_raw_us.location_name.unique():\n",
    "#         df_temp = pd.DataFrame(\n",
    "#             {\n",
    "#                 \"continent\": [\"North America\" for _ in range(len(date_range))],\n",
    "#                 \"country\": [\"US\" for _ in range(len(date_range))],\n",
    "#                 \"province\": [location for _ in range(len(date_range))],\n",
    "#                 \"date\": date_range,\n",
    "#             }\n",
    "#         )\n",
    "#         for policy in policies:\n",
    "#             start_date_policy_location = dict_state_to_policy_dates[location][policy][0]\n",
    "#             start_date_policy_location = (\n",
    "#                 start_date_policy_location\n",
    "#                 if start_date_policy_location is not np.nan\n",
    "#                 else \"2030-01-02\"\n",
    "#             )\n",
    "#             end_date_policy_location = dict_state_to_policy_dates[location][policy][1]\n",
    "#             end_date_policy_location = (\n",
    "#                 end_date_policy_location\n",
    "#                 if end_date_policy_location is not np.nan\n",
    "#                 else \"2030-01-01\"\n",
    "#             )\n",
    "#             df_temp[policy] = 0\n",
    "#             df_temp.loc[\n",
    "#                 (\n",
    "#                     (df_temp.date >= start_date_policy_location)\n",
    "#                     & (df_temp.date <= end_date_policy_location)\n",
    "#                 ),\n",
    "#                 policy,\n",
    "#             ] = 1\n",
    "\n",
    "#         list_df_concat.append(df_temp)\n",
    "\n",
    "#     df_policies_US = pd.concat(list_df_concat).reset_index(drop=True)\n",
    "#     df_policies_US.rename(\n",
    "#         columns={\n",
    "#             \"travel_limit\": \"Travel_severely_limited\",\n",
    "#             \"stay_home\": \"Stay_at_home_order\",\n",
    "#             \"educational_fac\": \"Educational_Facilities_Closed\",\n",
    "#             \"any_gathering_restrict\": \"Mass_Gathering_Restrictions\",\n",
    "#             \"any_business\": \"Initial_Business_Closure\",\n",
    "#             \"all_non-ess_business\": \"Non_Essential_Services_Closed\",\n",
    "#         },\n",
    "#         inplace=True,\n",
    "#     )\n",
    "#     return df_policies_US\n",
    "\n",
    "\n",
    "# def create_final_policy_features_us(df_policies_US: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Creates the final MECE policies in the US from the intermediary policies dataframe\n",
    "#     :param df_policies_US: intermediary dataframe with processed columns containing binary variables as to whether or \n",
    "#     not a policy is implemented in a given state at a given date\n",
    "#     :return: dataframe with the final MECE policies in the US used for DELPHI policy predictions\n",
    "#     \"\"\"\n",
    "#     df_policies_US_final = deepcopy(df_policies_US)\n",
    "#     msr = future_policies\n",
    "#     df_policies_US_final[msr[0]] = (df_policies_US.sum(axis=1) == 0).apply(\n",
    "#         lambda x: int(x)\n",
    "#     )\n",
    "#     df_policies_US_final[msr[1]] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             df_policies_US.sum(axis=1) == 1,\n",
    "#             df_policies_US[\"Mass_Gathering_Restrictions\"] == 1,\n",
    "#         )\n",
    "#     ]\n",
    "#     df_policies_US_final[msr[2]] = [\n",
    "#         int(a and b and c)\n",
    "#         for a, b, c in zip(\n",
    "#             df_policies_US.sum(axis=1) > 0,\n",
    "#             df_policies_US[\"Mass_Gathering_Restrictions\"] == 0,\n",
    "#             df_policies_US[\"Stay_at_home_order\"] == 0,\n",
    "#         )\n",
    "#     ]\n",
    "#     df_policies_US_final[msr[3]] = [\n",
    "#         int(a and b and c)\n",
    "#         for a, b, c in zip(\n",
    "#             df_policies_US.sum(axis=1) == 2,\n",
    "#             df_policies_US[\"Educational_Facilities_Closed\"] == 1,\n",
    "#             df_policies_US[\"Mass_Gathering_Restrictions\"] == 1,\n",
    "#         )\n",
    "#     ]\n",
    "#     df_policies_US_final[msr[4]] = [\n",
    "#         int(a and b and c and d)\n",
    "#         for a, b, c, d in zip(\n",
    "#             df_policies_US.sum(axis=1) > 1,\n",
    "#             df_policies_US[\"Educational_Facilities_Closed\"] == 0,\n",
    "#             df_policies_US[\"Mass_Gathering_Restrictions\"] == 1,\n",
    "#             df_policies_US[\"Stay_at_home_order\"] == 0,\n",
    "#         )\n",
    "#     ]\n",
    "#     df_policies_US_final[msr[5]] = [\n",
    "#         int(a and b and c and d)\n",
    "#         for a, b, c, d in zip(\n",
    "#             df_policies_US.sum(axis=1) > 2,\n",
    "#             df_policies_US[\"Educational_Facilities_Closed\"] == 1,\n",
    "#             df_policies_US[\"Mass_Gathering_Restrictions\"] == 1,\n",
    "#             df_policies_US[\"Stay_at_home_order\"] == 0,\n",
    "#         )\n",
    "#     ]\n",
    "#     df_policies_US_final[msr[6]] = (df_policies_US[\"Stay_at_home_order\"] == 1).apply(\n",
    "#         lambda x: int(x)\n",
    "#     )\n",
    "#     df_policies_US_final[\"country\"] = \"US\"\n",
    "#     df_policies_US_final = df_policies_US_final.loc[\n",
    "#         :, [\"country\", \"province\", \"date\"] + msr\n",
    "#     ]\n",
    "#     return df_policies_US_final\n",
    "\n",
    "\n",
    "# def read_policy_data_us_only(filepath_data_sandbox: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Reads and processes the policy data from IHME to obtain the MECE policies defined for DELPHI Policy Predictions\n",
    "#     :param filepath_data_sandbox: string, path to the data sandbox drawn from the config.yml file in the main script\n",
    "#     :return: fully processed dataframe containing the MECE policies implemented in each state of the US for the full \n",
    "#     time period necessary until the day when this function is called\n",
    "#     \"\"\"\n",
    "#     policies = [\n",
    "#         \"travel_limit\", \"stay_home\", \"educational_fac\", \"any_gathering_restrict\",\n",
    "#         \"any_business\", \"all_non-ess_business\",\n",
    "#     ]\n",
    "#     list_US_states = [\n",
    "#         \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\",\n",
    "#         \"District of Columbia\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
    "#         \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\",\n",
    "#         \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n",
    "#         \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\",\n",
    "#         \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\",\n",
    "#         \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n",
    "#     ]\n",
    "#     df = pd.read_csv(filepath_data_sandbox + \"12062020_raw_policy_data_us_only.csv\")\n",
    "#     df = df[df.location_name.isin(list_US_states)][\n",
    "#         [\n",
    "#             \"location_name\", \"travel_limit_start_date\", \"travel_limit_end_date\", \"stay_home_start_date\",\n",
    "#             \"stay_home_end_date\", \"educational_fac_start_date\", \"educational_fac_end_date\",\n",
    "#             \"any_gathering_restrict_start_date\", \"any_gathering_restrict_end_date\", \"any_business_start_date\",\n",
    "#             \"any_business_end_date\", \"all_non-ess_business_start_date\", \"all_non-ess_business_end_date\",\n",
    "#         ]\n",
    "#     ]\n",
    "#     dict_state_to_policy_dates = {}\n",
    "#     for location in df.location_name.unique():\n",
    "#         df_temp = df[df.location_name == location].reset_index(drop=True)\n",
    "#         dict_state_to_policy_dates[location] = {\n",
    "#             policy: [\n",
    "#                 df_temp.loc[0, f\"{policy}_start_date\"],\n",
    "#                 df_temp.loc[0, f\"{policy}_end_date\"],\n",
    "#             ]\n",
    "#             for policy in policies\n",
    "#         }\n",
    "#     check_us_policy_data_consistency(policies=policies, df_policy_raw_us=df)\n",
    "#     df_policies_US = create_intermediary_policy_features_us(\n",
    "#         df_policy_raw_us=df,\n",
    "#         dict_state_to_policy_dates=dict_state_to_policy_dates,\n",
    "#         policies=policies,\n",
    "#     )\n",
    "#     df_policies_US_final = create_final_policy_features_us(\n",
    "#         df_policies_US=df_policies_US\n",
    "#     )\n",
    "#     return df_policies_US_final\n",
    "\n",
    "\n",
    "# def read_oxford_international_policy_data(yesterday: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Reads the policy data from the Oxford dataset online and processes it to obtain the MECE policies for all other\n",
    "#     countries than the US\n",
    "#     :param yesterday: string date used in the main script as the day for which we read past parameters used as warm \n",
    "#     starts for the optimization\n",
    "#     :return: processed dataframe with MECE policies in each country of the world, used for policy predictions\n",
    "#     \"\"\"\n",
    "#     measures = pd.read_csv(\"https://github.com/OxCGRT/covid-policy-tracker/raw/master/data/OxCGRT_latest.csv\")\n",
    "#     filtr = [\"CountryName\", \"CountryCode\", \"Date\"]\n",
    "#     target = [\"ConfirmedCases\", \"ConfirmedDeaths\"]\n",
    "#     msr = [\n",
    "#         \"C1_School closing\",\n",
    "#         \"C2_Workplace closing\",\n",
    "#         \"C3_Cancel public events\",\n",
    "#         \"C4_Restrictions on gatherings\",\n",
    "#         \"C5_Close public transport\",\n",
    "#         \"C6_Stay at home requirements\",\n",
    "#         \"C7_Restrictions on internal movement\",\n",
    "#         \"C8_International travel controls\",\n",
    "#         \"H1_Public information campaigns\",\n",
    "#     ]\n",
    "\n",
    "#     flags = [\"C\" + str(i) + \"_Flag\" for i in range(1, 8)] + [\"H1_Flag\"]\n",
    "#     measures = measures.loc[:, filtr + msr + flags + target]\n",
    "#     measures[\"Date\"] = measures[\"Date\"].apply(\n",
    "#         lambda x: datetime.strptime(str(x), \"%Y%m%d\")\n",
    "#     )\n",
    "#     for col in target:\n",
    "#         # measures[col] = measures[col].fillna(0)\n",
    "#         measures[col] = measures.groupby(\"CountryName\")[col].ffill()\n",
    "\n",
    "#     measures[\"C1_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(measures[\"C1_School closing\"], measures[\"C1_Flag\"])\n",
    "#     ]\n",
    "#     measures[\"C2_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(measures[\"C2_Workplace closing\"], measures[\"C2_Flag\"])\n",
    "#     ]\n",
    "#     measures[\"C3_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(measures[\"C3_Cancel public events\"], measures[\"C3_Flag\"])\n",
    "#     ]\n",
    "#     measures[\"C4_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(\n",
    "#             measures[\"C4_Restrictions on gatherings\"], measures[\"C4_Flag\"]\n",
    "#         )\n",
    "#     ]\n",
    "#     measures[\"C5_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(measures[\"C5_Close public transport\"], measures[\"C5_Flag\"])\n",
    "#     ]\n",
    "#     measures[\"C6_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(measures[\"C6_Stay at home requirements\"], measures[\"C6_Flag\"])\n",
    "#     ]\n",
    "#     measures[\"C7_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(\n",
    "#             measures[\"C7_Restrictions on internal movement\"], measures[\"C7_Flag\"]\n",
    "#         )\n",
    "#     ]\n",
    "#     measures[\"H1_Flag\"] = [\n",
    "#         0 if x <= 0 else y\n",
    "#         for (x, y) in zip(\n",
    "#             measures[\"H1_Public information campaigns\"], measures[\"H1_Flag\"]\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C1_School closing\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(measures[\"C1_School closing\"] >= 2, measures[\"C1_Flag\"] == 1)\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C2_Workplace closing\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(measures[\"C2_Workplace closing\"] >= 2, measures[\"C2_Flag\"] == 1)\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C3_Cancel public events\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures[\"C3_Cancel public events\"] >= 2, measures[\"C3_Flag\"] == 1\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C4_Restrictions on gatherings\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures[\"C4_Restrictions on gatherings\"] >= 1, measures[\"C4_Flag\"] == 1\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C5_Close public transport\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures[\"C5_Close public transport\"] >= 2, measures[\"C5_Flag\"] == 1\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C6_Stay at home requirements\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures[\"C6_Stay at home requirements\"] >= 2, measures[\"C6_Flag\"] == 1\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C7_Restrictions on internal movement\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures[\"C7_Restrictions on internal movement\"] >= 2,\n",
    "#             measures[\"C7_Flag\"] == 1,\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     measures[\"C8_International travel controls\"] = [\n",
    "#         int(a) for a in (measures[\"C8_International travel controls\"] >= 3)\n",
    "#     ]\n",
    "\n",
    "#     measures[\"H1_Public information campaigns\"] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures[\"H1_Public information campaigns\"] >= 1, measures[\"H1_Flag\"] == 1\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     # measures = measures.loc[:, measures.isnull().mean() < 0.1]\n",
    "#     msr = set(measures.columns).intersection(set(msr))\n",
    "\n",
    "#     # measures = measures.fillna(0)\n",
    "#     measures = measures.dropna()\n",
    "#     for col in msr:\n",
    "#         measures[col] = measures[col].apply(lambda x: int(x > 0))\n",
    "#     measures = measures[[\"CountryName\", \"Date\"] + list(sorted(msr))]\n",
    "#     measures[\"CountryName\"] = measures.CountryName.replace(\n",
    "#         {\n",
    "#             \"United States\": \"US\",\n",
    "#             \"South Korea\": \"Korea, South\",\n",
    "#             \"Democratic Republic of Congo\": \"Congo (Kinshasa)\",\n",
    "#             \"Czech Republic\": \"Czechia\",\n",
    "#             \"Slovak Republic\": \"Slovakia\",\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     measures = measures.fillna(0)\n",
    "#     msr = future_policies\n",
    "\n",
    "#     measures[\"Restrict_Mass_Gatherings\"] = [\n",
    "#         int(a or b or c)\n",
    "#         for a, b, c in zip(\n",
    "#             measures[\"C3_Cancel public events\"],\n",
    "#             measures[\"C4_Restrictions on gatherings\"],\n",
    "#             measures[\"C5_Close public transport\"],\n",
    "#         )\n",
    "#     ]\n",
    "#     measures[\"Others\"] = [\n",
    "#         int(a or b or c)\n",
    "#         for a, b, c in zip(\n",
    "#             measures[\"C2_Workplace closing\"],\n",
    "#             measures[\"C7_Restrictions on internal movement\"],\n",
    "#             measures[\"C8_International travel controls\"],\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     del measures[\"C2_Workplace closing\"]\n",
    "#     del measures[\"C3_Cancel public events\"]\n",
    "#     del measures[\"C4_Restrictions on gatherings\"]\n",
    "#     del measures[\"C5_Close public transport\"]\n",
    "#     del measures[\"C7_Restrictions on internal movement\"]\n",
    "#     del measures[\"C8_International travel controls\"]\n",
    "\n",
    "#     output = deepcopy(measures)\n",
    "#     output[msr[0]] = (measures.iloc[:, 2:].sum(axis=1) == 0).apply(lambda x: int(x))\n",
    "#     output[msr[1]] = [\n",
    "#         int(a and b)\n",
    "#         for a, b in zip(\n",
    "#             measures.iloc[:, 2:].sum(axis=1) == 1,\n",
    "#             measures[\"Restrict_Mass_Gatherings\"] == 1,\n",
    "#         )\n",
    "#     ]\n",
    "#     output[msr[2]] = [\n",
    "#         int(a and b and c)\n",
    "#         for a, b, c in zip(\n",
    "#             measures.iloc[:, 2:].sum(axis=1) > 0,\n",
    "#             measures[\"Restrict_Mass_Gatherings\"] == 0,\n",
    "#             measures[\"C6_Stay at home requirements\"] == 0,\n",
    "#         )\n",
    "#     ]\n",
    "#     output[msr[3]] = [\n",
    "#         int(a and b and c)\n",
    "#         for a, b, c in zip(\n",
    "#             measures.iloc[:, 2:].sum(axis=1) == 2,\n",
    "#             measures[\"C1_School closing\"] == 1,\n",
    "#             measures[\"Restrict_Mass_Gatherings\"] == 1,\n",
    "#         )\n",
    "#     ]\n",
    "#     output[msr[4]] = [\n",
    "#         int(a and b and c and d)\n",
    "#         for a, b, c, d in zip(\n",
    "#             measures.iloc[:, 2:].sum(axis=1) > 1,\n",
    "#             measures[\"C1_School closing\"] == 0,\n",
    "#             measures[\"Restrict_Mass_Gatherings\"] == 1,\n",
    "#             measures[\"C6_Stay at home requirements\"] == 0,\n",
    "#         )\n",
    "#     ]\n",
    "#     output[msr[5]] = [\n",
    "#         int(a and b and c and d)\n",
    "#         for a, b, c, d in zip(\n",
    "#             measures.iloc[:, 2:].sum(axis=1) > 2,\n",
    "#             measures[\"C1_School closing\"] == 1,\n",
    "#             measures[\"Restrict_Mass_Gatherings\"] == 1,\n",
    "#             measures[\"C6_Stay at home requirements\"] == 0,\n",
    "#         )\n",
    "#     ]\n",
    "#     output[msr[6]] = (measures[\"C6_Stay at home requirements\"] == 1).apply(\n",
    "#         lambda x: int(x)\n",
    "#     )\n",
    "#     output.rename(columns={\"CountryName\": \"country\", \"Date\": \"date\"}, inplace=True)\n",
    "#     output[\"province\"] = \"None\"\n",
    "#     output = output.loc[:, [\"country\", \"province\", \"date\"] + msr]\n",
    "#     output = output[output.date <= yesterday].reset_index(drop=True)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def gamma_t(day: datetime, state: str, params_dict: dict) -> float:\n",
    "#     \"\"\"\n",
    "#     Computes values of our gamma(t) function that was used before the second wave modeling with the extra normal\n",
    "#     distribution, but is still being used for policy predictions\n",
    "#     :param day: day on which we want to compute the value of gamma(t)\n",
    "#     :param state: string, state name\n",
    "#     :param params_dict: dictionary with format {state: (dsd, median_day_of_action, rate_of_action)}\n",
    "#     :return: value of gamma(t) for that particular state on that day and with the input parameters\n",
    "#     \"\"\"\n",
    "#     dsd, median_day_of_action, rate_of_action = params_dict[state]\n",
    "#     t = (day - pd.to_datetime(dsd)).days\n",
    "#     gamma = (2 / np.pi) * np.arctan(\n",
    "#         -(t - median_day_of_action) / 20 * rate_of_action\n",
    "#     ) + 1\n",
    "#     return gamma\n",
    "\n",
    "\n",
    "# def make_increasing(sequence: list) -> list:\n",
    "#     \"\"\"\n",
    "#     Used to force the Confidence Intervals generated for DELPHI to be always increasing\n",
    "#     :param sequence: list, sequence of values\n",
    "#     :return: list, forcefully increasing sequence of values\n",
    "#     \"\"\"\n",
    "#     for i in range(len(sequence)):\n",
    "#         sequence[i] = max(sequence[i], sequence[max(i-1, 0)])\n",
    "#     return sequence\n",
    "\n",
    "\n",
    "# def get_normalized_policy_shifts_and_current_policy_us_only(\n",
    "#     policy_data_us_only: pd.DataFrame, past_parameters: pd.DataFrame\n",
    "# ) -> (dict, dict):\n",
    "#     \"\"\"\n",
    "#     Computes the normalized policy shifts and the current policy in each state of the US\n",
    "#     :param policy_data_us_only: processed dataframe with the MECE policies implemented per state for every day\n",
    "#     :param past_parameters: past parameters file used for policy shift generation (specifically computation of gamma(t)\n",
    "#     values in the process\n",
    "#     :return: a tuple of two dictionaries, {policy: normalized_shift_float_US} and {US_state: current_policy}\n",
    "#     \"\"\"\n",
    "#     dict_current_policy = {}\n",
    "#     policy_list = future_policies\n",
    "#     policy_data_us_only[\"province_cl\"] = policy_data_us_only[\"province\"].apply(\n",
    "#         lambda x: x.replace(\",\", \"\").strip().lower()\n",
    "#     )\n",
    "#     states_upper_set = set(policy_data_us_only[\"province\"])\n",
    "#     for state in states_upper_set:\n",
    "#         dict_current_policy[(\"US\", state)] = list(\n",
    "#             compress(\n",
    "#                 policy_list,\n",
    "#                 (\n",
    "#                     policy_data_us_only.query(\"province == @state\")[\n",
    "#                         policy_data_us_only.query(\"province == @state\")[\"date\"]\n",
    "#                         == policy_data_us_only.date.max()\n",
    "#                     ][policy_list]\n",
    "#                     == 1\n",
    "#                 )\n",
    "#                 .values.flatten()\n",
    "#                 .tolist(),\n",
    "#             )\n",
    "#         )[0]\n",
    "#     states_set = set(policy_data_us_only[\"province_cl\"])\n",
    "#     past_parameters_copy = deepcopy(past_parameters)\n",
    "#     past_parameters_copy[\"Province\"] = past_parameters_copy[\"Province\"].apply(\n",
    "#         lambda x: str(x).replace(\",\", \"\").strip().lower()\n",
    "#     )\n",
    "#     params_dic = {}\n",
    "#     for state in states_set:\n",
    "#         params_dic[state] = past_parameters_copy.query(\"Province == @state\")[\n",
    "#             [\"Data Start Date\", \"Median Day of Action\", \"Rate of Action\"]\n",
    "#         ].iloc[0]\n",
    "\n",
    "#     policy_data_us_only[\"Gamma\"] = [\n",
    "#         gamma_t(day, state, params_dic)\n",
    "#         for day, state in zip(\n",
    "#             policy_data_us_only[\"date\"], policy_data_us_only[\"province_cl\"]\n",
    "#         )\n",
    "#     ]\n",
    "#     n_measures = policy_data_us_only.iloc[:, 3:-2].shape[1]\n",
    "#     dict_normalized_policy_gamma = {\n",
    "#         policy_data_us_only.columns[3 + i]: policy_data_us_only[\n",
    "#             policy_data_us_only.iloc[:, 3 + i] == 1\n",
    "#         ]\n",
    "#         .iloc[:, -1]\n",
    "#         .mean()\n",
    "#         for i in range(n_measures)\n",
    "#     }\n",
    "#     normalize_val = dict_normalized_policy_gamma[policy_list[0]]\n",
    "#     for policy in dict_normalized_policy_gamma.keys():\n",
    "#         dict_normalized_policy_gamma[policy] = (\n",
    "#             dict_normalized_policy_gamma[policy] / normalize_val\n",
    "#         )\n",
    "\n",
    "#     return dict_normalized_policy_gamma, dict_current_policy\n",
    "\n",
    "\n",
    "# def get_normalized_policy_shifts_and_current_policy_all_countries(\n",
    "#     policy_data_countries: pd.DataFrame, past_parameters: pd.DataFrame\n",
    "# ) -> (dict, dict):\n",
    "#     \"\"\"\n",
    "#     Computes the normalized policy shifts and the current policy in each area of the world except the US\n",
    "#     (done in a separate function)\n",
    "#     :param policy_data_countries: processed dataframe with the MECE policies implemented per area for every day\n",
    "#     :param past_parameters: past parameters file used for policy shift generation (specifically computation of gamma(t)\n",
    "#     values in the process\n",
    "#     :return: a tuple of two dictionaries, {policy: normalized_shift_float_international} and {area: current_policy}\n",
    "#     \"\"\"\n",
    "#     dict_current_policy = {}\n",
    "#     policy_list = future_policies\n",
    "#     policy_data_countries[\"country_cl\"] = policy_data_countries[\"country\"].apply(\n",
    "#         lambda x: x.replace(\",\", \"\").strip().lower()\n",
    "#     )\n",
    "#     past_parameters_copy = deepcopy(past_parameters)\n",
    "#     past_parameters_copy[\"Country\"] = past_parameters_copy[\"Country\"].apply(\n",
    "#         lambda x: str(x).replace(\",\", \"\").strip().lower()\n",
    "#     )\n",
    "#     params_countries = past_parameters_copy[\"Country\"]\n",
    "#     params_countries = set(params_countries)\n",
    "#     policy_data_countries_bis = policy_data_countries.query(\n",
    "#         \"country_cl in @params_countries\"\n",
    "#     )\n",
    "#     countries_upper_set = set(\n",
    "#         policy_data_countries[policy_data_countries.country != \"US\"][\"country\"]\n",
    "#     )\n",
    "#     # countries_in_oxford_and_params = params_countries.intersection(countries_upper_set)\n",
    "#     for country in countries_upper_set:\n",
    "#         dict_current_policy[(country, \"None\")] = list(\n",
    "#             compress(\n",
    "#                 policy_list,\n",
    "#                 (\n",
    "#                     policy_data_countries.query(\"country == @country\")[\n",
    "#                         policy_data_countries.query(\"country == @country\")[\"date\"]\n",
    "#                         == policy_data_countries.query(\"country == @country\").date.max()\n",
    "#                     ][policy_list]\n",
    "#                     == 1\n",
    "#                 )\n",
    "#                 .values.flatten()\n",
    "#                 .tolist(),\n",
    "#             )\n",
    "#         )[0]\n",
    "#     countries_common = sorted([x.lower() for x in countries_upper_set])\n",
    "#     pastparam_tuples_in_oxford = past_parameters_copy[\n",
    "#         (past_parameters_copy.Country.isin(countries_common))\n",
    "#         & (past_parameters_copy.Province != \"None\")\n",
    "#     ].reset_index(drop=True)\n",
    "#     pastparam_tuples_in_oxford[\"tuple_name\"] = list(\n",
    "#         zip(pastparam_tuples_in_oxford.Country, pastparam_tuples_in_oxford.Province)\n",
    "#     )\n",
    "#     for tuple in pastparam_tuples_in_oxford.tuple_name.unique():\n",
    "#         country, province = tuple\n",
    "#         country = country[0].upper() + country[1:]\n",
    "#         dict_current_policy[(country, province)] = dict_current_policy[\n",
    "#             (country, \"None\")\n",
    "#         ]\n",
    "\n",
    "#     countries_set = set(policy_data_countries[\"country_cl\"])\n",
    "\n",
    "#     params_dic = {}\n",
    "#     countries_set = countries_set.intersection(params_countries)\n",
    "#     for country in countries_set:\n",
    "#         params_dic[country] = past_parameters_copy.query(\"Country == @country\")[\n",
    "#             [\"Data Start Date\", \"Median Day of Action\", \"Rate of Action\"]\n",
    "#         ].iloc[0]\n",
    "\n",
    "#     policy_data_countries_bis[\"Gamma\"] = [\n",
    "#         gamma_t(day, country, params_dic)\n",
    "#         for day, country in zip(\n",
    "#             policy_data_countries_bis[\"date\"], policy_data_countries_bis[\"country_cl\"]\n",
    "#         )\n",
    "#     ]\n",
    "#     n_measures = policy_data_countries_bis.iloc[:, 3:-2].shape[1]\n",
    "#     dict_normalized_policy_gamma = {\n",
    "#         policy_data_countries_bis.columns[3 + i]: policy_data_countries_bis[\n",
    "#             policy_data_countries_bis.iloc[:, 3 + i] == 1\n",
    "#         ]\n",
    "#         .iloc[:, -1]\n",
    "#         .mean()\n",
    "#         for i in range(n_measures)\n",
    "#     }\n",
    "#     normalize_val = dict_normalized_policy_gamma[policy_list[0]]\n",
    "#     for policy in dict_normalized_policy_gamma.keys():\n",
    "#         dict_normalized_policy_gamma[policy] = (\n",
    "#             dict_normalized_policy_gamma[policy] / normalize_val\n",
    "#         )\n",
    "\n",
    "#     return dict_normalized_policy_gamma, dict_current_policy\n",
    "\n",
    "\n",
    "# def get_testing_data_us() -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Function that retrieves testing data in the US from the CovidTracking website\n",
    "#     :return: a DataFrame where the column of interest is 'testing_cnt_daily'\n",
    "#     which gives the numbers of new daily tests per state\n",
    "#     \"\"\"\n",
    "#     df_test = pd.read_csv(\"https://covidtracking.com/api/v1/states/daily.csv\")\n",
    "#     df_test[\"country\"] = \"US\"\n",
    "#     df_test[\"continent\"] = \"North America\"\n",
    "#     df_test[\"province\"] = df_test.state.map(MAPPING_STATE_CODE_TO_STATE_NAME)\n",
    "#     df_test.drop(\"state\", axis=1, inplace=True)\n",
    "#     df_test[\"date\"] = df_test.date.apply(\n",
    "#         lambda x: str(x)[:4] + \"-\" + str(x)[4:6] + \"-\" + str(x)[6:]\n",
    "#     )\n",
    "#     df_test[\"date\"] = pd.to_datetime(df_test.date)\n",
    "#     df_test = df_test.sort_values([\"province\", \"date\"]).reset_index(drop=True)\n",
    "#     df_test = df_test[[\"continent\", \"country\", \"province\", \"date\", \"totalTestResults\"]]\n",
    "#     df_test.rename(columns={\"totalTestResults\": \"testing_cnt\"}, inplace=True)\n",
    "#     list_df_concat = []\n",
    "#     for state in df_test.province.unique():\n",
    "#         df_temp = df_test[df_test.province == state].reset_index(drop=True)\n",
    "#         df_temp[\"testing_cnt_shift\"] = df_temp.testing_cnt.shift(1)\n",
    "#         df_temp[\"testing_cnt_daily\"] = df_temp.testing_cnt - df_temp.testing_cnt_shift\n",
    "#         df_temp.loc[0, \"testing_cnt_daily\"] = df_temp.loc[0, \"testing_cnt\"]\n",
    "#         list_df_concat.append(df_temp)\n",
    "\n",
    "#     df_test_final = pd.concat(list_df_concat).reset_index(drop=True)\n",
    "#     df_test_final.drop([\"testing_cnt\", \"testing_cnt_shift\"], axis=1, inplace=True)\n",
    "#     return df_test_final\n",
    "\n",
    "# class DELPHIModelComparison:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         path_to_folder_danger_map: str,\n",
    "#         path_to_folder_data_sandbox: str,\n",
    "#         global_annealing_since_100days: pd.DataFrame,\n",
    "#         total_tnc_since_100days: pd.DataFrame,\n",
    "#         logger: Logger\n",
    "#     ):\n",
    "#         self.DANGER_MAP = path_to_folder_danger_map\n",
    "#         self.DATA_SANDBOX = path_to_folder_data_sandbox\n",
    "#         self.global_annealing_since_100days = global_annealing_since_100days\n",
    "#         self.total_tnc_since_100days = total_tnc_since_100days\n",
    "#         self.logger = logger\n",
    "\n",
    "#     @staticmethod\n",
    "#     def kl_divergence(y_true: list, y_pred: list) -> float:\n",
    "#         \"\"\"\n",
    "#         Compute the KL divergence between two lists\n",
    "#         :param y_true: list of true historical values\n",
    "#         :param y_pred: list of predicted values\n",
    "#         :return: a float, corresponding to the KL divergence\n",
    "#         \"\"\"\n",
    "#         y_true = np.asarray(y_true, dtype=np.float)\n",
    "#         y_pred = np.asarray(y_pred, dtype=np.float)\n",
    "        \n",
    "#         return np.sum(np.where(y_true != 0, y_true * np.log(y_true / y_pred), 0))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def max_ape(y_true: list, y_pred: list) -> float:\n",
    "#         \"\"\"\n",
    "#         Compute the Maximum Absolute Percentage Error between two lists\n",
    "#         :param y_true: list of true historical values\n",
    "#         :param y_pred: list of predicted values\n",
    "#         :return: a float, corresponding to the MAPE\n",
    "#         \"\"\"\n",
    "#         # ape = [abs(x-y)/x for x,y in zip(y_true, y_pred) if y!= 0 and x > 100]\n",
    "#         ape = [abs(x-y)/x for x,y in zip(y_true, y_pred) if x > 0]\n",
    "#         if len(ape)>0:\n",
    "#             return max(ape)\n",
    "#         return 0.0\n",
    "\n",
    "#     @staticmethod\n",
    "#     def max_ape_ma(y_true: list, y_pred: list, n:int = 10) -> float:\n",
    "#         \"\"\"\n",
    "#         Compute the Maximum Absolute Percentage Error between two lists\n",
    "#         :param y_true: list of true historical values\n",
    "#         :param y_pred: list of predicted values\n",
    "#         :return: a float, corresponding to the MAPE\n",
    "#         \"\"\"\n",
    "#         y_true_ma = np.cumsum(np.array(y_true))\n",
    "#         y_true_ma = y_true_ma[n:] - y_true_ma[:-n]\n",
    "#         y_pred_ma = np.cumsum(np.array(y_pred))\n",
    "#         y_pred_ma = y_pred_ma[n:] - y_pred_ma[:-n]\n",
    "\n",
    "#         ape = [abs(x-y)/x for x,y in zip(y_true_ma, y_pred_ma) if x > 0]\n",
    "#         if len(ape)>0:\n",
    "#             return max(ape)\n",
    "#         return 10.0\n",
    "\n",
    "#     def get_province(self, country: str, province: str, min_case_count=100) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Returns actual cases data for the given country and province\n",
    "#         :param country: str, the name of the country \n",
    "#         :param province: str, the name of the province\n",
    "#         :param min_case_count: int, the minimum number of cases since when data is selected\n",
    "#         :return: a pandas dataframe for date wise cases for the given country and provinve where cases >\n",
    "#         min_case_count\n",
    "#         \"\"\"\n",
    "#         province = '_'.join(province.split())\n",
    "#         country = '_'.join(country.split())\n",
    "#         true_df = pd.read_csv(self.DANGER_MAP + f'processed/Global/Cases_{country}_{province}.csv')\n",
    "#         true_df = true_df.query('case_cnt >= @min_case_count').sort_values('date').groupby('date').min().reset_index()\n",
    "#         return(true_df)\n",
    "\n",
    "#     def compare_metric(self,\n",
    "#                     province_tuple,\n",
    "#                     min_case_count=100,\n",
    "#                     metric=\"Canberra\",\n",
    "#                     threshold=1.0,\n",
    "#                     plot=False,\n",
    "#                     eps=0.02):\n",
    "#         \"\"\"\n",
    "#         Computes the given metric for predictions with annealing and tnc and the MAPE for annealing.\n",
    "#         Returns the metrics along with a flag showing whether annealing did better than tnc.\n",
    "#         :param province_tuple: a 3 tuple of str, tuple of (continent, country, province)\n",
    "#         :param min_case_count: int, the minimum number of cases since when data is selected\n",
    "#         :param metric: function, the primary metric that is used, KL divergence by default\n",
    "#         :param threshold: float, the threshold on Max APE score for annealing to be selected\n",
    "#         :param plot: boolean, to save plots of predictions or not, default = False\n",
    "#         :return: a 4 tuple of (if annealing is better, metric for annealing, metric for tnc,\n",
    "#         Max APE for annealing)\n",
    "#         \"\"\"\n",
    "#         today_date_str = \"\".join(str(datetime.now().date()).split(\"-\"))\n",
    "\n",
    "#         continent, country, province = province_tuple\n",
    "#         true_df = self.get_province(country, province, min_case_count=min_case_count)\n",
    "#         annealing_df = self.global_annealing_since_100days.query('Continent == @continent').query('Country == @country').query('Province == @province').sort_values('Day').groupby('Day').min().reset_index()\n",
    "#         tnc_df = self.total_tnc_since_100days.query('Continent == @continent').query('Country == @country').query('Province == @province').sort_values('Day').groupby('Day').min().reset_index()\n",
    "\n",
    "#         annealing_df['Annealing Prediction'] = annealing_df['Total Detected'].diff().apply(lambda x: x if x > 1 else 1)\n",
    "#         tnc_df['TNC Prediction'] = tnc_df['Total Detected'].diff().apply(lambda x: x if x > 1 else 1)\n",
    "#         true_df['True Value'] = true_df['case_cnt'].diff().apply(lambda x: x if x > 1 else 1)\n",
    "\n",
    "#         annealing_df = annealing_df[['Day', 'Annealing Prediction']].dropna()\n",
    "#         tnc_df = tnc_df[['Day', 'TNC Prediction']].dropna()\n",
    "#         true_df = true_df[['date', 'True Value']].dropna()\n",
    "#         true_df['date'] = true_df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "#         merged = true_df.merge(annealing_df, how='inner', left_on='date', right_on='Day').merge(tnc_df, how='inner', left_on='date', right_on='Day').drop(columns=['Day_x', 'Day_y'])\n",
    "\n",
    "#         if plot:\n",
    "#             if not os.path.exists(self.DATA_SANDBOX + \"plots/\"):\n",
    "#                 os.mkdir(self.DATA_SANDBOX + \"plots/\")\n",
    "#             plt.plot(merged['date'], merged['True Value'], label='True')\n",
    "#             plt.plot(merged['date'], merged['Annealing Prediction'], label='Annealing')\n",
    "#             plt.plot(merged['date'], merged['TNC Prediction'], label='TNC')\n",
    "#             plt.title(f\"{continent}, {country}, {province}\")\n",
    "#             plt.legend()\n",
    "#             plt.savefig(self.DATA_SANDBOX + f\"plots/model_v4_comparison_{country}_{province}_{today_date_str}.png\")\n",
    "#             plt.clf()\n",
    "\n",
    "#         if metric == \"KL\":\n",
    "#             self.logger.info(\"Using KL divergence metric\")\n",
    "#             metric = DELPHIModelComparison.kl_divergence\n",
    "#         elif metric == \"Canberra\":\n",
    "#             metric = distance.canberra\n",
    "#         else:\n",
    "#             self.logger.error(f\"Metric {metric} has not been implemented. Only KL divergence is implemented so far\")\n",
    "#             raise NotImplementedError(\"Only KL divergence is implemented as a comparison metric\")\n",
    "#         metric_annealing = metric(merged['True Value'], merged['Annealing Prediction'], w = list(range(len(merged[\"True Value\"]))))\n",
    "#         metric_tnc = metric(merged['True Value'], merged['TNC Prediction'], w = list(range(len(merged[\"True Value\"]))))\n",
    "#         max_ape = DELPHIModelComparison.max_ape_ma(merged['True Value'], merged['Annealing Prediction'])\n",
    "\n",
    "#         self.logger.info('Distance for Annealing: ' + str(metric_annealing))\n",
    "#         self.logger.info('Distance for TNC: ' + str(metric_tnc))\n",
    "#         self.logger.info(('Annealing' if metric_annealing < metric_tnc else 'TNC') + ' is better')\n",
    "\n",
    "#         if metric_annealing < metric_tnc - eps*abs(metric_tnc):\n",
    "#             self.logger.info(f'Max APE for Annealing: {max_ape:.3g} Threshold is {threshold}')\n",
    "#             if max_ape < threshold:\n",
    "#                 self.logger.debug('Max APE condition satisfied and Annealing better than TNC. Use Annealing.')\n",
    "#                 return (True, metric_annealing, metric_tnc, max_ape)\n",
    "#             else:\n",
    "#                 self.logger.debug('Annealing better than TNC but Max APE condition not satisfied. Retrain.')\n",
    "#                 return (False, metric_annealing, metric_tnc, max_ape)\n",
    "#         else:\n",
    "#             self.logger.debug('TNC better than Annealing. Retrain.')\n",
    "#             return (False, metric_annealing, metric_tnc, max_ape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

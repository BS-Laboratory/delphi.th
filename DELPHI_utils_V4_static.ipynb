{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from DELPHI_utils_V4_dynamic.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Authors: Hamza Tazi Bouardi (htazi@mit.edu), Michael L. Li (mlli@mit.edu), Omar Skali Lami (oskali@mit.edu)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "import json\n",
    "import import_ipynb\n",
    "from logging import Logger\n",
    "# from DELPHI_params_V4 import (\n",
    "#     TIME_DICT,\n",
    "#     default_policy,\n",
    "#     default_policy_enaction_time,\n",
    "# )\n",
    "from DELPHI_utils_V4_dynamic import make_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELPHIDataSaver:\n",
    "    def __init__(\n",
    "            self,\n",
    "            path_to_folder_danger_map: str,\n",
    "            path_to_website_predicted: str,\n",
    "            df_global_parameters: Union[pd.DataFrame, None],\n",
    "            df_global_predictions_since_today: pd.DataFrame,\n",
    "            df_global_predictions_since_100_cases: pd.DataFrame,\n",
    "            logger: Logger = None\n",
    "    ):\n",
    "        self.PATH_TO_FOLDER_DANGER_MAP = path_to_folder_danger_map\n",
    "        self.PATH_TO_WEBSITE_PREDICTED = path_to_website_predicted\n",
    "        self.df_global_parameters = df_global_parameters\n",
    "        self.df_global_predictions_since_today = df_global_predictions_since_today\n",
    "        self.df_global_predictions_since_100_cases = (\n",
    "            df_global_predictions_since_100_cases\n",
    "        )\n",
    "        self.logger = logger\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataframe(df, path, logger):\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        filename = path\n",
    "        while attempt <= 5 and not success:\n",
    "            success = True\n",
    "            attempt+=1\n",
    "            try:\n",
    "                df.to_csv(filename, index=False)\n",
    "            except OSError:\n",
    "                success = False\n",
    "                filename = path.replace(\".csv\", f\"_try_{attempt}.csv\")\n",
    "\n",
    "        if not success:\n",
    "            logger.error(\n",
    "                f\"Unable to save file {path}, skipping after {attempt} attempts\"\n",
    "            )\n",
    "            return attempt\n",
    "        return 0\n",
    "\n",
    "    def save_all_datasets(\n",
    "            self, optimizer: str, save_since_100_cases: bool = False, website: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves the parameters and predictions datasets (since 100 cases and since the day of running)\n",
    "        based on the different flags and the inputs to the DELPHIDataSaver initializer\n",
    "        :param optimizer: needs to be in (tnc, trust-constr, annealing) and will save files differently accordingly; the\n",
    "        default name corresponds to tnc where we don't specify the optimizer because that's the default one\n",
    "        :param save_since_100_cases: boolean, whether or not we also want to save the predictions since 100 cases\n",
    "        for all the areas (instead of since the day we actually ran the optimization)\n",
    "        :param website: boolean, whether or not we want to save the files in the website repository as well\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        today_date_str = \"\".join(str(datetime.now().date()).split(\"-\"))\n",
    "        if optimizer == \"tnc\":\n",
    "            subname_file = \"Global_V4\"\n",
    "        elif optimizer == \"annealing\":\n",
    "            subname_file = \"Global_V4_annealing\"\n",
    "        elif optimizer == \"trust-constr\":\n",
    "            subname_file = \"Global_V4_trust\"\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer not supported in this implementation\")\n",
    "        # Save parameters\n",
    "\n",
    "        DELPHIDataSaver.save_dataframe(\n",
    "            self.df_global_parameters,\n",
    "            self.PATH_TO_FOLDER_DANGER_MAP + f\"/predicted/Parameters_{subname_file}_{today_date_str}.csv\",\n",
    "            self.logger\n",
    "            )\n",
    "        # Save predictions since today\n",
    "        DELPHIDataSaver.save_dataframe(\n",
    "            self.df_global_predictions_since_today,\n",
    "            self.PATH_TO_FOLDER_DANGER_MAP + f\"/predicted/{subname_file}_{today_date_str}.csv\",\n",
    "            self.logger\n",
    "            )\n",
    "        if website:\n",
    "            DELPHIDataSaver.save_dataframe(\n",
    "                self.df_global_parameters,\n",
    "                self.PATH_TO_WEBSITE_PREDICTED + f\"data/predicted/Parameters_{subname_file}_{today_date_str}.csv\",\n",
    "                self.logger\n",
    "                )\n",
    "            DELPHIDataSaver.save_dataframe(\n",
    "                self.df_global_predictions_since_today,\n",
    "                self.PATH_TO_WEBSITE_PREDICTED\n",
    "                + f\"data/predicted/{subname_file}_{today_date_str}.csv\",\n",
    "                self.logger\n",
    "                )\n",
    "            DELPHIDataSaver.save_dataframe(\n",
    "                self.df_global_predictions_since_today,\n",
    "                self.PATH_TO_WEBSITE_PREDICTED + f\"data/predicted/Global.csv\",\n",
    "                self.logger\n",
    "                )\n",
    "        if save_since_100_cases:\n",
    "            # Save predictions since 100 cases\n",
    "            DELPHIDataSaver.save_dataframe(\n",
    "                self.df_global_predictions_since_100_cases,\n",
    "                self.PATH_TO_FOLDER_DANGER_MAP + f\"/predicted/{subname_file}_since100_{today_date_str}.csv\",\n",
    "                self.logger\n",
    "                )\n",
    "            if website:\n",
    "                DELPHIDataSaver.save_dataframe(\n",
    "                    self.df_global_predictions_since_100_cases,\n",
    "                    self.PATH_TO_WEBSITE_PREDICTED + f\"data/predicted/{subname_file}_since100_{today_date_str}.csv\",\n",
    "                    self.logger\n",
    "                    )\n",
    "                DELPHIDataSaver.save_dataframe(\n",
    "                    self.df_global_predictions_since_100_cases,\n",
    "                    self.PATH_TO_WEBSITE_PREDICTED + f\"data/predicted/{subname_file}_since100.csv\",\n",
    "                    self.logger\n",
    "                    )\n",
    "\n",
    "    def save_policy_predictions_to_json(self, website: bool = False, local_delphi: bool = False):\n",
    "        \"\"\"\n",
    "        Saves the policy predictions as a JSON file based on the different flags\n",
    "        :param website: boolean, whether or not we want to save the JSON file in the website repository as well\n",
    "        :param local_delphi: boolean, whether or not we want to save the JSON file in the DELPHI repository as well\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        today_date_str = \"\".join(str(datetime.now().date()).split(\"-\"))\n",
    "        dict_predictions_policies_world_since_100_cases = DELPHIDataSaver.create_nested_dict_from_final_dataframe(\n",
    "            self.df_global_predictions_since_100_cases\n",
    "        )\n",
    "        with open(\n",
    "                self.PATH_TO_FOLDER_DANGER_MAP\n",
    "                + f\"/predicted/world_Python_{today_date_str}_Scenarios_since_100_cases.json\",\n",
    "                \"w\",\n",
    "        ) as handle:\n",
    "            json.dump(dict_predictions_policies_world_since_100_cases, handle)\n",
    "\n",
    "        with open(\n",
    "                self.PATH_TO_FOLDER_DANGER_MAP\n",
    "                + f\"/predicted/world_Python_Scenarios_since_100_cases.json\",\n",
    "                \"w\",\n",
    "        ) as handle:\n",
    "            json.dump(dict_predictions_policies_world_since_100_cases, handle)\n",
    "\n",
    "        if local_delphi:\n",
    "            with open(\n",
    "                    f\"./world_Python_{today_date_str}_Scenarios_since_100_cases.json\", \"w\"\n",
    "            ) as handle:\n",
    "                json.dump(dict_predictions_policies_world_since_100_cases, handle)\n",
    "\n",
    "        if website:\n",
    "            with open(\n",
    "                    self.PATH_TO_WEBSITE_PREDICTED\n",
    "                    + f\"assets/policies/World_Scenarios.json\",\n",
    "                    \"w\",\n",
    "            ) as handle:\n",
    "                json.dump(dict_predictions_policies_world_since_100_cases, handle)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_nested_dict_from_final_dataframe(df_predictions: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Generates the nested dictionary with all the policy predictions which will then be saved as a JSON file\n",
    "        to be used on the website\n",
    "        :param df_predictions: dataframe with all policy predictions\n",
    "        :return: dictionary with nested keys and policy predictions to be saved as a JSON file\n",
    "        \"\"\"\n",
    "        dict_all_results = {\n",
    "            continent: {} for continent in df_predictions.Continent.unique()\n",
    "        }\n",
    "        for continent in dict_all_results.keys():\n",
    "            countries_in_continent = list(\n",
    "                df_predictions[df_predictions.Continent == continent].Country.unique()\n",
    "            )\n",
    "            dict_all_results[continent] = {\n",
    "                country: {} for country in countries_in_continent\n",
    "            }\n",
    "\n",
    "        keys_country_province = list(\n",
    "            set(\n",
    "                [\n",
    "                    (continent, country, province)\n",
    "                    for continent, country, province in zip(\n",
    "                    df_predictions.Continent.tolist(),\n",
    "                    df_predictions.Country.tolist(),\n",
    "                    df_predictions.Province.tolist(),\n",
    "                )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for continent, country, province in keys_country_province:\n",
    "            df_predictions_province = df_predictions[\n",
    "                (df_predictions.Country == country)\n",
    "                & (df_predictions.Province == province)\n",
    "                ].reset_index(drop=True)\n",
    "            # The first part contains only ground truth value, so it doesn't matter which\n",
    "            # policy/enaction time we choose to report these values\n",
    "            dict_all_results[continent][country][province] = {\n",
    "                \"Day\": sorted(list(df_predictions_province.Day.unique())),\n",
    "                \"Total Detected True\": df_predictions_province[\n",
    "                    (df_predictions_province.Policy == default_policy)\n",
    "                    & (df_predictions_province.Time == default_policy_enaction_time)\n",
    "                    ]\n",
    "                    .sort_values(\"Day\")[\"Total Detected True\"]\n",
    "                    .tolist(),\n",
    "                \"Total Detected Deaths True\": df_predictions_province[\n",
    "                    (df_predictions_province.Policy == default_policy)\n",
    "                    & (df_predictions_province.Time == default_policy_enaction_time)\n",
    "                    ]\n",
    "                    .sort_values(\"Day\")[\"Total Detected Deaths True\"]\n",
    "                    .tolist(),\n",
    "            }\n",
    "            dict_all_results[continent][country][province].update(\n",
    "                {\n",
    "                    policy: {\n",
    "                        policy_enaction_time: {\n",
    "                            \"Total Detected\": df_predictions_province[\n",
    "                                (df_predictions_province.Policy == policy)\n",
    "                                & (df_predictions_province.Time == policy_enaction_time)\n",
    "                                ]\n",
    "                                .sort_values(\"Day\")[\"Total Detected\"]\n",
    "                                .tolist(),\n",
    "                            \"Total Detected Deaths\": df_predictions_province[\n",
    "                                (df_predictions_province.Policy == policy)\n",
    "                                & (df_predictions_province.Time == policy_enaction_time)\n",
    "                                ]\n",
    "                                .sort_values(\"Day\")[\"Total Detected Deaths\"]\n",
    "                                .tolist(),\n",
    "                        }\n",
    "                        for policy_enaction_time in df_predictions_province.Time.unique()\n",
    "                    }\n",
    "                    for policy in df_predictions_province.Policy.unique()\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return dict_all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELPHIDataCreator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            x_sol_final: np.array,\n",
    "            date_day_since100: datetime,\n",
    "            best_params: np.array,\n",
    "            continent: str,\n",
    "            country: str,\n",
    "            province: str,\n",
    "            testing_data_included: bool = False,\n",
    "    ):\n",
    "        if testing_data_included:\n",
    "            assert (\n",
    "                    len(best_params) == 15\n",
    "            ), f\"Expected 9 best parameters, got {len(best_params)}\"\n",
    "        else:\n",
    "            assert (\n",
    "                    len(best_params) == 12\n",
    "            ), f\"Expected 7 best parameters, got {len(best_params)}\"\n",
    "        self.x_sol_final = x_sol_final\n",
    "        self.date_day_since100 = date_day_since100\n",
    "        self.best_params = best_params\n",
    "        self.continent = continent\n",
    "        self.country = country\n",
    "        self.province = province\n",
    "        self.testing_data_included = testing_data_included\n",
    "\n",
    "    def create_dataset_parameters(self, mape: float) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates the parameters dataset with the results from the optimization and the pre-computed MAPE\n",
    "        :param mape: MAPE on the last 15 days (or less if less historical days available) for that particular area\n",
    "        :return: dataframe with parameters and MAPE\n",
    "        \"\"\"\n",
    "        if self.testing_data_included:\n",
    "            print(\n",
    "                f\"Parameters dataset created without the testing data parameters\"\n",
    "                + \" beta_0, beta_1: code will have to be modified\"\n",
    "            )\n",
    "        df_parameters = pd.DataFrame(\n",
    "            {\n",
    "                \"Continent\": [self.continent],\n",
    "                \"Country\": [self.country],\n",
    "                \"Province\": [self.province],\n",
    "                \"Data Start Date\": [self.date_day_since100],\n",
    "                \"MAPE\": [mape],\n",
    "                \"Infection Rate\": [self.best_params[0]],\n",
    "                \"Median Day of Action\": [self.best_params[1]],\n",
    "                \"Rate of Action\": [self.best_params[2]],\n",
    "                \"Rate of Death\": [self.best_params[3]],\n",
    "                \"Mortality Rate\": [self.best_params[4]],\n",
    "                \"Rate of Mortality Rate Decay\": [self.best_params[5]],\n",
    "                \"Internal Parameter 1\": [self.best_params[6]],\n",
    "                \"Internal Parameter 2\": [self.best_params[7]],\n",
    "                \"Jump Magnitude\": [self.best_params[8]],\n",
    "                \"Jump Time\": [self.best_params[9]],\n",
    "                \"Jump Decay\": [self.best_params[10]],\n",
    "                \"Internal Parameter 3\": [self.best_params[11]],\n",
    "            }\n",
    "        )\n",
    "        return df_parameters\n",
    "\n",
    "    def create_datasets_predictions(self) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Creates two dataframes with the predictions of the DELPHI model, the first one since the day of the prediction,\n",
    "        the second since the day the area had 100 cases\n",
    "        :return: tuple of dataframes with predictions from DELPHI model\n",
    "        \"\"\"\n",
    "        n_days_btw_today_since_100 = (datetime.now() - self.date_day_since100).days\n",
    "        n_days_since_today = self.x_sol_final.shape[1] - n_days_btw_today_since_100\n",
    "        all_dates_since_today = [\n",
    "            str((datetime.now() + timedelta(days=i)).date())\n",
    "            for i in range(n_days_since_today)\n",
    "        ]\n",
    "        # Predictions\n",
    "        total_detected = self.x_sol_final[15, :]  # DT\n",
    "        total_detected = [int(round(x, 0)) for x in total_detected]\n",
    "        active_cases = (\n",
    "                self.x_sol_final[4, :]\n",
    "                + self.x_sol_final[5, :]\n",
    "                + self.x_sol_final[7, :]\n",
    "                + self.x_sol_final[8, :]\n",
    "        )  # DHR + DQR + DHD + DQD\n",
    "        active_cases = [int(round(x, 0)) for x in active_cases]\n",
    "        active_hospitalized = (\n",
    "                self.x_sol_final[4, :] + self.x_sol_final[7, :]\n",
    "        )  # DHR + DHD\n",
    "        active_hospitalized = [int(round(x, 0)) for x in active_hospitalized]\n",
    "        cumulative_hospitalized = self.x_sol_final[11, :]  # TH\n",
    "        cumulative_hospitalized = [int(round(x, 0)) for x in cumulative_hospitalized]\n",
    "        total_detected_deaths = self.x_sol_final[14, :]  # DD\n",
    "        total_detected_deaths = [int(round(x, 0)) for x in total_detected_deaths]\n",
    "        active_ventilated = (\n",
    "                self.x_sol_final[12, :] + self.x_sol_final[13, :]\n",
    "        )  # DVR + DVD\n",
    "        active_ventilated = [int(round(x, 0)) for x in active_ventilated]\n",
    "        # Generation of the dataframe since today\n",
    "        df_predictions_since_today_cont_country_prov = pd.DataFrame(\n",
    "            {\n",
    "                \"Continent\": [self.continent for _ in range(n_days_since_today)],\n",
    "                \"Country\": [self.country for _ in range(n_days_since_today)],\n",
    "                \"Province\": [self.province for _ in range(n_days_since_today)],\n",
    "                \"Day\": all_dates_since_today,\n",
    "                \"Total Detected\": total_detected[n_days_btw_today_since_100:],\n",
    "                \"Active\": active_cases[n_days_btw_today_since_100:],\n",
    "                \"Active Hospitalized\": active_hospitalized[n_days_btw_today_since_100:],\n",
    "                \"Cumulative Hospitalized\": cumulative_hospitalized[\n",
    "                                           n_days_btw_today_since_100:\n",
    "                                           ],\n",
    "                \"Total Detected Deaths\": total_detected_deaths[\n",
    "                                         n_days_btw_today_since_100:\n",
    "                                         ],\n",
    "                \"Active Ventilated\": active_ventilated[n_days_btw_today_since_100:],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Generation of the dataframe from the day since 100th case\n",
    "        all_dates_since_100 = [\n",
    "            str((self.date_day_since100 + timedelta(days=i)).date())\n",
    "            for i in range(self.x_sol_final.shape[1])\n",
    "        ]\n",
    "        df_predictions_since_100_cont_country_prov = pd.DataFrame(\n",
    "            {\n",
    "                \"Continent\": [self.continent for _ in range(len(all_dates_since_100))],\n",
    "                \"Country\": [self.country for _ in range(len(all_dates_since_100))],\n",
    "                \"Province\": [self.province for _ in range(len(all_dates_since_100))],\n",
    "                \"Day\": all_dates_since_100,\n",
    "                \"Total Detected\": total_detected,\n",
    "                \"Active\": active_cases,\n",
    "                \"Active Hospitalized\": active_hospitalized,\n",
    "                \"Cumulative Hospitalized\": cumulative_hospitalized,\n",
    "                \"Total Detected Deaths\": total_detected_deaths,\n",
    "                \"Active Ventilated\": active_ventilated,\n",
    "            }\n",
    "        )\n",
    "        return (\n",
    "            df_predictions_since_today_cont_country_prov,\n",
    "            df_predictions_since_100_cont_country_prov,\n",
    "        )\n",
    "\n",
    "    def create_datasets_raw(self) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Creates a dataset in the right format (with values for all 16 states of the DELPHI model)\n",
    "        for the Optimal Vaccine Allocation team\n",
    "        \"\"\"\n",
    "        n_days_btw_today_since_100 = (datetime.now() - self.date_day_since100).days\n",
    "        n_days_since_today = self.x_sol_final.shape[1] - n_days_btw_today_since_100\n",
    "        all_dates_since_today = [\n",
    "            str((datetime.now() + timedelta(days=i)).date())\n",
    "            for i in range(n_days_since_today)\n",
    "        ]\n",
    "\n",
    "        df_predictions_since_today_cont_country_prov = pd.DataFrame(\n",
    "            {\n",
    "                \"Continent\": [self.continent for _ in range(n_days_since_today)],\n",
    "                \"Country\": [self.country for _ in range(n_days_since_today)],\n",
    "                \"Province\": [self.province for _ in range(n_days_since_today)],\n",
    "                \"Day\": all_dates_since_today,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        intr_since_today = pd.DataFrame(\n",
    "            self.x_sol_final[:, n_days_btw_today_since_100:].transpose()\n",
    "        )\n",
    "        intr_since_today.columns = [\n",
    "            \"S\",\n",
    "            \"E\",\n",
    "            \"I\",\n",
    "            \"AR\",\n",
    "            \"DHR\",\n",
    "            \"DQR\",\n",
    "            \"AD\",\n",
    "            \"DHD\",\n",
    "            \"DQD\",\n",
    "            \"R\",\n",
    "            \"D\",\n",
    "            \"TH\",\n",
    "            \"DVR\",\n",
    "            \"DVD\",\n",
    "            \"DD\",\n",
    "            \"DT\",\n",
    "        ]\n",
    "        df_predictions_since_today_cont_country_prov = pd.concat(\n",
    "            [df_predictions_since_today_cont_country_prov, intr_since_today], axis=1\n",
    "        )\n",
    "        # Generation of the dataframe from the day since 100th case\n",
    "        all_dates_since_100 = [\n",
    "            str((self.date_day_since100 + timedelta(days=i)).date())\n",
    "            for i in range(self.x_sol_final.shape[1])\n",
    "        ]\n",
    "        df_predictions_since_100_cont_country_prov = pd.DataFrame(\n",
    "            {\n",
    "                \"Continent\": [self.continent for _ in range(len(all_dates_since_100))],\n",
    "                \"Country\": [self.country for _ in range(len(all_dates_since_100))],\n",
    "                \"Province\": [self.province for _ in range(len(all_dates_since_100))],\n",
    "                \"Day\": all_dates_since_100,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        intr_since_100 = pd.DataFrame(self.x_sol_final.transpose())\n",
    "\n",
    "        intr_since_100.columns = [\n",
    "            \"S\",\n",
    "            \"E\",\n",
    "            \"I\",\n",
    "            \"AR\",\n",
    "            \"DHR\",\n",
    "            \"DQR\",\n",
    "            \"AD\",\n",
    "            \"DHD\",\n",
    "            \"DQD\",\n",
    "            \"R\",\n",
    "            \"D\",\n",
    "            \"TH\",\n",
    "            \"DVR\",\n",
    "            \"DVD\",\n",
    "            \"DD\",\n",
    "            \"DT\",\n",
    "        ]\n",
    "\n",
    "        df_predictions_since_100_cont_country_prov = pd.concat(\n",
    "            [df_predictions_since_100_cont_country_prov, intr_since_100], axis=1\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            df_predictions_since_today_cont_country_prov,\n",
    "            df_predictions_since_100_cont_country_prov,\n",
    "        )\n",
    "\n",
    "    def create_datasets_with_confidence_intervals(\n",
    "            self,\n",
    "            cases_data_fit: list,\n",
    "            deaths_data_fit: list,\n",
    "            past_prediction_file: str = \"I://covid19orc//danger_map//predicted//Global_V2_20200720.csv\",\n",
    "            past_prediction_date: str = \"2020-07-04\",\n",
    "            q: float = 0.5,\n",
    "    ) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Generates the prediction datasets from the date with 100 cases and from the day of running, including columns\n",
    "        containing Confidence Intervals used in the website for cases and deaths\n",
    "        :param cases_data_fit: list, contains data used to fit on number of cases\n",
    "        :param deaths_data_fit: list, contains data used to fit on number of deaths\n",
    "        :param past_prediction_file: past prediction file's path for CI generation\n",
    "        :param past_prediction_date: past prediction's date for CI generation\n",
    "        :param q: quantile used for the CIs\n",
    "        :return: tuple of dataframes (since day of optimization & since 100 cases in the area) with predictions and\n",
    "        confidence intervals\n",
    "        \"\"\"\n",
    "        n_days_btw_today_since_100 = (datetime.now() - self.date_day_since100).days\n",
    "        n_days_since_today = self.x_sol_final.shape[1] - n_days_btw_today_since_100\n",
    "        all_dates_since_today = [\n",
    "            str((datetime.now() + timedelta(days=i)).date())\n",
    "            for i in range(n_days_since_today)\n",
    "        ]\n",
    "        # Predictions\n",
    "        total_detected = self.x_sol_final[15, :]  # DT\n",
    "        total_detected = [int(round(x, 0)) for x in total_detected]\n",
    "        active_cases = (\n",
    "                self.x_sol_final[4, :]\n",
    "                + self.x_sol_final[5, :]\n",
    "                + self.x_sol_final[7, :]\n",
    "                + self.x_sol_final[8, :]\n",
    "        )  # DHR + DQR + DHD + DQD\n",
    "        active_cases = [int(round(x, 0)) for x in active_cases]\n",
    "        active_hospitalized = (\n",
    "                self.x_sol_final[4, :] + self.x_sol_final[7, :]\n",
    "        )  # DHR + DHD\n",
    "        active_hospitalized = [int(round(x, 0)) for x in active_hospitalized]\n",
    "        cumulative_hospitalized = self.x_sol_final[11, :]  # TH\n",
    "        cumulative_hospitalized = [int(round(x, 0)) for x in cumulative_hospitalized]\n",
    "        total_detected_deaths = self.x_sol_final[14, :]  # DD\n",
    "        total_detected_deaths = [int(round(x, 0)) for x in total_detected_deaths]\n",
    "        active_ventilated = (\n",
    "                self.x_sol_final[12, :] + self.x_sol_final[13, :]\n",
    "        )  # DVR + DVD\n",
    "        active_ventilated = [int(round(x, 0)) for x in active_ventilated]\n",
    "\n",
    "        past_predictions = pd.read_csv(past_prediction_file)\n",
    "        past_predictions = (\n",
    "            past_predictions[\n",
    "                (past_predictions[\"Day\"] > past_prediction_date)\n",
    "                & (past_predictions[\"Country\"] == self.country)\n",
    "                & (past_predictions[\"Province\"] == self.province)\n",
    "                ]\n",
    "        ).sort_values(\"Day\")\n",
    "        if len(past_predictions) > 0:\n",
    "            known_dates_since_100 = [\n",
    "                str((self.date_day_since100 + timedelta(days=i)).date())\n",
    "                for i in range(len(cases_data_fit))\n",
    "            ]\n",
    "            cases_data_fit_past = [\n",
    "                y\n",
    "                for x, y in zip(known_dates_since_100, cases_data_fit)\n",
    "                if x > past_prediction_date\n",
    "            ]\n",
    "            deaths_data_fit_past = [\n",
    "                y\n",
    "                for x, y in zip(known_dates_since_100, deaths_data_fit)\n",
    "                if x > past_prediction_date\n",
    "            ]\n",
    "            total_detected_past = past_predictions[\"Total Detected\"].values[\n",
    "                                  : len(cases_data_fit_past)\n",
    "                                  ]\n",
    "            total_detected_deaths_past = past_predictions[\n",
    "                                             \"Total Detected Deaths\"\n",
    "                                         ].values[: len(deaths_data_fit_past)]\n",
    "            residual_cases_lb = np.sqrt(\n",
    "                np.mean(\n",
    "                    [(x - y) ** 2 for x, y in zip(cases_data_fit_past, total_detected_past)]\n",
    "                )\n",
    "            ) * scipy.stats.norm.ppf(0.5 - q / 2)\n",
    "            residual_cases_ub = np.sqrt(\n",
    "                np.mean(\n",
    "                    [(x - y) ** 2 for x, y in zip(cases_data_fit_past, total_detected_past)]\n",
    "                )\n",
    "            ) * scipy.stats.norm.ppf(0.5 + q / 2)\n",
    "            residual_deaths_lb = np.sqrt(\n",
    "                np.mean(\n",
    "                    [\n",
    "                        (x - y) ** 2\n",
    "                        for x, y in zip(deaths_data_fit_past, total_detected_deaths_past)\n",
    "                    ]\n",
    "                )\n",
    "            ) * scipy.stats.norm.ppf(0.5 - q / 2)\n",
    "            residual_deaths_ub = np.sqrt(\n",
    "                np.mean(\n",
    "                    [\n",
    "                        (x - y) ** 2\n",
    "                        for x, y in zip(deaths_data_fit_past, total_detected_deaths_past)\n",
    "                    ]\n",
    "                )\n",
    "            ) * scipy.stats.norm.ppf(0.5 + q / 2)\n",
    "\n",
    "            # Generation of the dataframe since today\n",
    "            df_predictions_since_today_cont_country_prov = pd.DataFrame(\n",
    "                {\n",
    "                    \"Continent\": [self.continent for _ in range(n_days_since_today)],\n",
    "                    \"Country\": [self.country for _ in range(n_days_since_today)],\n",
    "                    \"Province\": [self.province for _ in range(n_days_since_today)],\n",
    "                    \"Day\": all_dates_since_today,\n",
    "                    \"Total Detected\": total_detected[n_days_btw_today_since_100:],\n",
    "                    \"Active\": active_cases[n_days_btw_today_since_100:],\n",
    "                    \"Active Hospitalized\": active_hospitalized[\n",
    "                                           n_days_btw_today_since_100:\n",
    "                                           ],\n",
    "                    \"Cumulative Hospitalized\": cumulative_hospitalized[\n",
    "                                               n_days_btw_today_since_100:\n",
    "                                               ],\n",
    "                    \"Total Detected Deaths\": total_detected_deaths[\n",
    "                                             n_days_btw_today_since_100:\n",
    "                                             ],\n",
    "                    \"Active Ventilated\": active_ventilated[n_days_btw_today_since_100:],\n",
    "                    \"Total Detected True\": [np.nan for _ in range(n_days_since_today)],\n",
    "                    \"Total Detected Deaths True\": [\n",
    "                        np.nan for _ in range(n_days_since_today)\n",
    "                    ],\n",
    "                    \"Total Detected LB\": make_increasing([\n",
    "                        max(int(round(v + residual_cases_lb * np.sqrt(c), 0)), 0)\n",
    "                        for c, v in enumerate(\n",
    "                            total_detected[n_days_btw_today_since_100:]\n",
    "                        )\n",
    "                    ]),\n",
    "#                    \"Active LB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_lb * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                active_cases[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "#                    \"Active Hospitalized LB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_lb * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                active_hospitalized[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized LB\": make_increasing([\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_lb * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                cumulative_hospitalized[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ]),\n",
    "                    \"Total Detected Deaths LB\": make_increasing([\n",
    "                        max(int(round(v + residual_deaths_lb * np.sqrt(c), 0)), 0)\n",
    "                        for c, v in enumerate(\n",
    "                            total_detected_deaths[n_days_btw_today_since_100:]\n",
    "                        )\n",
    "                    ]),\n",
    "#                    \"Active Ventilated LB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_lb * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                active_ventilated[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "                    \"Total Detected UB\": [\n",
    "                        max(int(round(v + residual_cases_ub * np.sqrt(c), 0)), 0)\n",
    "                        for c, v in enumerate(\n",
    "                            total_detected[n_days_btw_today_since_100:]\n",
    "                        )\n",
    "                    ],\n",
    "#                    \"Active UB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_ub * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                active_cases[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "#                    \"Active Hospitalized UB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_ub * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                active_hospitalized[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized UB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_ub * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                cumulative_hospitalized[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "                    \"Total Detected Deaths UB\": [\n",
    "                        max(int(round(v + residual_deaths_ub * np.sqrt(c), 0)), 0)\n",
    "                        for c, v in enumerate(\n",
    "                            total_detected_deaths[n_days_btw_today_since_100:]\n",
    "                        )\n",
    "                    ],\n",
    "#                    \"Active Ventilated UB\": [\n",
    "#                        max(\n",
    "#                            int(round(v + residual_cases_ub * np.sqrt(c) * v / u, 0)), 0\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(\n",
    "#                                active_ventilated[n_days_btw_today_since_100:],\n",
    "#                                total_detected[n_days_btw_today_since_100:],\n",
    "#                            )\n",
    "#                        )\n",
    "#                    ],\n",
    "                }\n",
    "            )\n",
    "            # Generation of the dataframe from the day since 100th case\n",
    "            all_dates_since_100 = [\n",
    "                str((self.date_day_since100 + timedelta(days=i)).date())\n",
    "                for i in range(self.x_sol_final.shape[1])\n",
    "            ]\n",
    "            df_predictions_since_100_cont_country_prov = pd.DataFrame(\n",
    "                {\n",
    "                    \"Continent\": [\n",
    "                        self.continent for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "                    \"Country\": [self.country for _ in range(len(all_dates_since_100))],\n",
    "                    \"Province\": [\n",
    "                        self.province for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "                    \"Day\": all_dates_since_100,\n",
    "                    \"Total Detected\": total_detected,\n",
    "                    \"Active\": active_cases,\n",
    "                    \"Active Hospitalized\": active_hospitalized,\n",
    "                    \"Cumulative Hospitalized\": cumulative_hospitalized,\n",
    "                    \"Total Detected Deaths\": total_detected_deaths,\n",
    "                    \"Active Ventilated\": active_ventilated,\n",
    "                    \"Total Detected True\": cases_data_fit\n",
    "                                           + [\n",
    "                                               np.nan\n",
    "                                               for _ in range(len(all_dates_since_100) - len(cases_data_fit))\n",
    "                                           ],\n",
    "                    \"Total Detected Deaths True\": deaths_data_fit\n",
    "                                                  + [\n",
    "                                                      np.nan for _ in range(len(all_dates_since_100) - len(deaths_data_fit))\n",
    "                                                  ],\n",
    "                    \"Total Detected LB\": make_increasing([\n",
    "                        max(\n",
    "                            int(\n",
    "                                round(\n",
    "                                    v\n",
    "                                    + residual_cases_lb\n",
    "                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0)),\n",
    "                                    0,\n",
    "                                    )\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                        for c, v in enumerate(total_detected)\n",
    "                    ]),\n",
    "#                    \"Active LB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_lb\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(zip(active_cases, total_detected))\n",
    "#                    ],\n",
    "#                    \"Active Hospitalized LB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_lb\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(active_hospitalized, total_detected)\n",
    "#                        )\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized LB\": make_increasing([\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_lb\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(cumulative_hospitalized, total_detected)\n",
    "#                        )\n",
    "#                    ]),\n",
    "                    \"Total Detected Deaths LB\": make_increasing([\n",
    "                        max(\n",
    "                            int(\n",
    "                                round(\n",
    "                                    v\n",
    "                                    + residual_deaths_lb\n",
    "                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0)),\n",
    "                                    0,\n",
    "                                    )\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                        for c, v in enumerate(total_detected_deaths)\n",
    "                    ]),\n",
    "#                    \"Active Ventilated LB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_lb\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(active_ventilated, total_detected)\n",
    "#                        )\n",
    "#                    ],\n",
    "                    \"Total Detected UB\": [\n",
    "                        max(\n",
    "                            int(\n",
    "                                round(\n",
    "                                    v\n",
    "                                    + residual_cases_ub\n",
    "                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0)),\n",
    "                                    0,\n",
    "                                    )\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                        for c, v in enumerate(total_detected)\n",
    "                    ],\n",
    "#                    \"Active UB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_ub\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(zip(active_cases, total_detected))\n",
    "#                    ],\n",
    "#                    \"Active Hospitalized UB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_ub\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(active_hospitalized, total_detected)\n",
    "#                        )\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized UB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_ub\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(cumulative_hospitalized, total_detected)\n",
    "#                        )\n",
    "#                    ],\n",
    "                    \"Total Detected Deaths UB\": [\n",
    "                        max(\n",
    "                            int(\n",
    "                                round(\n",
    "                                    v\n",
    "                                    + residual_deaths_ub\n",
    "                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0)),\n",
    "                                    0,\n",
    "                                    )\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                        for c, v in enumerate(total_detected_deaths)\n",
    "                    ],\n",
    "#                    \"Active Ventilated UB\": [\n",
    "#                        max(\n",
    "#                            int(\n",
    "#                                round(\n",
    "#                                    v\n",
    "#                                    + residual_cases_ub\n",
    "#                                    * np.sqrt(max(c - n_days_btw_today_since_100, 0))\n",
    "#                                    * v\n",
    "#                                    / u,\n",
    "#                                    0,\n",
    "#                                    )\n",
    "#                            ),\n",
    "#                            0,\n",
    "#                        )\n",
    "#                        for c, (v, u) in enumerate(\n",
    "#                            zip(active_ventilated, total_detected)\n",
    "#                        )\n",
    "#                    ],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            df_predictions_since_today_cont_country_prov = pd.DataFrame(\n",
    "                {\n",
    "                    \"Continent\": [self.continent for _ in range(n_days_since_today)],\n",
    "                    \"Country\": [self.country for _ in range(n_days_since_today)],\n",
    "                    \"Province\": [self.province for _ in range(n_days_since_today)],\n",
    "                    \"Day\": all_dates_since_today,\n",
    "                    \"Total Detected\": total_detected[n_days_btw_today_since_100:],\n",
    "                    \"Active\": active_cases[n_days_btw_today_since_100:],\n",
    "                    \"Active Hospitalized\": active_hospitalized[\n",
    "                                           n_days_btw_today_since_100:\n",
    "                                           ],\n",
    "                    \"Cumulative Hospitalized\": cumulative_hospitalized[\n",
    "                                               n_days_btw_today_since_100:\n",
    "                                               ],\n",
    "                    \"Total Detected Deaths\": total_detected_deaths[\n",
    "                                             n_days_btw_today_since_100:\n",
    "                                             ],\n",
    "                    \"Active Ventilated\": active_ventilated[n_days_btw_today_since_100:],\n",
    "                    \"Total Detected True\": [np.nan for _ in range(n_days_since_today)],\n",
    "                    \"Total Detected Deaths True\": [\n",
    "                        np.nan for _ in range(n_days_since_today)\n",
    "                    ],\n",
    "                    \"Total Detected LB\": [np.nan for _ in range(n_days_since_today)],\n",
    "#                    \"Active LB\": [np.nan for _ in range(n_days_since_today)],\n",
    "#                    \"Active Hospitalized LB\": [\n",
    "#                        np.nan for _ in range(n_days_since_today)\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized LB\": [\n",
    "#                        np.nan for _ in range(n_days_since_today)\n",
    "#                    ],\n",
    "                    \"Total Detected Deaths LB\": [\n",
    "                        np.nan for _ in range(n_days_since_today)\n",
    "                    ],\n",
    "#                    \"Active Ventilated LB\": [np.nan for _ in range(n_days_since_today)],\n",
    "                    \"Total Detected UB\": [np.nan for _ in range(n_days_since_today)],\n",
    "#                    \"Active UB\": [np.nan for _ in range(n_days_since_today)],\n",
    "#                    \"Active Hospitalized UB\": [\n",
    "#                        np.nan for _ in range(n_days_since_today)\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized UB\": [\n",
    "#                        np.nan for _ in range(n_days_since_today)\n",
    "#                    ],\n",
    "                    \"Total Detected Deaths UB\": [\n",
    "                        np.nan for _ in range(n_days_since_today)\n",
    "                    ]\n",
    "#                    \"Active Ventilated UB\": [np.nan for _ in range(n_days_since_today)],\n",
    "                }\n",
    "            )\n",
    "            # Generation of the dataframe from the day since 100th case\n",
    "            all_dates_since_100 = [\n",
    "                str((self.date_day_since100 + timedelta(days=i)).date())\n",
    "                for i in range(self.x_sol_final.shape[1])\n",
    "            ]\n",
    "            df_predictions_since_100_cont_country_prov = pd.DataFrame(\n",
    "                {\n",
    "                    \"Continent\": [\n",
    "                        self.continent for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "                    \"Country\": [self.country for _ in range(len(all_dates_since_100))],\n",
    "                    \"Province\": [\n",
    "                        self.province for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "                    \"Day\": all_dates_since_100,\n",
    "                    \"Total Detected\": total_detected,\n",
    "                    \"Active\": active_cases,\n",
    "                    \"Active Hospitalized\": active_hospitalized,\n",
    "                    \"Cumulative Hospitalized\": cumulative_hospitalized,\n",
    "                    \"Total Detected Deaths\": total_detected_deaths,\n",
    "                    \"Active Ventilated\": active_ventilated,\n",
    "                    \"Total Detected True\": cases_data_fit\n",
    "                                           + [\n",
    "                                               np.nan\n",
    "                                               for _ in range(len(all_dates_since_100) - len(cases_data_fit))\n",
    "                                           ],\n",
    "                    \"Total Detected Deaths True\": deaths_data_fit\n",
    "                                                  + [\n",
    "                                                      np.nan for _ in range(len(all_dates_since_100) - len(deaths_data_fit))\n",
    "                                                  ],\n",
    "                    \"Total Detected LB\": [\n",
    "                        np.nan for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "#                    \"Active LB\": [np.nan for _ in range(len(all_dates_since_100))],\n",
    "#                    \"Active Hospitalized LB\": [\n",
    "#                        np.nan for _ in range(len(all_dates_since_100))\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized LB\": [\n",
    "#                        np.nan for _ in range(len(all_dates_since_100))\n",
    "#                    ],\n",
    "                    \"Total Detected Deaths LB\": [\n",
    "                        np.nan for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "#                    \"Active Ventilated LB\": [\n",
    "#                        np.nan for _ in range(len(all_dates_since_100))\n",
    "#                    ],\n",
    "                    \"Total Detected UB\": [\n",
    "                        np.nan for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "#                    \"Active UB\": [np.nan for _ in range(len(all_dates_since_100))],\n",
    "#                    \"Active Hospitalized UB\": [\n",
    "#                        np.nan for _ in range(len(all_dates_since_100))\n",
    "#                    ],\n",
    "#                    \"Cumulative Hospitalized UB\": [\n",
    "#                        np.nan for _ in range(len(all_dates_since_100))\n",
    "#                    ],\n",
    "                    \"Total Detected Deaths UB\": [\n",
    "                        np.nan for _ in range(len(all_dates_since_100))\n",
    "                    ],\n",
    "#                    \"Active Ventilated UB\": [\n",
    "#                        np.nan for _ in range(len(all_dates_since_100))\n",
    "#                    ],\n",
    "                }\n",
    "            )\n",
    "        return (\n",
    "            df_predictions_since_today_cont_country_prov,\n",
    "            df_predictions_since_100_cont_country_prov,\n",
    "        )\n",
    "\n",
    "    def create_datasets_predictions_scenario(\n",
    "            self, policy: str = \"Lockdown\", time: int = 0, totalcases=None\n",
    "    ) -> (pd.DataFrame, pd.DataFrame):\n",
    "        n_days_btw_today_since_100 = (datetime.now() - self.date_day_since100).days\n",
    "        n_days_since_today = self.x_sol_final.shape[1] - n_days_btw_today_since_100\n",
    "        all_dates_since_today = [\n",
    "            str((datetime.now() + timedelta(days=i)).date())\n",
    "            for i in range(n_days_since_today)\n",
    "        ]\n",
    "        # Predictions\n",
    "        total_detected = self.x_sol_final[15, :]  # DT\n",
    "        total_detected = [int(round(x, 0)) for x in total_detected]\n",
    "        active_cases = (\n",
    "                self.x_sol_final[4, :]\n",
    "                + self.x_sol_final[5, :]\n",
    "                + self.x_sol_final[7, :]\n",
    "                + self.x_sol_final[8, :]\n",
    "        )  # DHR + DQR + DHD + DQD\n",
    "        active_cases = [int(round(x, 0)) for x in active_cases]\n",
    "        active_hospitalized = (\n",
    "                self.x_sol_final[4, :] + self.x_sol_final[7, :]\n",
    "        )  # DHR + DHD\n",
    "        active_hospitalized = [int(round(x, 0)) for x in active_hospitalized]\n",
    "        cumulative_hospitalized = self.x_sol_final[11, :]  # TH\n",
    "        cumulative_hospitalized = [int(round(x, 0)) for x in cumulative_hospitalized]\n",
    "        total_detected_deaths = self.x_sol_final[14, :]  # DD\n",
    "        total_detected_deaths = [int(round(x, 0)) for x in total_detected_deaths]\n",
    "        active_ventilated = (\n",
    "                self.x_sol_final[12, :] + self.x_sol_final[13, :]\n",
    "        )  # DVR + DVD\n",
    "        active_ventilated = [int(round(x, 0)) for x in active_ventilated]\n",
    "        # Generation of the dataframe since today\n",
    "        df_predictions_since_today_cont_country_prov = pd.DataFrame(\n",
    "            {\n",
    "                \"Policy\": [policy for _ in range(n_days_since_today)],\n",
    "                \"Time\": [TIME_DICT[time] for _ in range(n_days_since_today)],\n",
    "                \"Continent\": [self.continent for _ in range(n_days_since_today)],\n",
    "                \"Country\": [self.country for _ in range(n_days_since_today)],\n",
    "                \"Province\": [self.province for _ in range(n_days_since_today)],\n",
    "                \"Day\": all_dates_since_today,\n",
    "                \"Total Detected\": total_detected[n_days_btw_today_since_100:],\n",
    "                \"Active\": active_cases[n_days_btw_today_since_100:],\n",
    "                \"Active Hospitalized\": active_hospitalized[n_days_btw_today_since_100:],\n",
    "                \"Cumulative Hospitalized\": cumulative_hospitalized[\n",
    "                                           n_days_btw_today_since_100:\n",
    "                                           ],\n",
    "                \"Total Detected Deaths\": total_detected_deaths[\n",
    "                                         n_days_btw_today_since_100:\n",
    "                                         ],\n",
    "                \"Active Ventilated\": active_ventilated[n_days_btw_today_since_100:],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Generation of the dataframe from the day since 100th case\n",
    "        all_dates_since_100 = [\n",
    "            str((self.date_day_since100 + timedelta(days=i)).date())\n",
    "            for i in range(self.x_sol_final.shape[1])\n",
    "        ]\n",
    "        df_predictions_since_100_cont_country_prov = pd.DataFrame(\n",
    "            {\n",
    "                \"Policy\": [policy for _ in range(len(all_dates_since_100))],\n",
    "                \"Time\": [TIME_DICT[time] for _ in range(len(all_dates_since_100))],\n",
    "                \"Continent\": [self.continent for _ in range(len(all_dates_since_100))],\n",
    "                \"Country\": [self.country for _ in range(len(all_dates_since_100))],\n",
    "                \"Province\": [self.province for _ in range(len(all_dates_since_100))],\n",
    "                \"Day\": all_dates_since_100,\n",
    "                \"Total Detected\": total_detected,\n",
    "                \"Active\": active_cases,\n",
    "                \"Active Hospitalized\": active_hospitalized,\n",
    "                \"Cumulative Hospitalized\": cumulative_hospitalized,\n",
    "                \"Total Detected Deaths\": total_detected_deaths,\n",
    "                \"Active Ventilated\": active_ventilated,\n",
    "            }\n",
    "        )\n",
    "        if (\n",
    "                totalcases is not None\n",
    "        ):  # Merging the historical values to both dataframes when available\n",
    "            df_predictions_since_today_cont_country_prov = df_predictions_since_today_cont_country_prov.merge(\n",
    "                totalcases[\n",
    "                    [\"country\", \"province\", \"date\", \"case_cnt\", \"death_cnt\"]\n",
    "                ].fillna(\"None\"),\n",
    "                left_on=[\"Country\", \"Province\", \"Day\"],\n",
    "                right_on=[\"country\", \"province\", \"date\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            df_predictions_since_today_cont_country_prov.rename(\n",
    "                columns={\n",
    "                    \"case_cnt\": \"Total Detected True\",\n",
    "                    \"death_cnt\": \"Total Detected Deaths True\",\n",
    "                },\n",
    "                inplace=True,\n",
    "            )\n",
    "            df_predictions_since_today_cont_country_prov.drop(\n",
    "                [\"country\", \"province\", \"date\"], axis=1, inplace=True\n",
    "            )\n",
    "            df_predictions_since_100_cont_country_prov = df_predictions_since_100_cont_country_prov.merge(\n",
    "                totalcases[\n",
    "                    [\"country\", \"province\", \"date\", \"case_cnt\", \"death_cnt\"]\n",
    "                ].fillna(\"None\"),\n",
    "                left_on=[\"Country\", \"Province\", \"Day\"],\n",
    "                right_on=[\"country\", \"province\", \"date\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            df_predictions_since_100_cont_country_prov.rename(\n",
    "                columns={\n",
    "                    \"case_cnt\": \"Total Detected True\",\n",
    "                    \"death_cnt\": \"Total Detected Deaths True\",\n",
    "                },\n",
    "                inplace=True,\n",
    "            )\n",
    "            df_predictions_since_100_cont_country_prov.drop(\n",
    "                [\"country\", \"province\", \"date\"], axis=1, inplace=True\n",
    "            )\n",
    "        return (\n",
    "            df_predictions_since_today_cont_country_prov,\n",
    "            df_predictions_since_100_cont_country_prov,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELPHIAggregations:\n",
    "    @staticmethod\n",
    "    def get_aggregation_per_country(df_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregates predictions at the country level from the predictions dataframe\n",
    "        :param df_predictions: DELPHI predictions dataframe\n",
    "        :return: DELPHI predictions dataframe aggregated at the country level\n",
    "        \"\"\"\n",
    "        df_predictions = df_predictions[df_predictions[\"Province\"] != \"None\"]\n",
    "        df_agg_country = df_predictions.groupby([\"Continent\", \"Country\", \"Day\"]).sum().reset_index()\n",
    "        df_agg_country[\"Province\"] = \"None\"\n",
    "        df_agg_country = df_agg_country[df_predictions.columns]\n",
    "        return df_agg_country\n",
    "\n",
    "    @staticmethod\n",
    "    def get_aggregation_per_continent(df_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregates predictions at the continent level from the predictions dataframe\n",
    "        :param df_predictions: DELPHI predictions dataframe\n",
    "        :return: DELPHI predictions dataframe aggregated at the continent level\n",
    "        \"\"\"\n",
    "        df_agg_continent = df_predictions.groupby([\"Continent\", \"Day\"]).sum().reset_index()\n",
    "        df_agg_continent[\"Country\"] = \"None\"\n",
    "        df_agg_continent[\"Province\"] = \"None\"\n",
    "        df_agg_continent = df_agg_continent[df_predictions.columns]\n",
    "        return df_agg_continent\n",
    "\n",
    "    @staticmethod\n",
    "    def get_aggregation_world(df_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregates predictions at the world level from the predictions dataframe\n",
    "        :param df_predictions: DELPHI predictions dataframe\n",
    "        :return: DELPHI predictions dataframe aggregated at the world level (only one row in this dataframe)\n",
    "        \"\"\"\n",
    "        df_agg_world = df_predictions.groupby(\"Day\").sum().reset_index()\n",
    "        df_agg_world[\"Continent\"] = \"None\"\n",
    "        df_agg_world[\"Country\"] = \"None\"\n",
    "        df_agg_world[\"Province\"] = \"None\"\n",
    "        df_agg_world = df_agg_world[df_predictions.columns]\n",
    "        return df_agg_world\n",
    "\n",
    "    @staticmethod\n",
    "    def append_all_aggregations(df_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates and appends all the predictions' aggregations at the country, continent and world levels\n",
    "        :param df_predictions: dataframe with the raw predictions from DELPHI\n",
    "        :return: dataframe with raw predictions from DELPHI and aggregated ones at the country, continent & world levels\n",
    "        \"\"\"\n",
    "        df_agg_since_today_per_country = DELPHIAggregations.get_aggregation_per_country(df_predictions)\n",
    "        df_agg_since_today_per_continent = DELPHIAggregations.get_aggregation_per_continent(\n",
    "            df_predictions\n",
    "        )\n",
    "        df_agg_since_today_world = DELPHIAggregations.get_aggregation_world(df_predictions)\n",
    "        df_predictions = pd.concat(\n",
    "            [\n",
    "                df_predictions,\n",
    "                df_agg_since_today_per_country,\n",
    "                df_agg_since_today_per_continent,\n",
    "                df_agg_since_today_world,\n",
    "            ]\n",
    "        )\n",
    "        df_predictions.sort_values([\"Continent\", \"Country\", \"Province\", \"Day\"], inplace=True)\n",
    "        return df_predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_aggregation_per_country_with_cf(\n",
    "            df_predictions: pd.DataFrame,\n",
    "            past_prediction_file: str = \"I://covid19orc//danger_map//predicted//Global_V2_20200720.csv\",\n",
    "            past_prediction_date: str = \"2020-07-04\",\n",
    "            q: float = 0.5\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates aggregations at the country level as well as associated confidence intervals\n",
    "        :param df_predictions: dataframe containing the raw predictions from the DELPHI model\n",
    "        :param past_prediction_file: past prediction file's path for CI generation\n",
    "        :param past_prediction_date: past prediction's date for CI generation\n",
    "        :param q: quantile used for the CIs\n",
    "        :return: dataframe with country level aggregated predictions & associated confidence intervals\n",
    "        \"\"\"\n",
    "        df_predictions = df_predictions[df_predictions[\"Province\"] != \"None\"]\n",
    "        columns_without_bounds = [x for x in df_predictions.columns if (\"LB\" not in x) and (\"UB\" not in x)]\n",
    "        df_agg_country = df_predictions[columns_without_bounds].groupby([\"Continent\", \"Country\", \"Day\"]).sum(min_count = 1).reset_index()\n",
    "        df_agg_country[\"Province\"] = \"None\"\n",
    "        df_agg_country = df_agg_country[columns_without_bounds]\n",
    "        aggregated_countries = set(zip(df_agg_country[\"Country\"],df_agg_country[\"Province\"]))\n",
    "        past_predictions = pd.read_csv(past_prediction_file)\n",
    "        if len(aggregated_countries)>0:\n",
    "            list_df_aggregated_countries = []\n",
    "            for country, province in aggregated_countries:\n",
    "                past_predictions_temp = (past_predictions[(past_predictions['Day'] > past_prediction_date) & (past_predictions['Country'] == country) & (past_predictions['Province'] == province)]).sort_values(\"Day\")\n",
    "                df_agg_country_temp = (df_agg_country[(df_agg_country['Country'] == country) & (df_agg_country['Province'] == province)]).sort_values(\"Day\").reset_index(drop=True)\n",
    "                total_detected = df_agg_country_temp['Total Detected'] \n",
    "                total_detected_deaths = df_agg_country_temp['Total Detected Deaths'] \n",
    "    #            active_cases = df_agg_country_temp['Active'] \n",
    "    #            active_hospitalized = df_agg_country_temp['Active Hospitalized'] \n",
    "    #            cumulative_hospitalized = df_agg_country_temp['Cumulative Hospitalized'] \n",
    "    #            active_ventilated = df_agg_country_temp['Active Ventilated'] \n",
    "                cases_fit_data = df_agg_country_temp['Total Detected True'] \n",
    "                deaths_fit_data = df_agg_country_temp['Total Detected Deaths True'] \n",
    "                since_100_dates = df_agg_country_temp['Day'] \n",
    "                n_days_btw_today_since_100 = (datetime.now() - pd.to_datetime(min(since_100_dates))).days\n",
    "                if len(past_predictions_temp) > 0:\n",
    "                    cases_fit_data_past = [y for x, y in zip(since_100_dates,cases_fit_data) if ((x > past_prediction_date) and (not np.isnan(y)))]\n",
    "                    deaths_fit_data_past = [y for x, y in zip(since_100_dates,deaths_fit_data) if ((x > past_prediction_date) and (not np.isnan(y)))]\n",
    "                    total_detected_past = past_predictions_temp[\"Total Detected\"].values[:len(cases_fit_data_past)]\n",
    "                    total_detected_deaths_past = past_predictions_temp[\"Total Detected Deaths\"].values[:len(deaths_fit_data_past)]\n",
    "                    residual_cases_lb = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(cases_fit_data_past,total_detected_past)])) * scipy.stats.norm.ppf(0.5 - q /2)\n",
    "                    residual_cases_ub = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(cases_fit_data_past,total_detected_past)])) * scipy.stats.norm.ppf(0.5 + q /2)\n",
    "                    residual_deaths_lb = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(deaths_fit_data_past,total_detected_deaths_past)])) * scipy.stats.norm.ppf(0.5 - q /2)\n",
    "                    residual_deaths_ub = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(deaths_fit_data_past,total_detected_deaths_past)])) *  scipy.stats.norm.ppf(0.5 + q /2)\n",
    "            \n",
    "                    # Generation of the dataframe from the day since 100th case\n",
    "                    df_predictions_since_100_cont_country_prov = pd.DataFrame({\n",
    "                        \"Total Detected LB\": make_increasing([max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected)]),\n",
    "    #                    \"Active LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_cases, total_detected))],\n",
    "    #                    \"Active Hospitalized LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_hospitalized, total_detected))],\n",
    "    #                    \"Cumulative Hospitalized LB\": make_increasing([max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(cumulative_hospitalized, total_detected))]),\n",
    "                        \"Total Detected Deaths LB\": make_increasing([max(int(round(v + residual_deaths_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected_deaths)]),\n",
    "    #                    \"Active Ventilated LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_ventilated, total_detected))],\n",
    "                        \"Total Detected UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected)],\n",
    "    #                    \"Active UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_cases, total_detected))],\n",
    "    #                    \"Active Hospitalized UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_hospitalized, total_detected))],\n",
    "    #                    \"Cumulative Hospitalized UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(cumulative_hospitalized, total_detected))],\n",
    "                        \"Total Detected Deaths UB\": [max(int(round(v + residual_deaths_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected_deaths)],\n",
    "    #                    \"Active Ventilated UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_ventilated, total_detected))],\n",
    "                    })\n",
    "                    df_agg_country_temp = pd.concat([df_agg_country_temp, df_predictions_since_100_cont_country_prov], axis = 1)\n",
    "                else:\n",
    "                    df_predictions_since_100_cont_country_prov = pd.DataFrame({\n",
    "                        \"Total Detected LB\": [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Active LB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Active Hospitalized LB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Cumulative Hospitalized LB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "                        \"Total Detected Deaths LB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Active Ventilated LB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "                        \"Total Detected UB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Active UB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Active Hospitalized UB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Cumulative Hospitalized UB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "                        \"Total Detected Deaths UB\":  [np.nan for _ in range(len(df_agg_country_temp))],  \n",
    "    #                    \"Active Ventilated UB\": [np.nan for _ in range(len(df_agg_country_temp))]\n",
    "                    })\n",
    "                    df_agg_country_temp = pd.concat([df_agg_country_temp, df_predictions_since_100_cont_country_prov], axis = 1)\n",
    "    \n",
    "                list_df_aggregated_countries.append(df_agg_country_temp)\n",
    "            df_agg_country_final = pd.concat(list_df_aggregated_countries)\n",
    "            return df_agg_country_final \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_aggregation_per_continent_with_cf(\n",
    "            df_predictions: pd.DataFrame,\n",
    "            past_prediction_file: str = \"I://covid19orc//danger_map//predicted//Global_V2_20200720.csv\",\n",
    "            past_prediction_date: str = \"2020-07-04\",\n",
    "            q: float = 0.5\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates aggregations at the continent level as well as associated confidence intervals\n",
    "        :param df_predictions: dataframe containing the raw predictions from the DELPHI model\n",
    "        :param past_prediction_file: past prediction file's path for CI generation\n",
    "        :param past_prediction_date: past prediction's date for CI generation\n",
    "        :param q: quantile used for the CIs\n",
    "        :return: dataframe with continent level aggregated predictions & associated confidence intervals\n",
    "        \"\"\"\n",
    "        columns_without_bounds = [x for x in df_predictions.columns if (\"LB\" not in x) and (\"UB\" not in x)]\n",
    "        df_agg_continent = df_predictions[columns_without_bounds].groupby([\"Continent\", \"Day\"]).sum(min_count = 1).reset_index()\n",
    "        df_agg_continent[\"Country\"] = \"None\"\n",
    "        df_agg_continent[\"Province\"] = \"None\"\n",
    "        df_agg_continent = df_agg_continent[columns_without_bounds]\n",
    "        aggregated_continents = set(zip(df_agg_continent[\"Continent\"], df_agg_continent[\"Country\"],df_agg_continent[\"Province\"]))\n",
    "        past_predictions = pd.read_csv(past_prediction_file)\n",
    "        list_df_aggregated_continents = []\n",
    "        for continent, country, province in aggregated_continents:\n",
    "            past_predictions_temp = (past_predictions[(past_predictions['Day'] > past_prediction_date) & (past_predictions['Continent'] == continent) & (past_predictions['Country'] == country) & (past_predictions['Country'] == province)]).sort_values(\"Day\")\n",
    "            df_agg_continent_temp = (df_agg_continent[(df_agg_continent['Continent'] == continent)]).sort_values(\"Day\").reset_index(drop=True)\n",
    "            total_detected = df_agg_continent_temp['Total Detected'] \n",
    "            total_detected_deaths = df_agg_continent_temp['Total Detected Deaths'] \n",
    "#            active_cases = df_agg_continent_temp['Active'] \n",
    "#            active_hospitalized = df_agg_continent_temp['Active Hospitalized'] \n",
    "#            cumulative_hospitalized = df_agg_continent_temp['Cumulative Hospitalized'] \n",
    "#            active_ventilated = df_agg_continent_temp['Active Ventilated'] \n",
    "            cases_fit_data = df_agg_continent_temp['Total Detected True'] \n",
    "            deaths_fit_data = df_agg_continent_temp['Total Detected Deaths True'] \n",
    "            since_100_dates = df_agg_continent_temp['Day']   \n",
    "            n_days_btw_today_since_100 = (datetime.now() - pd.to_datetime(min(since_100_dates))).days\n",
    "            if len(past_predictions_temp) > 0:\n",
    "                cases_fit_data_past = [y for x, y in zip(since_100_dates,cases_fit_data) if ((x > past_prediction_date) and (not np.isnan(y)))]\n",
    "                deaths_fit_data_past = [y for x, y in zip(since_100_dates,deaths_fit_data) if ((x > past_prediction_date) and (not np.isnan(y)))]\n",
    "                total_detected_past = past_predictions_temp[\"Total Detected\"].values[:len(cases_fit_data_past)]\n",
    "                total_detected_deaths_past = past_predictions_temp[\"Total Detected Deaths\"].values[:len(deaths_fit_data_past)]\n",
    "                residual_cases_lb = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(cases_fit_data_past,total_detected_past)])) * scipy.stats.norm.ppf(0.5 - q /2)\n",
    "                residual_cases_ub = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(cases_fit_data_past,total_detected_past)])) * scipy.stats.norm.ppf(0.5 + q /2)\n",
    "                residual_deaths_lb = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(deaths_fit_data_past,total_detected_deaths_past)])) * scipy.stats.norm.ppf(0.5 - q /2)\n",
    "                residual_deaths_ub = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(deaths_fit_data_past,total_detected_deaths_past)])) *  scipy.stats.norm.ppf(0.5 + q /2)\n",
    "                # Generation of the dataframe from the day since 100th case\n",
    "                df_predictions_since_100_cont_country_prov = pd.DataFrame({\n",
    "                    \"Total Detected LB\": make_increasing([max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected)]),\n",
    "#                    \"Active LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_cases, total_detected))],\n",
    "#                    \"Active Hospitalized LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_hospitalized, total_detected))],\n",
    "#                    \"Cumulative Hospitalized LB\": make_increasing([max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(cumulative_hospitalized, total_detected))]),\n",
    "                    \"Total Detected Deaths LB\": make_increasing([max(int(round(v + residual_deaths_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected_deaths)]),\n",
    "#                    \"Active Ventilated LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_ventilated, total_detected))],\n",
    "                    \"Total Detected UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected)],\n",
    "#                    \"Active UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_cases, total_detected))],\n",
    "#                    \"Active Hospitalized UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_hospitalized, total_detected))],\n",
    "#                    \"Cumulative Hospitalized UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(cumulative_hospitalized, total_detected))],\n",
    "                    \"Total Detected Deaths UB\": [max(int(round(v + residual_deaths_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected_deaths)],\n",
    "#                    \"Active Ventilated UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_ventilated, total_detected))],\n",
    "                })\n",
    "                df_agg_continent_temp = pd.concat([df_agg_continent_temp, df_predictions_since_100_cont_country_prov], axis = 1)\n",
    "            else:\n",
    "                df_predictions_since_100_cont_country_prov = pd.DataFrame({\n",
    "                    \"Total Detected LB\": [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Active LB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Active Hospitalized LB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Cumulative Hospitalized LB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "                    \"Total Detected Deaths LB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Active Ventilated LB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "                    \"Total Detected UB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Active UB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Active Hospitalized UB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Cumulative Hospitalized UB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "                    \"Total Detected Deaths UB\":  [np.nan for _ in range(len(df_agg_continent_temp))],  \n",
    "#                    \"Active Ventilated UB\": [np.nan for _ in range(len(df_agg_continent_temp))]\n",
    "                })\n",
    "                df_agg_continent_temp = pd.concat([df_agg_continent_temp, df_predictions_since_100_cont_country_prov], axis = 1)\n",
    "\n",
    "            list_df_aggregated_continents.append(df_agg_continent_temp)\n",
    "        df_agg_continent_final = pd.concat(list_df_aggregated_continents)\n",
    "        return df_agg_continent_final \n",
    "\n",
    "    @staticmethod\n",
    "    def get_aggregation_world_with_cf(\n",
    "            df_predictions: pd.DataFrame,\n",
    "            past_prediction_file: str = \"I://covid19orc//danger_map//predicted//Global_V2_20200720.csv\",\n",
    "            past_prediction_date: str = \"2020-07-04\",\n",
    "            q: float = 0.5\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates aggregations at the world level as well as associated confidence intervals\n",
    "        :param df_predictions: dataframe containing the raw predictions from the DELPHI model\n",
    "        :param past_prediction_file: past prediction file's path for CI generation\n",
    "        :param past_prediction_date: past prediction's date for CI generation\n",
    "        :param q: quantile used for the CIs\n",
    "        :return: dataframe with continent world aggregated predictions & associated confidence intervals\n",
    "        \"\"\"\n",
    "        columns_without_bounds = [x for x in df_predictions.columns if (\"LB\" not in x) and (\"UB\" not in x)]\n",
    "        df_agg_world = df_predictions[columns_without_bounds].groupby([\"Day\"]).sum(min_count = 1).reset_index()\n",
    "        df_agg_world[\"Continent\"] = \"None\"\n",
    "        df_agg_world[\"Country\"] = \"None\"\n",
    "        df_agg_world[\"Province\"] = \"None\"\n",
    "        df_agg_world = df_agg_world[columns_without_bounds]\n",
    "        past_predictions = pd.read_csv(past_prediction_file)\n",
    "        past_predictions_temp = (past_predictions[(past_predictions['Day'] > past_prediction_date) & (past_predictions['Continent'] == \"None\") & (past_predictions['Country'] == \"None\") & (past_predictions['Province'] == \"None\")]).sort_values(\"Day\")\n",
    "        total_detected = df_agg_world['Total Detected'] \n",
    "        total_detected_deaths = df_agg_world['Total Detected Deaths'] \n",
    "#        active_cases = df_agg_world['Active'] \n",
    "#        active_hospitalized = df_agg_world['Active Hospitalized'] \n",
    "#        cumulative_hospitalized = df_agg_world['Cumulative Hospitalized'] \n",
    "#        active_ventilated = df_agg_world['Active Ventilated'] \n",
    "        cases_fit_data = df_agg_world['Total Detected True'] \n",
    "        deaths_fit_data = df_agg_world['Total Detected Deaths True'] \n",
    "        since_100_dates = df_agg_world['Day']   \n",
    "        n_days_btw_today_since_100 = (datetime.now() - pd.to_datetime(min(since_100_dates))).days\n",
    "        if len(past_predictions_temp) > 0:\n",
    "            cases_fit_data_past = [y for x, y in zip(since_100_dates,cases_fit_data) if ((x > past_prediction_date) and (not np.isnan(y)))]\n",
    "            deaths_fit_data_past = [y for x, y in zip(since_100_dates,deaths_fit_data) if ((x > past_prediction_date) and (not np.isnan(y)))]\n",
    "            total_detected_past = past_predictions_temp[\"Total Detected\"].values[:len(cases_fit_data_past)]\n",
    "            total_detected_deaths_past = past_predictions_temp[\"Total Detected Deaths\"].values[:len(deaths_fit_data_past)]\n",
    "            residual_cases_lb = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(cases_fit_data_past,total_detected_past)])) * scipy.stats.norm.ppf(0.5 - q /2)\n",
    "            residual_cases_ub = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(cases_fit_data_past,total_detected_past)])) * scipy.stats.norm.ppf(0.5 + q /2)\n",
    "            residual_deaths_lb = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(deaths_fit_data_past,total_detected_deaths_past)])) * scipy.stats.norm.ppf(0.5 - q /2)\n",
    "            residual_deaths_ub = np.sqrt(np.mean([(x- y) ** 2 for x,y in zip(deaths_fit_data_past,total_detected_deaths_past)])) *  scipy.stats.norm.ppf(0.5 + q /2)\n",
    "    \n",
    "            # Generation of the dataframe from the day since 100th case\n",
    "            df_predictions_since_100_cont_country_prov = pd.DataFrame({\n",
    "                \"Total Detected LB\": make_increasing([max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected)]),\n",
    "#                \"Active LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_cases, total_detected))],\n",
    "#                \"Active Hospitalized LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_hospitalized, total_detected))],\n",
    "#                \"Cumulative Hospitalized LB\": make_increasing([max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(cumulative_hospitalized, total_detected))]),\n",
    "                \"Total Detected Deaths LB\": make_increasing([max(int(round(v + residual_deaths_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected_deaths)]),\n",
    "#                \"Active Ventilated LB\": [max(int(round(v + residual_cases_lb * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_ventilated, total_detected))],\n",
    "                \"Total Detected UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected)],\n",
    "#                \"Active UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_cases, total_detected))],\n",
    "#                \"Active Hospitalized UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_hospitalized, total_detected))],\n",
    "#                \"Cumulative Hospitalized UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(cumulative_hospitalized, total_detected))],\n",
    "                \"Total Detected Deaths UB\": [max(int(round(v + residual_deaths_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)),0)),0) for c, v in enumerate(total_detected_deaths)],\n",
    "#                \"Active Ventilated UB\": [max(int(round(v + residual_cases_ub * np.sqrt(max(c - n_days_btw_today_since_100, 0)) * v / u,0)),0) for c, (v, u) in enumerate(zip(active_ventilated, total_detected))],\n",
    "            })\n",
    "            df_agg_world_final = pd.concat([df_agg_world, df_predictions_since_100_cont_country_prov], axis = 1)\n",
    "        else:\n",
    "            df_predictions_since_100_cont_country_prov = pd.DataFrame({\n",
    "                \"Total Detected LB\": [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Active LB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Active Hospitalized LB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Cumulative Hospitalized LB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "                \"Total Detected Deaths LB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Active Ventilated LB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "                \"Total Detected UB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Active UB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Active Hospitalized UB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Cumulative Hospitalized UB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "                \"Total Detected Deaths UB\":  [np.nan for _ in range(len(df_agg_world))],  \n",
    "#                \"Active Ventilated UB\": [np.nan for _ in range(len(df_agg_world))]\n",
    "            })\n",
    "            df_agg_world_final = pd.concat([df_agg_world, df_predictions_since_100_cont_country_prov], axis = 1)\n",
    "        return df_agg_world_final \n",
    "\n",
    "    @staticmethod\n",
    "    def append_all_aggregations_cf(\n",
    "            df_predictions: pd.DataFrame,\n",
    "            past_prediction_file: str = \"I://covid19orc//danger_map//predicted//Global_V2_20200720.csv\",\n",
    "            past_prediction_date: str = \"2020-07-04\",\n",
    "            q: float = 0.5\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates and appends all the predictions' aggregations & Confidnece Intervals at the country, continent and\n",
    "        world levels\n",
    "        :param df_predictions: dataframe with the raw predictions from DELPHI\n",
    "        :param past_prediction_file: past prediction file's path for CI generation\n",
    "        :param past_prediction_date: past prediction's date for CI generation\n",
    "        :param q: quantile used for the CIs\n",
    "        :return: dataframe with predictions raw from DELPHI and aggregated ones, as well as associated confidence\n",
    "        intervals at the country, continent & world levels\n",
    "        \"\"\"\n",
    "        df_agg_since_today_per_country = DELPHIAggregations.get_aggregation_per_country_with_cf(\n",
    "            df_predictions=df_predictions,\n",
    "            past_prediction_file=past_prediction_file,\n",
    "            past_prediction_date=past_prediction_date,\n",
    "            q=q\n",
    "        )\n",
    "        df_agg_since_today_per_continent = DELPHIAggregations.get_aggregation_per_continent_with_cf(\n",
    "            df_predictions=df_predictions,\n",
    "            past_prediction_file=past_prediction_file,\n",
    "            past_prediction_date=past_prediction_date,\n",
    "            q=q\n",
    "        )\n",
    "        df_agg_since_today_world = DELPHIAggregations.get_aggregation_world_with_cf(\n",
    "            df_predictions=df_predictions,\n",
    "            past_prediction_file=past_prediction_file,\n",
    "            past_prediction_date=past_prediction_date,\n",
    "            q=q\n",
    "        )\n",
    "        df_predictions = pd.concat([\n",
    "            df_predictions, df_agg_since_today_per_country,\n",
    "            df_agg_since_today_per_continent, df_agg_since_today_world\n",
    "            ],sort=False)\n",
    "        df_predictions.sort_values([\"Continent\", \"Country\", \"Province\", \"Day\"], inplace=True)\n",
    "        df_predictions_from_today = df_predictions[df_predictions.Day >= str((pd.to_datetime(datetime.now())).date())]\n",
    "        return df_predictions_from_today, df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DELPHIAggregationsPolicies:\n",
    "#     @staticmethod\n",
    "#     def get_aggregation_per_country(df_policy_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Aggregates policy predictions at the country level from the predictions dataframe\n",
    "#         :param df_policy_predictions: DELPHI policy predictions dataframe\n",
    "#         :return: DELPHI policy predictions dataframe aggregated at the country level\n",
    "#         \"\"\"\n",
    "#         df_policy_predictions = df_policy_predictions[df_policy_predictions[\"Province\"] != \"None\"]\n",
    "#         df_agg_country = df_policy_predictions.groupby(\n",
    "#             [\"Policy\", \"Time\", \"Continent\", \"Country\", \"Day\"]\n",
    "#         ).sum().reset_index()\n",
    "#         df_agg_country[\"Province\"] = \"None\"\n",
    "#         df_agg_country = df_agg_country[\n",
    "#             [\n",
    "#                 \"Policy\", \"Time\", \"Continent\", \"Country\", \"Province\", \"Day\", \"Total Detected\", \"Active\",\n",
    "#                 \"Active Hospitalized\", \"Cumulative Hospitalized\", \"Total Detected Deaths\", \"Active Ventilated\",\n",
    "#             ]\n",
    "#         ]\n",
    "#         return df_agg_country\n",
    "\n",
    "#     @staticmethod\n",
    "#     def get_aggregation_per_continent(df_policy_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Aggregates policy predictions at the continent level from the predictions dataframe\n",
    "#         :param df_policy_predictions: DELPHI policy predictions dataframe\n",
    "#         :return: DELPHI policy predictions dataframe aggregated at the continent level\n",
    "#         \"\"\"\n",
    "#         df_agg_continent = (\n",
    "#             df_policy_predictions.groupby([\"Policy\", \"Time\", \"Continent\", \"Day\"]).sum().reset_index()\n",
    "#         )\n",
    "#         df_agg_continent[\"Country\"] = \"None\"\n",
    "#         df_agg_continent[\"Province\"] = \"None\"\n",
    "#         df_agg_continent = df_agg_continent[\n",
    "#             [\n",
    "#                 \"Policy\", \"Time\", \"Continent\", \"Country\", \"Province\", \"Day\", \"Total Detected\", \"Active\",\n",
    "#                 \"Active Hospitalized\", \"Cumulative Hospitalized\", \"Total Detected Deaths\", \"Active Ventilated\",\n",
    "#             ]\n",
    "#         ]\n",
    "#         return df_agg_continent\n",
    "\n",
    "#     @staticmethod\n",
    "#     def get_aggregation_world(df_policy_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Aggregates policy predictions at the world level from the predictions dataframe\n",
    "#         :param df_policy_predictions: DELPHI policy predictions dataframe\n",
    "#         :return: DELPHI policy predictions dataframe aggregated at the world level\n",
    "#         \"\"\"\n",
    "#         df_agg_world = df_policy_predictions.groupby([\"Policy\", \"Time\", \"Day\"]).sum().reset_index()\n",
    "#         df_agg_world[\"Continent\"] = \"None\"\n",
    "#         df_agg_world[\"Country\"] = \"None\"\n",
    "#         df_agg_world[\"Province\"] = \"None\"\n",
    "#         df_agg_world = df_agg_world[\n",
    "#             [\n",
    "#                 \"Policy\", \"Time\", \"Continent\", \"Country\", \"Province\", \"Day\", \"Total Detected\", \"Active\",\n",
    "#                 \"Active Hospitalized\", \"Cumulative Hospitalized\", \"Total Detected Deaths\", \"Active Ventilated\",\n",
    "#             ]\n",
    "#         ]\n",
    "#         return df_agg_world\n",
    "\n",
    "#     @staticmethod\n",
    "#     def append_all_aggregations(df_policy_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Creates and appends all the policy predictions' aggregations at the country, continent and world levels\n",
    "#         :param df_predictions: dataframe with the raw policy predictions from DELPHI\n",
    "#         :return: dataframe with raw policy predictions from DELPHI and aggregated ones at the country,\n",
    "#         continent & world levels\n",
    "#         \"\"\"\n",
    "#         df_agg_since_today_per_country = DELPHIAggregations.get_aggregation_per_country(df_policy_predictions)\n",
    "#         df_agg_since_today_per_continent = DELPHIAggregations.get_aggregation_per_continent(df_policy_predictions)\n",
    "#         df_agg_since_today_world = DELPHIAggregations.get_aggregation_world(df_policy_predictions)\n",
    "#         df_policy_predictions = pd.concat(\n",
    "#             [\n",
    "#                 df_policy_predictions,\n",
    "#                 df_agg_since_today_per_country,\n",
    "#                 df_agg_since_today_per_continent,\n",
    "#                 df_agg_since_today_world,\n",
    "#             ]\n",
    "#         )\n",
    "#         df_policy_predictions.sort_values(\n",
    "#             [\"Policy\", \"Time\", \"Continent\", \"Country\", \"Province\", \"Day\"], inplace=True\n",
    "#         )\n",
    "#         return df_policy_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DELPHIBacktest:\n",
    "#     def __init__(\n",
    "#             self, path_to_folder_danger_map: str, prediction_date: str, n_days_backtest: int,\n",
    "#             get_mae: bool, get_mse: bool, logger: Logger,\n",
    "#     ):\n",
    "#         self.historical_data_path = path_to_folder_danger_map + \"processed/Global/\"\n",
    "#         self.prediction_data_path = path_to_folder_danger_map + \"predicted/\"\n",
    "#         self.prediction_date = prediction_date\n",
    "#         self.n_days_backtest = n_days_backtest\n",
    "#         self.get_mae = get_mae\n",
    "#         self.get_mse = get_mse\n",
    "#         self.logger = logger\n",
    "\n",
    "#     def get_historical_data_df(self) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Generates a concatenation of all historical data available in the danger_map folder, all areas\n",
    "#         starting from the prediction date given by the user, and keeping only relevant columns\n",
    "#         :return: a dataframe with all relevant historical data\n",
    "#         \"\"\"\n",
    "#         list_historical_data_filepaths = [\n",
    "#             self.historical_data_path + filename\n",
    "#             for filename in os.listdir(self.historical_data_path)\n",
    "#             if \"Cases_\" in filename\n",
    "#         ]\n",
    "#         df_historical = []\n",
    "#         for filepath_historical in list_historical_data_filepaths:\n",
    "#             df_historical.append(pd.read_csv(filepath_historical))\n",
    "\n",
    "#         df_historical = pd.concat(df_historical).sort_values(\n",
    "#             [\"country\", \"province\", \"date\"]\n",
    "#         ).reset_index(drop=True)[\n",
    "#             [\"country\", \"province\", \"date\", \"day_since100\", \"case_cnt\", \"death_cnt\"]\n",
    "#         ]\n",
    "#         df_historical[\"province\"].fillna(\"None\", inplace=True)\n",
    "#         df_historical.rename(\n",
    "#             columns={\"country\": \"Country\", \"province\": \"Province\", \"date\": \"Day\"}, inplace=True\n",
    "#         )\n",
    "#         df_historical[\"tuple\"] = list(zip(df_historical.Country, df_historical.Province))\n",
    "#         df_historical = df_historical[\n",
    "#             (df_historical.Day >= self.prediction_date)\n",
    "#         ].reset_index(drop=True)\n",
    "#         df_historical = df_historical[df_historical.tuple != (\"US\", \"None\")].reset_index(drop=True)\n",
    "#         return df_historical\n",
    "\n",
    "#     def get_prediction_data(self) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Retrieve the predicted data on the prediction_date given as an input by the user running\n",
    "#         :param prediction_date: prediction date to be used to look for the file in the danger_map folder, format\n",
    "#         has to be YYYY-MM-DD (it is asserted outside of this function)\n",
    "#         :return: a dataframe that contains the relevant predictions on the relevant prediction date\n",
    "#         \"\"\"\n",
    "#         prediction_date_filename = \"\".join(self.prediction_date.split(\"-\"))\n",
    "#         if os.path.exists(self.prediction_data_path + f\"Global_V2_{prediction_date_filename}.csv\"):\n",
    "#             self.logger.info(\"Backtesting on DELPHI V3.0 predictions because filename contains _V2\")\n",
    "#             df_prediction = pd.read_csv(self.prediction_data_path + f\"Global_V2_{prediction_date_filename}.csv\")\n",
    "#         elif os.path.exists(self.prediction_data_path + f\"Global_V2_{prediction_date_filename}.csv\"):\n",
    "#             self.logger.info(\"Backtesting on DELPHI V1.0 or V2.0 predictions because filename doesn't contain _V2\")\n",
    "#             df_prediction = pd.read_csv(self.prediction_data_path + f\"Global_V2_{prediction_date_filename}.csv\")\n",
    "#         else:\n",
    "#             raise ValueError(f\"The file on prediction date {self.prediction_date} has never been generated\")\n",
    "\n",
    "#         return df_prediction[[\"Continent\", \"Country\", \"Province\", \"Day\", \"Total Detected\", \"Total Detected Deaths\"]]\n",
    "\n",
    "#     def get_feasibility_flag(self, df_historical: pd.DataFrame, df_prediction: pd.DataFrame) -> bool:\n",
    "#         \"\"\"\n",
    "#         Checks that there is enough historical and prediction data to perform the backtest based on the user input\n",
    "#         :param df_historical: a dataframe with all relevant historical data\n",
    "#         :param df_prediction: a dataframe that contains the relevant predictions on the relevant prediction date\n",
    "#         :return: a True boolean, otherwise will raise a ValueError with more details as to why backtest is infeasible\n",
    "#         \"\"\"\n",
    "#         max_date_historical = df_historical.Day.max()\n",
    "#         max_date_prediction = df_prediction.Day.max()\n",
    "#         max_date_backtest = str((pd.to_datetime(self.prediction_date) + timedelta(days=self.n_days_backtest)).date())\n",
    "#         days_missing_historical = (pd.to_datetime(max_date_backtest) - pd.to_datetime(max_date_historical)).days\n",
    "#         days_missing_prediction = (pd.to_datetime(max_date_backtest) - pd.to_datetime(max_date_prediction)).days\n",
    "#         if (days_missing_historical > 0) or (days_missing_prediction > 0):\n",
    "#             feasibility_flag = False\n",
    "#         else:\n",
    "#             feasibility_flag = True\n",
    "\n",
    "#         if not feasibility_flag:\n",
    "#             error_message = (\n",
    "#                     \"Backtest date and number of days of backtest incompatible with available data: missing \" +\n",
    "#                     f\"{days_missing_historical} days of historical data and {days_missing_prediction} days of prediction data \" +\n",
    "#                     \"(if negative value for number of days: not missing)\"\n",
    "#             )\n",
    "#             self.logger.warning(error_message)\n",
    "#             raise ValueError(error_message)\n",
    "\n",
    "#         self.logger.info(f\"Backtest is feasible based on input prediction date and number of backtesting days\")\n",
    "#         return True\n",
    "\n",
    "#     def generate_empty_metrics_dict(self) -> dict:\n",
    "#         \"\"\"\n",
    "#         Generates the format of the dictionary that will compose the dataframe with all backtest metrics\n",
    "#         based on the get_mae and get_mse flags given by the user\n",
    "#         :return: a dictionary with either 5, 7 or 9 keys depending on the MAE and MSE flags\n",
    "#         \"\"\"\n",
    "#         dict_df_backtest_metrics = {\n",
    "#             \"prediction_date\": [],\n",
    "#             \"n_days_backtest\": [],\n",
    "#             \"tuple_area\": [],\n",
    "#             \"mape_cases\": [],\n",
    "#             \"mape_deaths\": [],\n",
    "#         }\n",
    "#         if self.get_mae:\n",
    "#             dict_df_backtest_metrics[\"mae_cases\"] = []\n",
    "#             dict_df_backtest_metrics[\"mae_deaths\"] = []\n",
    "\n",
    "#         if self.get_mse:\n",
    "#             dict_df_backtest_metrics[\"mse_cases\"] = []\n",
    "#             dict_df_backtest_metrics[\"mse_deaths\"] = []\n",
    "\n",
    "#         return dict_df_backtest_metrics\n",
    "\n",
    "#     def get_backtest_metrics_area(\n",
    "#             self, df_backtest: pd.DataFrame, tuple_area: tuple, dict_df_backtest_metrics: dict,\n",
    "#     ) -> dict:\n",
    "#         \"\"\"\n",
    "#         Updates the backtest metrics dictionary with metrics values for that particular area tuple\n",
    "#         :param df_backtest: pre-processed dataframe containing historical and prediction data\n",
    "#         :param tuple_area: tuple of the format (Continent, Country, Province)\n",
    "#         :param dict_df_backtest_metrics: dictionary containing all the metrics and will be used to create final\n",
    "#         backtest metrics dataframe\n",
    "#         :return: updated dictionary with backtest metrics for that particular area tuple\n",
    "#         \"\"\"\n",
    "#         df_temp = df_backtest[df_backtest.tuple_complete == tuple_area]\n",
    "#         max_date_backtest = str((pd.to_datetime(self.prediction_date) + timedelta(days=self.n_days_backtest)).date())\n",
    "#         df_temp = df_temp[(df_temp.Day >= self.prediction_date) & (df_temp.Day <= max_date_backtest)].sort_values(\n",
    "#             [\"Continent\", \"Country\", \"Province\", \"Day\"]\n",
    "#         ).reset_index(drop=True)\n",
    "#         mae_cases, mape_cases = compute_mae_and_mape(\n",
    "#             y_true=df_temp.case_cnt.tolist(), y_pred=df_temp[\"Total Detected\"].tolist()\n",
    "#         )\n",
    "#         mae_deaths, mape_deaths = compute_mae_and_mape(\n",
    "#             y_true=df_temp.death_cnt.tolist(), y_pred=df_temp[\"Total Detected Deaths\"].tolist()\n",
    "#         )\n",
    "#         mse_cases = compute_mse(y_true=df_temp.case_cnt.tolist(), y_pred=df_temp[\"Total Detected\"].tolist())\n",
    "#         mse_deaths = compute_mse(y_true=df_temp.death_cnt.tolist(), y_pred=df_temp[\"Total Detected Deaths\"].tolist())\n",
    "#         dict_df_backtest_metrics[\"prediction_date\"].append(self.prediction_date)\n",
    "#         dict_df_backtest_metrics[\"n_days_backtest\"].append(self.n_days_backtest)\n",
    "#         dict_df_backtest_metrics[\"tuple_area\"].append(tuple_area)\n",
    "#         dict_df_backtest_metrics[\"mape_cases\"].append(mape_cases)\n",
    "#         dict_df_backtest_metrics[\"mape_deaths\"].append(mape_deaths)\n",
    "#         if self.get_mae:\n",
    "#             dict_df_backtest_metrics[\"mae_cases\"].append(mae_cases)\n",
    "#             dict_df_backtest_metrics[\"mae_deaths\"].append(mae_deaths)\n",
    "\n",
    "#         if self.get_mse:\n",
    "#             dict_df_backtest_metrics[\"mse_cases\"].append(mse_cases)\n",
    "#             dict_df_backtest_metrics[\"mse_deaths\"].append(mse_deaths)\n",
    "\n",
    "#         return dict_df_backtest_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_conditions(params_fitted: tuple, global_params_fixed: tuple) -> list:\n",
    "    \"\"\"\n",
    "    Generates the initial conditions for the DELPHI model based on global fixed parameters (mostly populations and some\n",
    "    constant rates) and fitted parameters (the internal parameters k1 and k2)\n",
    "    :param params_fitted: tuple of parameters being fitted, mostly interested in k1 and k2 here (parameters 7 and 8)\n",
    "    :param global_params_fixed: tuple of fixed and constant parameters for the model defined a while ago\n",
    "    :return: a list of initial conditions for all 16 states of the DELPHI model\n",
    "    \"\"\"\n",
    "    alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = params_fitted \n",
    "    N, R_upperbound, R_heuristic, R_0, PopulationD, PopulationI, p_d, p_h, p_v = global_params_fixed\n",
    "\n",
    "    PopulationR = min(R_upperbound - 1, min(int(R_0*p_d), R_heuristic))\n",
    "    PopulationCI = (PopulationI - PopulationD - PopulationR)*k3\n",
    "\n",
    "    S_0 = (\n",
    "        (N - PopulationCI / p_d)\n",
    "        - (PopulationCI / p_d * (k1 + k2))\n",
    "        - (PopulationR / p_d)\n",
    "        - (PopulationD / p_d)\n",
    "    )\n",
    "    E_0 = PopulationCI / p_d * k1\n",
    "    I_0 = PopulationCI / p_d * k2\n",
    "    UR_0 = (PopulationCI / p_d - PopulationCI) * (1 - p_dth)\n",
    "    DHR_0 = (PopulationCI * p_h) * (1 - p_dth)\n",
    "    DQR_0 = PopulationCI * (1 - p_h) * (1 - p_dth)\n",
    "    UD_0 = (PopulationCI / p_d - PopulationCI) * p_dth\n",
    "    DHD_0 = PopulationCI * p_h * p_dth\n",
    "    DQD_0 = PopulationCI * (1 - p_h) * p_dth\n",
    "    R_0 = PopulationR / p_d\n",
    "    D_0 = PopulationD / p_d\n",
    "    TH_0 = PopulationCI * p_h\n",
    "    DVR_0 = (PopulationCI * p_h * p_v) * (1 - p_dth)\n",
    "    DVD_0 = (PopulationCI * p_h * p_v) * p_dth\n",
    "    DD_0 = PopulationD\n",
    "    DT_0 = PopulationI\n",
    "    x_0_cases = [\n",
    "        S_0, E_0, I_0, UR_0, DHR_0, DQR_0, UD_0, DHD_0, DQD_0, R_0,\n",
    "        D_0, TH_0, DVR_0, DVD_0, DD_0, DT_0,\n",
    "    ]\n",
    "    return x_0_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_initial_conditions_with_testing(params_fitted: tuple, global_params_fixed: tuple) -> list:\n",
    "#     \"\"\"\n",
    "#     Generates the initial conditions for the DELPHI model based on global fixed parameters (mostly populations and some\n",
    "#     constant rates) and fitted parameters (the internal parameters k1 and k2) when including testing in the modeling\n",
    "#     :param params_fitted: tuple of parameters being fitted, mostly interested in k1 and k2 here (parameters 7 and 8)\n",
    "#     :param global_params_fixed: tuple of fixed and constant parameters for the model defined a while ago\n",
    "#     :return: a list of initial conditions for all 16 states of the DELPHI model\n",
    "#     \"\"\"\n",
    "#     alpha, days, r_s, r_dth, p_dth, k1, k2, beta_0, beta_1 = params_fitted\n",
    "#     N, PopulationCI, PopulationR, PopulationD, PopulationI, p_d, p_h, p_v = (\n",
    "#         global_params_fixed\n",
    "#     )\n",
    "#     S_0 = (\n",
    "#         (N - PopulationCI / p_d)\n",
    "#         - (PopulationCI / p_d * (k1 + k2))\n",
    "#         - (PopulationR / p_d)\n",
    "#         - (PopulationD / p_d)\n",
    "#     )\n",
    "#     E_0 = PopulationCI / p_d * k1\n",
    "#     I_0 = PopulationCI / p_d * k2\n",
    "#     UR_0 = (PopulationCI / p_d - PopulationCI) * (1 - p_dth)\n",
    "#     DHR_0 = (PopulationCI * p_h) * (1 - p_dth)\n",
    "#     DQR_0 = PopulationCI * (1 - p_h) * (1 - p_dth)\n",
    "#     UD_0 = (PopulationCI / p_d - PopulationCI) * p_dth\n",
    "#     DHD_0 = PopulationCI * p_h * p_dth\n",
    "#     DQD_0 = PopulationCI * (1 - p_h) * p_dth\n",
    "#     R_0 = PopulationR / p_d\n",
    "#     D_0 = PopulationD / p_d\n",
    "#     TH_0 = PopulationCI * p_h\n",
    "#     DVR_0 = (PopulationCI * p_h * p_v) * (1 - p_dth)\n",
    "#     DVD_0 = (PopulationCI * p_h * p_v) * p_dth\n",
    "#     DD_0 = PopulationD\n",
    "#     DT_0 = PopulationI\n",
    "#     x_0_cases = [\n",
    "#         S_0, E_0, I_0, UR_0, DHR_0, DQR_0, UD_0, DHD_0,\n",
    "#         DQD_0, R_0, D_0, TH_0, DVR_0, DVD_0, DD_0, DT_0,\n",
    "#     ]\n",
    "#     return x_0_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fitting_data_from_validcases(validcases: pd.DataFrame) -> (float, list, list):\n",
    "    \"\"\"\n",
    "    Creates the balancing coefficient (regularization coefficient between cases & deaths in cost function) as well as\n",
    "    the cases and deaths data on which to be fitted\n",
    "    :param validcases: Dataframe containing cases and deaths data on the relevant time period for our optimization\n",
    "    :return: the balancing coefficient and two lists containing cases and deaths over the right time span for fitting\n",
    "    \"\"\"\n",
    "    validcases_nondeath = validcases[\"case_cnt\"].tolist()\n",
    "    validcases_death = validcases[\"death_cnt\"].tolist()\n",
    "    balance = validcases_nondeath[-1] / max(validcases_death[-1], 10) / 3\n",
    "    cases_data_fit = validcases_nondeath\n",
    "    deaths_data_fit = validcases_death\n",
    "    return balance, cases_data_fit, deaths_data_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_value(\n",
    "        optimizer: str, balance: float, x_sol: list, cases_data_fit: list, deaths_data_fit: list, weights: list\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Obtain the value of the loss function depending on the optimizer (as it is different for global optimization using\n",
    "    simulated annealing)\n",
    "    :param optimizer: String, for now either tnc, trust-constr or annealing\n",
    "    :param balance: Regularization coefficient between cases and deaths\n",
    "    :param x_sol: Solution previously fitted by the optimizer containing fitted values for all 16 states\n",
    "    :param fitcasend: cases data to be fitted on\n",
    "    :param deaths_data_fit: deaths data to be fitted on\n",
    "    :param weights: time-related weights to give more importance to recent data points in the fit (in the loss function)\n",
    "    :return: float, corresponding to the value of the loss function\n",
    "    \"\"\"\n",
    "    if optimizer in [\"tnc\", \"trust-constr\"]:\n",
    "        residuals_value = sum(\n",
    "            np.multiply((x_sol[15, :] - cases_data_fit) ** 2, weights)\n",
    "            + balance\n",
    "            * balance\n",
    "            * np.multiply((x_sol[14, :] - deaths_data_fit) ** 2, weights)\n",
    "        )\n",
    "    elif optimizer == \"annealing\":\n",
    "        residuals_value = sum(\n",
    "            np.multiply((x_sol[15, :] - cases_data_fit) ** 2, weights)\n",
    "            + balance\n",
    "            * balance\n",
    "            * np.multiply((x_sol[14, :] - deaths_data_fit) ** 2, weights)\n",
    "        ) + sum(\n",
    "            np.multiply(\n",
    "                (x_sol[15, 7:] - x_sol[15, :-7] - cases_data_fit[7:] + cases_data_fit[:-7]) ** 2,\n",
    "                weights[7:],\n",
    "            )\n",
    "            + balance * balance * np.multiply(\n",
    "                (x_sol[14, 7:] - x_sol[14, :-7] - deaths_data_fit[7:] + deaths_data_fit[:-7]) ** 2,\n",
    "                weights[7:],\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not in 'tnc', 'trust-constr' or 'annealing' so not supported\")\n",
    "\n",
    "    return residuals_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape_data_fitting(cases_data_fit: list, deaths_data_fit: list, x_sol_final: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Computes MAPE on cases & deaths (averaged) either on last 15 days of historical data (if there are more than 15)\n",
    "    or exactly the number of days in the historical data (if less than 15)\n",
    "    :param cases_data_fit: list, contains data used to fit on number of cases\n",
    "    :param deaths_data_fit: list, contains data used to fit on number of deaths\n",
    "    :param x_sol_final: numpy array, contains the predicted solution by the DELPHI model for all 16 states\n",
    "    :return: a float corresponding to the average MAPE on cases and deaths on a given period of time (15 days is default\n",
    "    but otherwise the number of days available in the historical data)\n",
    "    \"\"\"\n",
    "    if len(cases_data_fit) > 15:  # In which case we can compute MAPE on last 15 days\n",
    "        mape_data = (\n",
    "                compute_mape(\n",
    "                    cases_data_fit[-15:],\n",
    "                    x_sol_final[15, len(cases_data_fit) - 15: len(cases_data_fit)],\n",
    "                ) + compute_mape(\n",
    "                    deaths_data_fit[-15:],\n",
    "                    x_sol_final[14, len(deaths_data_fit) - 15: len(deaths_data_fit)],\n",
    "                )\n",
    "        ) / 2\n",
    "    else:  # We take MAPE on all available previous days (less than 15)\n",
    "        mape_data = (\n",
    "                compute_mape(cases_data_fit, x_sol_final[15, : len(cases_data_fit)])\n",
    "                + compute_mape(deaths_data_fit, x_sol_final[14, : len(deaths_data_fit)])\n",
    "        ) / 2\n",
    "\n",
    "    return mape_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_sign_mape(y_true: list, y_pred: list) -> float:\n",
    "#     \"\"\"\n",
    "#     Compute the sign of the Mean Percentage Error, mainly to know if we're constantly over or undershooting\n",
    "#     :param y_true: list of true historical values\n",
    "#     :param y_pred: list of predicted values\n",
    "#     :return: a float, +1 or -1 for the sign of the MPE\n",
    "#     \"\"\"\n",
    "#     # Mean Percentage Error, without the absolute value\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "#     mpe = np.mean((y_true - y_pred)[y_true > 0] / y_true[y_true > 0]) * 100\n",
    "#     sign = np.sign(mpe)\n",
    "#     return sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mape_daily_delta_since_last_train(\n",
    "#     true_last_train: list, pred_last_train: list, y_true: list, y_pred: list\n",
    "# ) -> float:\n",
    "#     \"\"\"\n",
    "#     Computed the Mean Absolute Percentage Error between the daily differences of prediction between a previous train\n",
    "#     and a current train true/predicted values\n",
    "#     :param true_last_train: list of true historical values for the previous train considered\n",
    "#     :param pred_last_train: list of predicted values for the previous train considered\n",
    "#     :param y_true: list of true historical values for the current train considered\n",
    "#     :param y_pred: list of predicted values for the current train considered\n",
    "#     :return: a float corresponding between the MAPE between the daily differences of prediction/truth between 2\n",
    "#     different training processes\n",
    "#     \"\"\"\n",
    "#     delta_true = np.array([y_true_i - true_last_train for y_true_i in y_true])\n",
    "#     delta_pred = np.array([y_pred_i - pred_last_train for y_pred_i in y_pred])\n",
    "#     mape_daily_delta = (\n",
    "#         np.mean(\n",
    "#             np.abs(delta_true - delta_pred)[delta_true > 0] / delta_true[delta_true > 0]\n",
    "#         )\n",
    "#         * 100\n",
    "#     )\n",
    "#     return mape_daily_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mse(y_true: list, y_pred: list) -> float:\n",
    "#     \"\"\"\n",
    "#     Compute the Mean Squared Error between two lists\n",
    "#     :param y_true: list of true historical values\n",
    "#     :param y_pred: list of predicted values\n",
    "#     :return: a float, corresponding to the MSE\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "#     mse = np.mean((y_true - y_pred) ** 2)\n",
    "#     return float(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mae_and_mape(y_true: list, y_pred: list) -> (float, float):\n",
    "#     \"\"\"\n",
    "#     Compute the Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) between two lists of values\n",
    "#     :param y_true: list of true historical values\n",
    "#     :param y_pred: list of predicted values\n",
    "#     :return: a tuple of floats, corresponding to (MAE, MAPE)\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "#     mae = np.mean(np.abs((y_true - y_pred)))\n",
    "#     mape = np.mean(np.abs((y_true - y_pred)[y_true > 0] / y_true[y_true > 0])) * 100\n",
    "#     return mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mape(y_true: list, y_pred: list) -> float:\n",
    "#     \"\"\"\n",
    "#     Compute the Mean Absolute Percentage Error (MAPE) between two lists of values\n",
    "#     :param y_true: list of true historical values\n",
    "#     :param y_pred: list of predicted values\n",
    "#     :return: a float corresponding to the MAPE\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "#     mape = np.mean(np.abs((y_true - y_pred)[y_true > 0] / y_true[y_true > 0])) * 100\n",
    "#     return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user psutil\n",
    "# !pip install --user import_ipynb\n",
    "# !pip install --user scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Hamza Tazi Bouardi (htazi@mit.edu), Michael L. Li (mlli@mit.edu), Omar Skali Lami (oskali@mit.edu)\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import import_ipynb\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import dual_annealing\n",
    "from DELPHI_utils_V4_static import (\n",
    "    DELPHIAggregations, DELPHIDataSaver, DELPHIDataCreator, get_initial_conditions,\n",
    "    get_mape_data_fitting, create_fitting_data_from_validcases, get_residuals_value\n",
    ")\n",
    "from DELPHI_utils_V4_dynamic import get_bounds_params_from_pastparams\n",
    "from DELPHI_params_V4 import (\n",
    "    fitting_start_date,\n",
    "    default_parameter_list,\n",
    "    dict_default_reinit_parameters,\n",
    "    dict_default_reinit_lower_bounds,\n",
    "    dict_default_reinit_upper_bounds,\n",
    "    default_upper_bound,\n",
    "    default_lower_bound,\n",
    "    percentage_drift_upper_bound,\n",
    "    percentage_drift_lower_bound,\n",
    "    percentage_drift_upper_bound_annealing,\n",
    "    percentage_drift_lower_bound_annealing,\n",
    "    default_upper_bound_annealing,\n",
    "    default_lower_bound_annealing,\n",
    "    default_lower_bound_t_jump,\n",
    "    default_upper_bound_t_jump,\n",
    "    default_lower_bound_std_normal,\n",
    "    default_upper_bound_std_normal,\n",
    "    default_bounds_params,\n",
    "    validcases_threshold,\n",
    "    IncubeD,\n",
    "    RecoverID,\n",
    "    RecoverHD,\n",
    "    DetectD,\n",
    "    VentilatedD,\n",
    "    default_maxT,\n",
    "    p_v,\n",
    "    p_d,\n",
    "    p_h,\n",
    "    max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing Global Variables ##########################################################################\n",
    "with open(\"config.yml\", \"r\") as ymlfile:\n",
    "    CONFIG = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "CONFIG_FILEPATHS = CONFIG[\"filepaths\"]\n",
    "time_beginning = time.time()\n",
    "yesterday = \"\".join(str(datetime.now().date() - timedelta(days=1)).split(\"-\"))\n",
    "yesterday_logs_filename = \"\".join(\n",
    "    (str(datetime.now().date() - timedelta(days=1)) + f\"_{datetime.now().hour}H{datetime.now().minute}M\").split(\"-\")\n",
    ")\n",
    "with open('run_configs/run-config.yml', \"r\") as ymlfile:\n",
    "    RUN_CONFIG = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "USER_RUNNING = RUN_CONFIG[\"arguments\"][\"user\"]\n",
    "OPTIMIZER = RUN_CONFIG[\"arguments\"][\"optimizer\"]\n",
    "GET_CONFIDENCE_INTERVALS = bool(int(RUN_CONFIG[\"arguments\"][\"confidence_intervals\"]))\n",
    "SAVE_TO_WEBSITE = bool(int(RUN_CONFIG[\"arguments\"][\"website\"]))\n",
    "SAVE_SINCE100_CASES = bool(int(RUN_CONFIG[\"arguments\"][\"since100case\"]))\n",
    "PATH_TO_FOLDER_DANGER_MAP = CONFIG_FILEPATHS[\"danger_map\"][USER_RUNNING]\n",
    "PATH_TO_DATA_SANDBOX = CONFIG_FILEPATHS[\"data_sandbox\"][USER_RUNNING]\n",
    "PATH_TO_WEBSITE_PREDICTED = CONFIG_FILEPATHS[\"website\"][USER_RUNNING]\n",
    "past_prediction_date = \"\".join(str(datetime.now().date() - timedelta(days=14)).split(\"-\"))\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DELPHI function sove_and_predict_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_and_predict_area(\n",
    "        tuple_area_state_: tuple,\n",
    "        yesterday_: str,\n",
    "        past_parameters_: pd.DataFrame,\n",
    "        popcountries: pd.DataFrame,\n",
    "        startT: str = None, # added to change optimmization start date\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallelizable version of the fitting & solving process for DELPHI V4, this function is called with multiprocessing\n",
    "    :param tuple_area_: tuple corresponding to (continent, country, province)\n",
    "    :param yesterday_: string corresponding to the date from which the model will read the previous parameters. The\n",
    "    format has to be 'YYYYMMDD'\n",
    "    :param past_parameters_: Parameters from yesterday_ used as a starting point for the fitting process\n",
    "    :param popcountries: DataFrame containing population information for all countries and provinces\n",
    "    :startT: string for the date from when the pandemic will be modelled (format should be 'YYYY-MM-DD')\n",
    "    :return: either None if can't optimize (either less than 100 cases or less than 7 days with 100 cases) or a tuple\n",
    "    with 3 dataframes related to that tuple_area_ (parameters df, predictions since yesterday_+1, predictions since\n",
    "    first day with 100 cases) and a scipy.optimize object (OptimizeResult) that contains the predictions for all\n",
    "    16 states of the model (and some other information that isn't used)\n",
    "    \"\"\"\n",
    "    time_entering = time.time()\n",
    "    continent, country, province, initial_state = tuple_area_state_\n",
    "    country_sub = country.replace(\" \", \"_\")\n",
    "    province_sub = province.replace(\" \", \"_\")\n",
    "    print(f\"starting to predict for {continent}, {country}, {province}\")\n",
    "    if os.path.exists(PATH_TO_FOLDER_DANGER_MAP + f\"processed/Cases_{country_sub}_{province_sub}.csv\"):\n",
    "        totalcases = pd.read_csv(\n",
    "            PATH_TO_FOLDER_DANGER_MAP + f\"processed/Cases_{country_sub}_{province_sub}.csv\"\n",
    "        )\n",
    "        if totalcases.day_since100.max() < 0:\n",
    "            logging.warning(\n",
    "                f\"Not enough cases (less than 100) for Continent={continent}, Country={country} and Province={province}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if past_parameters_ is not None:\n",
    "            parameter_list_total = past_parameters_[\n",
    "                (past_parameters_.Country == country)\n",
    "                & (past_parameters_.Province == province)\n",
    "            ].reset_index(drop=True)\n",
    "            if len(parameter_list_total) > 0:\n",
    "                parameter_list_line = parameter_list_total.iloc[-1, :].values.tolist()\n",
    "                parameter_list = parameter_list_line[5:]\n",
    "                bounds_params = get_bounds_params_from_pastparams(\n",
    "                    optimizer=OPTIMIZER,\n",
    "                    parameter_list=parameter_list,\n",
    "                    dict_default_reinit_parameters=dict_default_reinit_parameters,\n",
    "                    percentage_drift_lower_bound=percentage_drift_lower_bound,\n",
    "                    default_lower_bound=default_lower_bound,\n",
    "                    dict_default_reinit_lower_bounds=dict_default_reinit_lower_bounds,\n",
    "                    percentage_drift_upper_bound=percentage_drift_upper_bound,\n",
    "                    default_upper_bound=default_upper_bound,\n",
    "                    dict_default_reinit_upper_bounds=dict_default_reinit_upper_bounds,\n",
    "                    percentage_drift_lower_bound_annealing=percentage_drift_lower_bound_annealing,\n",
    "                    default_lower_bound_annealing=default_lower_bound_annealing,\n",
    "                    percentage_drift_upper_bound_annealing=percentage_drift_upper_bound_annealing,\n",
    "                    default_upper_bound_annealing=default_upper_bound_annealing,\n",
    "                    default_lower_bound_t_jump=default_lower_bound_t_jump,\n",
    "                    default_upper_bound_t_jump=default_upper_bound_t_jump,\n",
    "                    default_lower_bound_std_normal=default_lower_bound_std_normal,\n",
    "                    default_upper_bound_std_normal=default_upper_bound_std_normal,\n",
    "                )\n",
    "                start_date = pd.to_datetime(parameter_list_line[3])\n",
    "                bounds_params = tuple(bounds_params)\n",
    "            else:\n",
    "                # Otherwise use established lower/upper bounds\n",
    "                parameter_list = default_parameter_list\n",
    "                bounds_params = default_bounds_params\n",
    "                start_date = pd.to_datetime(totalcases.loc[totalcases.day_since100 == 0, \"date\"].iloc[-1])\n",
    "        else:\n",
    "            # Otherwise use established lower/upper bounds\n",
    "            parameter_list = default_parameter_list\n",
    "            bounds_params = default_bounds_params\n",
    "            start_date = pd.to_datetime(totalcases.loc[totalcases.day_since100 == 0, \"date\"].iloc[-1])\n",
    "\n",
    "        if startT is not None:\n",
    "            input_start_date = pd.to_datetime(startT)\n",
    "            if input_start_date > start_date:\n",
    "                delta_days = (input_start_date - start_date).days\n",
    "                parameter_list[9] = parameter_list[9] - delta_days\n",
    "                bounds_params_list = list(bounds_params)\n",
    "                bounds_params_list[9] = (bounds_params_list[9][0]-delta_days, bounds_params_list[9][1]-delta_days)\n",
    "                bounds_params = tuple(bounds_params_list)\n",
    "                start_date = input_start_date\n",
    "            validcases = totalcases[\n",
    "                (totalcases.date >= str(start_date))\n",
    "                & (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "            ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "        else:\n",
    "            validcases = totalcases[\n",
    "                (totalcases.day_since100 >= 0)\n",
    "                & (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "            ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "        # Now we start the modeling part:\n",
    "        if len(validcases) <= validcases_threshold:\n",
    "            logging.warning(\n",
    "                f\"Not enough historical data (less than a week)\"\n",
    "                + f\"for Continent={continent}, Country={country} and Province={province}\"\n",
    "            )\n",
    "            return None\n",
    "        else:\n",
    "            PopulationT = popcountries[\n",
    "                (popcountries.Country == country) & (popcountries.Province == province)\n",
    "            ].pop2016.iloc[-1]\n",
    "            N = PopulationT\n",
    "            PopulationI = validcases.loc[0, \"case_cnt\"]\n",
    "            PopulationD = validcases.loc[0, \"death_cnt\"]\n",
    "            if initial_state is not None:\n",
    "                R_0 = initial_state[9]\n",
    "            else:\n",
    "                R_0 = validcases.loc[0, \"death_cnt\"] * 5 if validcases.loc[0, \"case_cnt\"] - validcases.loc[0, \"death_cnt\"]> validcases.loc[0, \"death_cnt\"] * 5 else 0\n",
    "                bounds_params_list = list(bounds_params)\n",
    "                bounds_params_list[-1] = (0.999,1)\n",
    "                bounds_params = tuple(bounds_params_list)\n",
    "            cases_t_14days = totalcases[totalcases.date >= str(start_date- pd.Timedelta(14, 'D'))]['case_cnt'].values[0]\n",
    "            deaths_t_9days = totalcases[totalcases.date >= str(start_date - pd.Timedelta(9, 'D'))]['death_cnt'].values[0]\n",
    "            R_upperbound = validcases.loc[0, \"case_cnt\"] - validcases.loc[0, \"death_cnt\"]\n",
    "            R_heuristic = cases_t_14days - deaths_t_9days\n",
    "            if int(R_0*p_d) >= R_upperbound and R_heuristic >= R_upperbound:\n",
    "                logging.error(f\"Initial conditions for PopulationR too high for {country}-{province}, on {startT}\")\n",
    "            \"\"\"\n",
    "            Fixed Parameters based on meta-analysis:\n",
    "            p_h: Hospitalization Percentage\n",
    "            RecoverHD: Average Days until Recovery\n",
    "            VentilationD: Number of Days on Ventilation for Ventilated Patients\n",
    "            maxT: Maximum # of Days Modeled\n",
    "            p_d: Percentage of True Cases Detected\n",
    "            p_v: Percentage of Hospitalized Patients Ventilated,\n",
    "            balance: Regularization coefficient between cases and deaths\n",
    "            \"\"\"\n",
    "            maxT = (default_maxT - start_date).days + 1\n",
    "            t_cases = validcases[\"day_since100\"].tolist() - validcases.loc[0, \"day_since100\"]\n",
    "            balance, cases_data_fit, deaths_data_fit = create_fitting_data_from_validcases(validcases)\n",
    "            GLOBAL_PARAMS_FIXED = (N, R_upperbound, R_heuristic, R_0, PopulationD, PopulationI, p_d, p_h, p_v)\n",
    "\n",
    "            def model_covid(\n",
    "                t, x, alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3\n",
    "            ) -> list:\n",
    "                \"\"\"\n",
    "                SEIR based model with 16 distinct states, taking into account undetected, deaths, hospitalized and\n",
    "                recovered, and using an ArcTan government response curve, corrected with a Gaussian jump in case of\n",
    "                a resurgence in cases\n",
    "                :param t: time step\n",
    "                :param x: set of all the states in the model (here, 16 of them)\n",
    "                :param alpha: Infection rate\n",
    "                :param days: Median day of action (used in the arctan governmental response)\n",
    "                :param r_s: Median rate of action (used in the arctan governmental response)\n",
    "                :param r_dth: Rate of death\n",
    "                :param p_dth: Initial mortality percentage\n",
    "                :param r_dthdecay: Rate of decay of mortality percentage\n",
    "                :param k1: Internal parameter 1 (used for initial conditions)\n",
    "                :param k2: Internal parameter 2 (used for initial conditions)\n",
    "                :param jump: Amplitude of the Gaussian jump modeling the resurgence in cases\n",
    "                :param t_jump: Time where the Gaussian jump will reach its maximum value\n",
    "                :param std_normal: Standard Deviation of the Gaussian jump (~ time span of the resurgence in cases)\n",
    "                :param k3: Internal parameter 2 (used for initial conditions)\n",
    "                :return: predictions for all 16 states, which are the following\n",
    "                [0 S, 1 E, 2 I, 3 UR, 4 DHR, 5 DQR, 6 UD, 7 DHD, 8 DQD, 9 R, 10 D, 11 TH, 12 DVR,13 DVD, 14 DD, 15 DT]\n",
    "                \"\"\"\n",
    "                r_i = np.log(2) / IncubeD  # Rate of infection leaving incubation phase\n",
    "                r_d = np.log(2) / DetectD  # Rate of detection\n",
    "                r_ri = np.log(2) / RecoverID  # Rate of recovery not under infection\n",
    "                r_rh = np.log(2) / RecoverHD  # Rate of recovery under hospitalization\n",
    "                r_rv = np.log(2) / VentilatedD  # Rate of recovery under ventilation\n",
    "                gamma_t = (\n",
    "                    (2 / np.pi) * np.arctan(-(t - days) / 20 * r_s) + 1\n",
    "                    + jump * np.exp(-(t - t_jump) ** 2 / (2 * std_normal ** 2))\n",
    "                )\n",
    "                p_dth_mod = (2 / np.pi) * (p_dth - 0.001) * (np.arctan(-t / 20 * r_dthdecay) + np.pi / 2) + 0.001\n",
    "                assert (\n",
    "                    len(x) == 16\n",
    "                ), f\"Too many input variables, got {len(x)}, expected 16\"\n",
    "                S, E, I, AR, DHR, DQR, AD, DHD, DQD, R, D, TH, DVR, DVD, DD, DT = x\n",
    "                # Equations on main variables\n",
    "                dSdt = -alpha * gamma_t * S * I / N\n",
    "                dEdt = alpha * gamma_t * S * I / N - r_i * E\n",
    "                dIdt = r_i * E - r_d * I\n",
    "                dARdt = r_d * (1 - p_dth_mod) * (1 - p_d) * I - r_ri * AR\n",
    "                dDHRdt = r_d * (1 - p_dth_mod) * p_d * p_h * I - r_rh * DHR\n",
    "                dDQRdt = r_d * (1 - p_dth_mod) * p_d * (1 - p_h) * I - r_ri * DQR\n",
    "                dADdt = r_d * p_dth_mod * (1 - p_d) * I - r_dth * AD\n",
    "                dDHDdt = r_d * p_dth_mod * p_d * p_h * I - r_dth * DHD\n",
    "                dDQDdt = r_d * p_dth_mod * p_d * (1 - p_h) * I - r_dth * DQD\n",
    "                dRdt = r_ri * (AR + DQR) + r_rh * DHR\n",
    "                dDdt = r_dth * (AD + DQD + DHD)\n",
    "                # Helper states (usually important for some kind of output)\n",
    "                dTHdt = r_d * p_d * p_h * I\n",
    "                dDVRdt = r_d * (1 - p_dth_mod) * p_d * p_h * p_v * I - r_rv * DVR\n",
    "                dDVDdt = r_d * p_dth_mod * p_d * p_h * p_v * I - r_dth * DVD\n",
    "                dDDdt = r_dth * (DHD + DQD)\n",
    "                dDTdt = r_d * p_d * I\n",
    "                return [\n",
    "                    dSdt, dEdt, dIdt, dARdt, dDHRdt, dDQRdt, dADdt, dDHDdt,\n",
    "                    dDQDdt, dRdt, dDdt, dTHdt, dDVRdt, dDVDdt, dDDdt, dDTdt,\n",
    "                ]\n",
    "\n",
    "            def residuals_totalcases(params) -> float:\n",
    "                \"\"\"\n",
    "                Function that makes sure the parameters are in the right range during the fitting process and computes\n",
    "                the loss function depending on the optimizer that has been chosen for this run as a global variable\n",
    "                :param params: currently fitted values of the parameters during the fitting process\n",
    "                :return: the value of the loss function as a float that is optimized against (in our case, minimized)\n",
    "                \"\"\"\n",
    "                # Variables Initialization for the ODE system\n",
    "                alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = params\n",
    "                # Force params values to stay in a certain range during the optimization process with re-initializations\n",
    "                params = (\n",
    "                    max(alpha, dict_default_reinit_parameters[\"alpha\"]),\n",
    "                    days,\n",
    "                    max(r_s, dict_default_reinit_parameters[\"r_s\"]),\n",
    "                    max(min(r_dth, 1), dict_default_reinit_parameters[\"r_dth\"]),\n",
    "                    max(min(p_dth, 1), dict_default_reinit_parameters[\"p_dth\"]),\n",
    "                    max(r_dthdecay, dict_default_reinit_parameters[\"r_dthdecay\"]),\n",
    "                    max(k1, dict_default_reinit_parameters[\"k1\"]),\n",
    "                    max(k2, dict_default_reinit_parameters[\"k2\"]),\n",
    "                    max(jump, dict_default_reinit_parameters[\"jump\"]),\n",
    "                    max(t_jump, dict_default_reinit_parameters[\"t_jump\"]),\n",
    "                    max(std_normal, dict_default_reinit_parameters[\"std_normal\"]),\n",
    "                    max(k3, dict_default_reinit_lower_bounds[\"k3\"]),\n",
    "                )\n",
    "\n",
    "                x_0_cases = get_initial_conditions(\n",
    "                    params_fitted=params, global_params_fixed=GLOBAL_PARAMS_FIXED\n",
    "                )\n",
    "                x_sol_total = solve_ivp(\n",
    "                    fun=model_covid,\n",
    "                    y0=x_0_cases,\n",
    "                    t_span=[t_cases[0], t_cases[-1]],\n",
    "                    t_eval=t_cases,\n",
    "                    args=tuple(params),\n",
    "                )\n",
    "                x_sol = x_sol_total.y\n",
    "                weights = list(range(1, len(cases_data_fit) + 1))\n",
    "                # weights = [(x/len(cases_data_fit))**2 for x in weights]\n",
    "                if x_sol_total.status == 0:\n",
    "                    residuals_value = get_residuals_value(\n",
    "                        optimizer=OPTIMIZER,\n",
    "                        balance=balance,\n",
    "                        x_sol=x_sol,\n",
    "                        cases_data_fit=cases_data_fit,\n",
    "                        deaths_data_fit=deaths_data_fit,\n",
    "                        weights=weights\n",
    "                    )\n",
    "                else:\n",
    "                    residuals_value = 1e16\n",
    "                return residuals_value\n",
    "\n",
    "            if OPTIMIZER in [\"tnc\", \"trust-constr\"]:\n",
    "                output = minimize(\n",
    "                    residuals_totalcases,\n",
    "                    parameter_list,\n",
    "                    method=OPTIMIZER,\n",
    "                    bounds=bounds_params,\n",
    "                    options={\"maxiter\": max_iter},\n",
    "                )\n",
    "            elif OPTIMIZER == \"annealing\":\n",
    "                output = dual_annealing(\n",
    "                    residuals_totalcases, x0=parameter_list, bounds=bounds_params\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Optimizer not in 'tnc', 'trust-constr' or 'annealing' so not supported\")\n",
    "\n",
    "            if (OPTIMIZER in [\"tnc\", \"trust-constr\"]) or (OPTIMIZER == \"annealing\" and output.success):\n",
    "                best_params = output.x\n",
    "                t_predictions = [i for i in range(maxT)]\n",
    "    \n",
    "                def solve_best_params_and_predict(optimal_params):\n",
    "                    # Variables Initialization for the ODE system\n",
    "                    alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = optimal_params\n",
    "                    optimal_params = [\n",
    "                        max(alpha, dict_default_reinit_parameters[\"alpha\"]),\n",
    "                        days,\n",
    "                        max(r_s, dict_default_reinit_parameters[\"r_s\"]),\n",
    "                        max(min(r_dth, 1), dict_default_reinit_parameters[\"r_dth\"]),\n",
    "                        max(min(p_dth, 1), dict_default_reinit_parameters[\"p_dth\"]),\n",
    "                        max(r_dthdecay, dict_default_reinit_parameters[\"r_dthdecay\"]),\n",
    "                        max(k1, dict_default_reinit_parameters[\"k1\"]),\n",
    "                        max(k2, dict_default_reinit_parameters[\"k2\"]),\n",
    "                        max(jump, dict_default_reinit_parameters[\"jump\"]),\n",
    "                        max(t_jump, dict_default_reinit_parameters[\"t_jump\"]),\n",
    "                        max(std_normal, dict_default_reinit_parameters[\"std_normal\"]),\n",
    "                        max(k3, dict_default_reinit_lower_bounds[\"k3\"]),\n",
    "                    ]\n",
    "                    x_0_cases = get_initial_conditions(\n",
    "                        params_fitted=optimal_params,\n",
    "                        global_params_fixed=GLOBAL_PARAMS_FIXED,\n",
    "                    )\n",
    "                    x_sol_best = solve_ivp(\n",
    "                        fun=model_covid,\n",
    "                        y0=x_0_cases,\n",
    "                        t_span=[t_predictions[0], t_predictions[-1]],\n",
    "                        t_eval=t_predictions,\n",
    "                        args=tuple(optimal_params),\n",
    "                    ).y\n",
    "                    return x_sol_best\n",
    "\n",
    "                x_sol_final = solve_best_params_and_predict(best_params)\n",
    "                data_creator = DELPHIDataCreator(\n",
    "                    x_sol_final=x_sol_final,\n",
    "                    date_day_since100=start_date,\n",
    "                    best_params=best_params,\n",
    "                    continent=continent,\n",
    "                    country=country,\n",
    "                    province=province,\n",
    "                    testing_data_included=False,\n",
    "                )\n",
    "                mape_data = get_mape_data_fitting(\n",
    "                    cases_data_fit=cases_data_fit, deaths_data_fit=deaths_data_fit, x_sol_final=x_sol_final\n",
    "                )\n",
    "                \n",
    "                logging.info(f\"In-Sample MAPE Last 15 Days {country, province}: {round(mape_data, 3)} %\")\n",
    "                logging.debug(f\"Best fitted parameters for {country, province}: {best_params}\")\n",
    "                df_parameters_area = data_creator.create_dataset_parameters(mape_data)\n",
    "                # Creating the datasets for predictions of this area\n",
    "                if GET_CONFIDENCE_INTERVALS:\n",
    "                   df_predictions_since_today_area, df_predictions_since_100_area = (\n",
    "                       data_creator.create_datasets_with_confidence_intervals(\n",
    "                           cases_data_fit, deaths_data_fit,\n",
    "                           past_prediction_file=PATH_TO_FOLDER_DANGER_MAP + f\"predicted/Global_V4_{past_prediction_date}.csv\",\n",
    "                           past_prediction_date=str(pd.to_datetime(past_prediction_date).date()))\n",
    "                   )\n",
    "                else:\n",
    "                    df_predictions_since_today_area, df_predictions_since_100_area = data_creator.create_datasets_predictions()\n",
    "                logging.info(\n",
    "                    f\"Finished predicting for Continent={continent}, Country={country} and Province={province} in \"\n",
    "                    + f\"{round(time.time() - time_entering, 2)} seconds\"\n",
    "                )\n",
    "                logging.info(\"--------------------------------------------------------------------------------------------\")\n",
    "                return (\n",
    "                    df_parameters_area,\n",
    "                    df_predictions_since_today_area,\n",
    "                    df_predictions_since_100_area,\n",
    "                    output,\n",
    "                )\n",
    "            else:\n",
    "                return None\n",
    "    else:  # file for that tuple (continent, country, province) doesn't exist in processed files\n",
    "        logging.info(\n",
    "            f\"Skipping Continent={continent}, Country={country} and Province={province} as no processed file available\"\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to predict for Africa, Burkina Faso, None\n",
      "starting to predict for Asia, Afghanistan, None\n",
      "starting to predict for Africa, Chad, None\n",
      "starting to predict for Europe, Austria, None\n",
      "starting to predict for Africa, Burundi, None\n",
      "starting to predict for Europe, Albania, None\n",
      "starting to predict for South America, Chile, None\n",
      "starting to predict for Africa, Algeria, None\n",
      "starting to predict for Africa, Cabo Verde, None\n",
      "starting to predict for Asia, Azerbaijan, None\n",
      "starting to predict for South America, Colombia, None\n",
      "starting to predict for Europe, Andorra, None\n",
      "starting to predict for Asia, Bahrain, None\n",
      "starting to predict for Africa, Comoros, None\n",
      "starting to predict for Africa, Angola, None\n",
      "starting to predict for Asia, Cambodia, None\n",
      "starting to predict for Asia, Bangladesh, None\n",
      "starting to predict for Africa, Congo (Brazzaville), None\n",
      "starting to predict for North America, Antigua and Barbuda, None\n",
      "starting to predict for Africa, Cameroon, None\n",
      "starting to predict for South America, Argentina, None\n",
      "starting to predict for North America, Canada, Alberta\n",
      "starting to predict for North America, Barbados, None\n",
      "starting to predict for Africa, Congo (Kinshasa), None\n",
      "starting to predict for Asia, Armenia, None\n",
      "starting to predict for North America, Canada, British Columbia\n",
      "starting to predict for North America, Costa Rica, None\n",
      "starting to predict for Oceania, Australia, Australian Capital Territory\n",
      "starting to predict for Europe, Belarus, None\n",
      "starting to predict for North America, Canada, Manitoba\n",
      "starting to predict for Oceania, Australia, New South Wales\n",
      "starting to predict for Africa, Cote d'Ivoire, None\n",
      "starting to predict for Europe, Belgium, None\n",
      "starting to predict for Europe, Croatia, None\n",
      "starting to predict for North America, Canada, New Brunswick\n",
      "starting to predict for Oceania, Australia, Queensland\n",
      "starting to predict for North America, Belize, None\n",
      "starting to predict for North America, Cuba, None\n",
      "starting to predict for Africa, Benin, None\n",
      "starting to predict for North America, Canada, Newfoundland and Labrador\n",
      "starting to predict for Europe, Cyprus, None\n",
      "starting to predict for Oceania, Australia, South Australia\n",
      "starting to predict for Asia, Bhutan, None\n",
      "starting to predict for North America, Canada, Nova Scotia\n",
      "starting to predict for Europe, Czechia, None\n",
      "starting to predict for Oceania, Australia, Tasmania\n",
      "starting to predict for North America, Canada, Ontario\n",
      "starting to predict for South America, Bolivia, None\n",
      "starting to predict for Oceania, Australia, Victoria\n",
      "starting to predict for Europe, Denmark, None\n",
      "starting to predict for Europe, Bosnia and Herzegovina, None\n",
      "starting to predict for North America, Canada, Quebec\n",
      "starting to predict for Oceania, Australia, Western Australia\n",
      "starting to predict for Africa, Djibouti, None\n",
      "starting to predict for North America, Canada, Saskatchewan\n",
      "starting to predict for North America, Dominican Republic, None\n",
      "starting to predict for Africa, Central African Republic, None\n",
      "starting to predict for South America, Ecuador, None\n",
      "starting to predict for Africa, Botswana, None\n",
      "starting to predict for South America, Brazil, None\n",
      "starting to predict for Africa, Egypt, None\n",
      "starting to predict for Europe, Bulgaria, None\n",
      "starting to predict for North America, El Salvador, None\n",
      "starting to predict for Asia, Japan, None\n",
      "starting to predict for Africa, Equatorial Guinea, None\n",
      "starting to predict for Africa, Madagascar, None\n",
      "starting to predict for Asia, Jordan, None\n",
      "starting to predict for Europe, Estonia, None\n",
      "starting to predict for Africa, Guinea, None\n",
      "starting to predict for Africa, Malawi, None\n",
      "starting to predict for Asia, Kazakhstan, None\n",
      "starting to predict for Africa, Ethiopia, None\n",
      "starting to predict for Africa, Guinea-Bissau, None\n",
      "starting to predict for Africa, Kenya, None\n",
      "starting to predict for Europe, Finland, None\n",
      "starting to predict for Europe, France, None\n",
      "starting to predict for South America, Guyana, None\n",
      "starting to predict for Asia, Korea, South, None\n",
      "starting to predict for Asia, Malaysia, None\n",
      "starting to predict for Africa, Gabon, None\n",
      "starting to predict for Europe, Kosovo, None\n",
      "starting to predict for Africa, Gambia, None\n",
      "starting to predict for Asia, Georgia, None\n",
      "starting to predict for Asia, Kuwait, None\n",
      "starting to predict for Europe, Latvia, None\n",
      "starting to predict for Europe, Germany, None\n",
      "starting to predict for North America, Haiti, None\n",
      "starting to predict for Asia, Lebanon, None\n",
      "starting to predict for North America, Honduras, None\n",
      "starting to predict for Africa, Lesotho, None\n",
      "starting to predict for Africa, Ghana, None\n",
      "starting to predict for Europe, Hungary, None\n",
      "starting to predict for Africa, Liberia, None\n",
      "starting to predict for Europe, Greece, None\n",
      "starting to predict for Europe, Iceland, None\n",
      "starting to predict for Africa, Libya, None\n",
      "starting to predict for North America, Guatemala, None\n",
      "starting to predict for Asia, India, None\n",
      "starting to predict for Europe, Liechtenstein, None\n",
      "starting to predict for Europe, Lithuania, None\n",
      "starting to predict for Africa, Namibia, None\n",
      "starting to predict for Asia, Indonesia, None\n",
      "starting to predict for Asia, Iran, None\n",
      "starting to predict for Asia, Nepal, None\n",
      "starting to predict for Asia, Iraq, None\n",
      "starting to predict for Europe, Netherlands, None\n",
      "starting to predict for Europe, Luxembourg, None\n",
      "starting to predict for Oceania, New Zealand, None\n",
      "starting to predict for Europe, Ireland, None\n",
      "starting to predict for North America, Nicaragua, None\n",
      "starting to predict for Asia, Israel, None\n",
      "starting to predict for Asia, Philippines, None\n",
      "starting to predict for Africa, Niger, None\n",
      "starting to predict for Europe, Italy, None\n",
      "starting to predict for Europe, Poland, None\n",
      "starting to predict for Africa, Jamaica, None\n",
      "starting to predict for Europe, Portugal, None\n",
      "starting to predict for Africa, Nigeria, None\n",
      "starting to predict for Asia, Qatar, None\n",
      "starting to predict for Europe, North Macedonia, None\n",
      "starting to predict for Europe, Slovakia, None\n",
      "starting to predict for Europe, Romania, None\n",
      "starting to predict for Europe, Norway, None\n",
      "starting to predict for Europe, Slovenia, None\n",
      "starting to predict for Asia, Russia, None\n",
      "starting to predict for Asia, Oman, None\n",
      "starting to predict for Africa, Somalia, None\n",
      "starting to predict for Asia, Pakistan, None\n",
      "starting to predict for Africa, Rwanda, None\n",
      "starting to predict for North America, Panama, None\n",
      "starting to predict for Africa, South Africa, None\n",
      "starting to predict for Europe, San Marino, None\n",
      "starting to predict for Africa, Sao Tome and Principe, None\n",
      "starting to predict for Africa, South Sudan, None\n",
      "starting to predict for Oceania, Papua New Guinea, None\n",
      "starting to predict for Asia, Saudi Arabia, None\n",
      "starting to predict for Europe, Spain, None\n",
      "starting to predict for Africa, Senegal, None\n",
      "starting to predict for South America, Paraguay, None\n",
      "starting to predict for Asia, Sri Lanka, None\n",
      "starting to predict for South America, Peru, None\n",
      "starting to predict for Europe, Serbia, None\n",
      "starting to predict for Africa, Sudan, None\n",
      "starting to predict for South America, Trinidad and Tobago, None\n",
      "starting to predict for Africa, Seychelles, None\n",
      "starting to predict for Africa, Tunisia, None\n",
      "starting to predict for Africa, Sierra Leone, None\n",
      "starting to predict for South America, Suriname, None\n",
      "starting to predict for Asia, Turkey, None\n",
      "starting to predict for Asia, Singapore, None\n",
      "starting to predict for Europe, Sweden, None\n",
      "starting to predict for North America, US, Alabama\n",
      "starting to predict for Europe, Switzerland, None\n",
      "starting to predict for North America, US, Alaska\n",
      "starting to predict for North America, US, Idaho\n",
      "starting to predict for Asia, Tajikistan, None\n",
      "starting to predict for North America, US, Arizona\n",
      "starting to predict for North America, US, Illinois\n",
      "starting to predict for Africa, Tanzania, None\n",
      "starting to predict for North America, US, Indiana\n",
      "starting to predict for Asia, Thailand, None\n",
      "starting to predict for North America, US, Arkansas\n",
      "starting to predict for North America, US, Iowa\n",
      "starting to predict for North America, US, California\n",
      "starting to predict for North America, US, Colorado\n",
      "starting to predict for North America, US, Connecticut\n",
      "starting to predict for North America, US, Delaware\n",
      "starting to predict for North America, US, Kansas\n",
      "starting to predict for North America, US, District of Columbia\n",
      "starting to predict for North America, US, Florida\n",
      "starting to predict for North America, US, Georgia\n",
      "starting to predict for North America, US, Hawaii\n",
      "starting to predict for North America, US, Kentucky\n",
      "starting to predict for North America, US, Nebraska\n",
      "starting to predict for North America, US, Louisiana\n",
      "starting to predict for North America, US, Nevada\n",
      "starting to predict for North America, US, Maine\n",
      "starting to predict for North America, US, New Hampshire\n",
      "starting to predict for North America, US, Maryland\n",
      "starting to predict for North America, US, New Jersey\n",
      "starting to predict for North America, US, New Mexico\n",
      "starting to predict for North America, US, Massachusetts\n",
      "starting to predict for North America, US, New York\n",
      "starting to predict for North America, US, North Carolina\n",
      "starting to predict for North America, US, Michigan\n",
      "starting to predict for North America, US, Minnesota\n",
      "starting to predict for North America, US, North Dakota\n",
      "starting to predict for North America, US, Mississippi\n",
      "starting to predict for North America, US, Ohio\n",
      "starting to predict for North America, US, Oklahoma\n",
      "starting to predict for North America, US, Missouri\n",
      "starting to predict for North America, US, Oregon\n",
      "starting to predict for North America, US, Montana\n",
      "starting to predict for North America, US, Pennsylvania\n",
      "starting to predict for North America, US, Tennessee\n",
      "starting to predict for North America, US, Rhode Island\n",
      "starting to predict for North America, US, South Carolina\n",
      "starting to predict for North America, US, Texas\n",
      "starting to predict for North America, US, Utah\n",
      "starting to predict for North America, US, South Dakota\n",
      "starting to predict for North America, US, Vermont\n",
      "starting to predict for North America, US, Virginia\n",
      "starting to predict for South America, Venezuela, None\n",
      "starting to predict for North America, US, Washington\n",
      "starting to predict for Asia, Vietnam, None\n",
      "starting to predict for North America, US, West Virginia\n",
      "starting to predict for North America, US, Wisconsin\n",
      "starting to predict for North America, US, Wyoming\n",
      "starting to predict for Africa, Uganda, None\n",
      "starting to predict for Europe, Ukraine, None\n",
      "starting to predict for Asia, United Arab Emirates, None\n",
      "starting to predict for Europe, United Kingdom, None\n",
      "starting to predict for South America, Uruguay, None\n",
      "starting to predict for Asia, Uzbekistan, None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_mape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-8-bd222581d34e>\", line 308, in solve_and_predict_area\n    cases_data_fit=cases_data_fit, deaths_data_fit=deaths_data_fit, x_sol_final=x_sol_final\n  File \"<string>\", line 13, in get_mape_data_fitting\nNameError: name 'compute_mape' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7a0c2bd385ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cpu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         for result_area in tqdm(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolve_and_predict_area_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         ):\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_mape' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    assert USER_RUNNING in CONFIG_FILEPATHS[\"delphi_repo\"].keys(), f\"User {USER_RUNNING} not referenced in config.yml\"\n",
    "    if not os.path.exists(CONFIG_FILEPATHS[\"logs\"][USER_RUNNING] + \"model_fitting/\"):\n",
    "        os.mkdir(CONFIG_FILEPATHS[\"logs\"][USER_RUNNING] + \"model_fitting/\")\n",
    "\n",
    "    logger_filename = (\n",
    "            CONFIG_FILEPATHS[\"logs\"][USER_RUNNING] +\n",
    "            f\"model_fitting/delphi_model_V4_{yesterday_logs_filename}_{OPTIMIZER}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        filename=logger_filename,\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "        datefmt=\"%m-%d-%Y %I:%M:%S %p\",\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"The user is {USER_RUNNING}, the chosen optimizer for this run was {OPTIMIZER} and \" +\n",
    "        f\"generation of Confidence Intervals' flag is {GET_CONFIDENCE_INTERVALS}\"\n",
    "    )\n",
    "    popcountries = pd.read_csv(\n",
    "        PATH_TO_FOLDER_DANGER_MAP + f\"processed/Population_Global.csv\"\n",
    "    )\n",
    "    popcountries[\"tuple_area\"] = list(zip(popcountries.Continent, popcountries.Country, popcountries.Province))\n",
    "\n",
    "    if not os.path.exists(PATH_TO_DATA_SANDBOX + f\"predicted/raw_predictions/Predicted_model_state_V4_{fitting_start_date}.csv\"):\n",
    "        logging.error(f\"Initial model state file not found, can not train from {fitting_start_date}. Use model_V4 to train on entire data.\")\n",
    "        raise FileNotFoundError\n",
    "    df_initial_states = pd.read_csv(\n",
    "        PATH_TO_DATA_SANDBOX + f\"predicted/raw_predictions/Predicted_model_state_V4_{fitting_start_date}.csv\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        past_parameters = pd.read_csv(\n",
    "            PATH_TO_FOLDER_DANGER_MAP\n",
    "            + f\"predicted/Parameters_Global_V4_{yesterday}.csv\"\n",
    "        )\n",
    "        print(PATH_TO_FOLDER_DANGER_MAP+ f\"predicted/Parameters_Global_V4_{yesterday}.csv\")\n",
    "    except:\n",
    "        past_parameters = None\n",
    "\n",
    "    ### Fitting the Model ###\n",
    "    # Initalizing lists of the different dataframes that will be concatenated in the end\n",
    "    list_df_global_predictions_since_today = []\n",
    "    list_df_global_predictions_since_100_cases = []\n",
    "    list_df_global_parameters = []\n",
    "    obj_value = 0\n",
    "    solve_and_predict_area_partial = partial(\n",
    "        solve_and_predict_area,\n",
    "        yesterday_=yesterday,\n",
    "        past_parameters_=past_parameters,\n",
    "        popcountries=popcountries,\n",
    "        startT=fitting_start_date\n",
    "    )\n",
    "    n_cpu = psutil.cpu_count(logical = False) \n",
    "    logging.info(f\"Number of CPUs found and used in this run: {n_cpu}\")\n",
    "    list_tuples = [(\n",
    "        r.continent, \n",
    "        r.country, \n",
    "        r.province, \n",
    "        r.values[:16] if not pd.isna(r.S) else None\n",
    "        ) for _, r in df_initial_states.iterrows()]\n",
    "\n",
    "#    list_tuples = [t for t in list_tuples if t[2] in [\"Connecticut\"]]\n",
    "    # , \"Poland\", \"Belgium\", \"France\", \"Greece\"]]\n",
    "\n",
    "    logging.info(f\"Number of areas to be fitted in this run: {len(list_tuples)}\")\n",
    "    with mp.Pool(n_cpu) as pool:\n",
    "        for result_area in tqdm(\n",
    "            pool.map_async(solve_and_predict_area_partial, list_tuples).get(),\n",
    "            total=len(list_tuples),\n",
    "        ):\n",
    "            if result_area is not None:\n",
    "                (\n",
    "                    df_parameters_area,\n",
    "                    df_predictions_since_today_area,\n",
    "                    df_predictions_since_100_area,\n",
    "                    output,\n",
    "                ) = result_area\n",
    "                obj_value = obj_value + output.fun\n",
    "                # Then we add it to the list of df to be concatenated to update the tracking df\n",
    "                list_df_global_parameters.append(df_parameters_area)\n",
    "                list_df_global_predictions_since_today.append(df_predictions_since_today_area)\n",
    "                list_df_global_predictions_since_100_cases.append(df_predictions_since_100_area)\n",
    "            else:\n",
    "                continue\n",
    "        logging.info(\"Finished the Multiprocessing for all areas\")\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # Appending parameters, aggregations per country, per continent, and for the world\n",
    "    # for predictions today & since 100\n",
    "    today_date_str = \"\".join(str(datetime.now().date()).split(\"-\"))\n",
    "    df_global_parameters = pd.concat(list_df_global_parameters).sort_values(\n",
    "        [\"Country\", \"Province\"]\n",
    "    ).reset_index(drop=True)\n",
    "    df_global_predictions_since_today = pd.concat(list_df_global_predictions_since_today)\n",
    "    df_global_predictions_since_today = DELPHIAggregations.append_all_aggregations(\n",
    "        df_global_predictions_since_today\n",
    "    )\n",
    "    df_global_predictions_since_100_cases = pd.concat(list_df_global_predictions_since_100_cases)\n",
    "    if GET_CONFIDENCE_INTERVALS:\n",
    "        df_global_predictions_since_today, df_global_predictions_since_100_cases = DELPHIAggregations.append_all_aggregations_cf(\n",
    "            df_global_predictions_since_100_cases,\n",
    "            past_prediction_file=PATH_TO_FOLDER_DANGER_MAP + f\"predicted/Global_V4_{past_prediction_date}.csv\",\n",
    "            past_prediction_date=str(pd.to_datetime(past_prediction_date).date())\n",
    "        )\n",
    "    else:\n",
    "        df_global_predictions_since_100_cases = DELPHIAggregations.append_all_aggregations(\n",
    "            df_global_predictions_since_100_cases\n",
    "        )\n",
    "\n",
    "    logger = logging.getLogger(\"V4Logger\")\n",
    "    delphi_data_saver = DELPHIDataSaver(\n",
    "        path_to_folder_danger_map=PATH_TO_FOLDER_DANGER_MAP,\n",
    "        path_to_website_predicted=PATH_TO_WEBSITE_PREDICTED,\n",
    "        df_global_parameters=df_global_parameters,\n",
    "        df_global_predictions_since_today=df_global_predictions_since_today,\n",
    "        df_global_predictions_since_100_cases=df_global_predictions_since_100_cases,\n",
    "        logger=logger\n",
    "    )\n",
    "    delphi_data_saver.save_all_datasets(optimizer=OPTIMIZER, save_since_100_cases=SAVE_SINCE100_CASES, website=SAVE_TO_WEBSITE)\n",
    "    logging.info(\n",
    "        f\"Exported all 3 datasets to website & danger_map repositories, \"\n",
    "        + f\"total runtime was {round((time.time() - time_beginning)/60, 2)} minutes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

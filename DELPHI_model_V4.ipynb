{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/bhoom/.local/lib/python3.6/site-packages (from scipy) (1.18.3)\n",
      "Installing collected packages: scipy\n",
      "\u001b[33m  WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /home/bhoom/.local/include/python3.6m/scipy\n",
      "  sysconfig: /home/bhoom/.local/include/python3.6/scipy\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = True\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/bhoom/.local/include/python3.6m/UNKNOWN\n",
      "sysconfig: /home/bhoom/.local/include/python3.6/UNKNOWN\u001b[0m\n",
      "Successfully installed scipy-1.5.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user psutil\n",
    "# !pip install --user import_ipynb\n",
    "# !pip install --user scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from DELPHI_utils_V4_static.ipynb\n",
      "importing Jupyter notebook from DELPHI_utils_V4_dynamic.ipynb\n",
      "importing Jupyter notebook from DELPHI_params_V4.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Authors: Hamza Tazi Bouardi (htazi@mit.edu), Michael L. Li (mlli@mit.edu), Omar Skali Lami (oskali@mit.edu)\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import import_ipynb\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import dual_annealing\n",
    "from DELPHI_utils_V4_static import (\n",
    "    DELPHIAggregations, DELPHIDataSaver, DELPHIDataCreator, get_initial_conditions,\n",
    "    get_mape_data_fitting, create_fitting_data_from_validcases, get_residuals_value\n",
    ")\n",
    "from DELPHI_utils_V4_dynamic import get_bounds_params_from_pastparams\n",
    "from DELPHI_params_V4 import (\n",
    "    fitting_start_date,\n",
    "    default_parameter_list,\n",
    "    dict_default_reinit_parameters,\n",
    "    dict_default_reinit_lower_bounds,\n",
    "    dict_default_reinit_upper_bounds,\n",
    "    default_upper_bound,\n",
    "    default_lower_bound,\n",
    "    percentage_drift_upper_bound,\n",
    "    percentage_drift_lower_bound,\n",
    "    percentage_drift_upper_bound_annealing,\n",
    "    percentage_drift_lower_bound_annealing,\n",
    "    default_upper_bound_annealing,\n",
    "    default_lower_bound_annealing,\n",
    "    default_lower_bound_t_jump,\n",
    "    default_upper_bound_t_jump,\n",
    "    default_lower_bound_std_normal,\n",
    "    default_upper_bound_std_normal,\n",
    "    default_bounds_params,\n",
    "    validcases_threshold,\n",
    "    IncubeD,\n",
    "    RecoverID,\n",
    "    RecoverHD,\n",
    "    DetectD,\n",
    "    VentilatedD,\n",
    "    default_maxT,\n",
    "    p_v,\n",
    "    p_d,\n",
    "    p_h,\n",
    "    max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing Global Variables ##########################################################################\n",
    "with open(\"config.yml\", \"r\") as ymlfile:\n",
    "    CONFIG = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "CONFIG_FILEPATHS = CONFIG[\"filepaths\"]\n",
    "time_beginning = time.time()\n",
    "yesterday = \"\".join(str(datetime.now().date() - timedelta(days=1)).split(\"-\"))\n",
    "yesterday_logs_filename = \"\".join(\n",
    "    (str(datetime.now().date() - timedelta(days=1)) + f\"_{datetime.now().hour}H{datetime.now().minute}M\").split(\"-\")\n",
    ")\n",
    "with open('run_configs/run-config.yml', \"r\") as ymlfile:\n",
    "    RUN_CONFIG = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "USER_RUNNING = RUN_CONFIG[\"arguments\"][\"user\"]\n",
    "OPTIMIZER = RUN_CONFIG[\"arguments\"][\"optimizer\"]\n",
    "GET_CONFIDENCE_INTERVALS = bool(int(RUN_CONFIG[\"arguments\"][\"confidence_intervals\"]))\n",
    "SAVE_TO_WEBSITE = bool(int(RUN_CONFIG[\"arguments\"][\"website\"]))\n",
    "SAVE_SINCE100_CASES = bool(int(RUN_CONFIG[\"arguments\"][\"since100case\"]))\n",
    "PATH_TO_FOLDER_DANGER_MAP = CONFIG_FILEPATHS[\"danger_map\"][USER_RUNNING]\n",
    "PATH_TO_DATA_SANDBOX = CONFIG_FILEPATHS[\"data_sandbox\"][USER_RUNNING]\n",
    "PATH_TO_WEBSITE_PREDICTED = CONFIG_FILEPATHS[\"website\"][USER_RUNNING]\n",
    "past_prediction_date = \"\".join(str(datetime.now().date() - timedelta(days=14)).split(\"-\"))\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DELPHI function sove_and_predict_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_and_predict_area(\n",
    "        tuple_area_state_: tuple,\n",
    "        yesterday_: str,\n",
    "        past_parameters_: pd.DataFrame,\n",
    "        popcountries: pd.DataFrame,\n",
    "        startT: str = None, # added to change optimmization start date\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallelizable version of the fitting & solving process for DELPHI V4, this function is called with multiprocessing\n",
    "    :param tuple_area_: tuple corresponding to (continent, country, province)\n",
    "    :param yesterday_: string corresponding to the date from which the model will read the previous parameters. The\n",
    "    format has to be 'YYYYMMDD'\n",
    "    :param past_parameters_: Parameters from yesterday_ used as a starting point for the fitting process\n",
    "    :param popcountries: DataFrame containing population information for all countries and provinces\n",
    "    :startT: string for the date from when the pandemic will be modelled (format should be 'YYYY-MM-DD')\n",
    "    :return: either None if can't optimize (either less than 100 cases or less than 7 days with 100 cases) or a tuple\n",
    "    with 3 dataframes related to that tuple_area_ (parameters df, predictions since yesterday_+1, predictions since\n",
    "    first day with 100 cases) and a scipy.optimize object (OptimizeResult) that contains the predictions for all\n",
    "    16 states of the model (and some other information that isn't used)\n",
    "    \"\"\"\n",
    "    time_entering = time.time()\n",
    "    continent, country, province, initial_state = tuple_area_state_\n",
    "    country_sub = country.replace(\" \", \"_\")\n",
    "    province_sub = province.replace(\" \", \"_\")\n",
    "    print(f\"starting to predict for {continent}, {country}, {province}\")\n",
    "    if os.path.exists(PATH_TO_FOLDER_DANGER_MAP + f\"processed/Cases_{country_sub}_{province_sub}.csv\"):\n",
    "        totalcases = pd.read_csv(\n",
    "            PATH_TO_FOLDER_DANGER_MAP + f\"processed/Cases_{country_sub}_{province_sub}.csv\"\n",
    "        )\n",
    "        if totalcases.day_since100.max() < 0:\n",
    "            logging.warning(\n",
    "                f\"Not enough cases (less than 100) for Continent={continent}, Country={country} and Province={province}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if past_parameters_ is not None:\n",
    "            parameter_list_total = past_parameters_[\n",
    "                (past_parameters_.Country == country)\n",
    "                & (past_parameters_.Province == province)\n",
    "            ].reset_index(drop=True)\n",
    "            if len(parameter_list_total) > 0:\n",
    "                parameter_list_line = parameter_list_total.iloc[-1, :].values.tolist()\n",
    "                parameter_list = parameter_list_line[5:]\n",
    "                bounds_params = get_bounds_params_from_pastparams(\n",
    "                    optimizer=OPTIMIZER,\n",
    "                    parameter_list=parameter_list,\n",
    "                    dict_default_reinit_parameters=dict_default_reinit_parameters,\n",
    "                    percentage_drift_lower_bound=percentage_drift_lower_bound,\n",
    "                    default_lower_bound=default_lower_bound,\n",
    "                    dict_default_reinit_lower_bounds=dict_default_reinit_lower_bounds,\n",
    "                    percentage_drift_upper_bound=percentage_drift_upper_bound,\n",
    "                    default_upper_bound=default_upper_bound,\n",
    "                    dict_default_reinit_upper_bounds=dict_default_reinit_upper_bounds,\n",
    "                    percentage_drift_lower_bound_annealing=percentage_drift_lower_bound_annealing,\n",
    "                    default_lower_bound_annealing=default_lower_bound_annealing,\n",
    "                    percentage_drift_upper_bound_annealing=percentage_drift_upper_bound_annealing,\n",
    "                    default_upper_bound_annealing=default_upper_bound_annealing,\n",
    "                    default_lower_bound_t_jump=default_lower_bound_t_jump,\n",
    "                    default_upper_bound_t_jump=default_upper_bound_t_jump,\n",
    "                    default_lower_bound_std_normal=default_lower_bound_std_normal,\n",
    "                    default_upper_bound_std_normal=default_upper_bound_std_normal,\n",
    "                )\n",
    "                start_date = pd.to_datetime(parameter_list_line[3])\n",
    "                bounds_params = tuple(bounds_params)\n",
    "            else:\n",
    "                # Otherwise use established lower/upper bounds\n",
    "                parameter_list = default_parameter_list\n",
    "                bounds_params = default_bounds_params\n",
    "                start_date = pd.to_datetime(totalcases.loc[totalcases.day_since100 == 0, \"date\"].iloc[-1])\n",
    "        else:\n",
    "            # Otherwise use established lower/upper bounds\n",
    "            parameter_list = default_parameter_list\n",
    "            bounds_params = default_bounds_params\n",
    "            start_date = pd.to_datetime(totalcases.loc[totalcases.day_since100 == 0, \"date\"].iloc[-1])\n",
    "\n",
    "        if startT is not None:\n",
    "            input_start_date = pd.to_datetime(startT)\n",
    "            if input_start_date > start_date:\n",
    "                delta_days = (input_start_date - start_date).days\n",
    "                parameter_list[9] = parameter_list[9] - delta_days\n",
    "                bounds_params_list = list(bounds_params)\n",
    "                bounds_params_list[9] = (bounds_params_list[9][0]-delta_days, bounds_params_list[9][1]-delta_days)\n",
    "                bounds_params = tuple(bounds_params_list)\n",
    "                start_date = input_start_date\n",
    "            validcases = totalcases[\n",
    "                (totalcases.date >= str(start_date))\n",
    "                & (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "            ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "        else:\n",
    "            validcases = totalcases[\n",
    "                (totalcases.day_since100 >= 0)\n",
    "                & (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "            ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "        # Now we start the modeling part:\n",
    "        if len(validcases) <= validcases_threshold:\n",
    "            logging.warning(\n",
    "                f\"Not enough historical data (less than a week)\"\n",
    "                + f\"for Continent={continent}, Country={country} and Province={province}\"\n",
    "            )\n",
    "            return None\n",
    "        else:\n",
    "            PopulationT = popcountries[\n",
    "                (popcountries.Country == country) & (popcountries.Province == province)\n",
    "            ].pop2016.iloc[-1]\n",
    "            N = PopulationT\n",
    "            PopulationI = validcases.loc[0, \"case_cnt\"]\n",
    "            PopulationD = validcases.loc[0, \"death_cnt\"]\n",
    "            if initial_state is not None:\n",
    "                R_0 = initial_state[9]\n",
    "            else:\n",
    "                R_0 = validcases.loc[0, \"death_cnt\"] * 5 if validcases.loc[0, \"case_cnt\"] - validcases.loc[0, \"death_cnt\"]> validcases.loc[0, \"death_cnt\"] * 5 else 0\n",
    "                bounds_params_list = list(bounds_params)\n",
    "                bounds_params_list[-1] = (0.999,1)\n",
    "                bounds_params = tuple(bounds_params_list)\n",
    "            cases_t_14days = totalcases[totalcases.date >= str(start_date- pd.Timedelta(14, 'D'))]['case_cnt'].values[0]\n",
    "            deaths_t_9days = totalcases[totalcases.date >= str(start_date - pd.Timedelta(9, 'D'))]['death_cnt'].values[0]\n",
    "            R_upperbound = validcases.loc[0, \"case_cnt\"] - validcases.loc[0, \"death_cnt\"]\n",
    "            R_heuristic = cases_t_14days - deaths_t_9days\n",
    "            if int(R_0*p_d) >= R_upperbound and R_heuristic >= R_upperbound:\n",
    "                logging.error(f\"Initial conditions for PopulationR too high for {country}-{province}, on {startT}\")\n",
    "            \"\"\"\n",
    "            Fixed Parameters based on meta-analysis:\n",
    "            p_h: Hospitalization Percentage\n",
    "            RecoverHD: Average Days until Recovery\n",
    "            VentilationD: Number of Days on Ventilation for Ventilated Patients\n",
    "            maxT: Maximum # of Days Modeled\n",
    "            p_d: Percentage of True Cases Detected\n",
    "            p_v: Percentage of Hospitalized Patients Ventilated,\n",
    "            balance: Regularization coefficient between cases and deaths\n",
    "            \"\"\"\n",
    "            maxT = (default_maxT - start_date).days + 1\n",
    "            t_cases = validcases[\"day_since100\"].tolist() - validcases.loc[0, \"day_since100\"]\n",
    "            balance, cases_data_fit, deaths_data_fit = create_fitting_data_from_validcases(validcases)\n",
    "            GLOBAL_PARAMS_FIXED = (N, R_upperbound, R_heuristic, R_0, PopulationD, PopulationI, p_d, p_h, p_v)\n",
    "\n",
    "            def model_covid(\n",
    "                t, x, alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3\n",
    "            ) -> list:\n",
    "                \"\"\"\n",
    "                SEIR based model with 16 distinct states, taking into account undetected, deaths, hospitalized and\n",
    "                recovered, and using an ArcTan government response curve, corrected with a Gaussian jump in case of\n",
    "                a resurgence in cases\n",
    "                :param t: time step\n",
    "                :param x: set of all the states in the model (here, 16 of them)\n",
    "                :param alpha: Infection rate\n",
    "                :param days: Median day of action (used in the arctan governmental response)\n",
    "                :param r_s: Median rate of action (used in the arctan governmental response)\n",
    "                :param r_dth: Rate of death\n",
    "                :param p_dth: Initial mortality percentage\n",
    "                :param r_dthdecay: Rate of decay of mortality percentage\n",
    "                :param k1: Internal parameter 1 (used for initial conditions)\n",
    "                :param k2: Internal parameter 2 (used for initial conditions)\n",
    "                :param jump: Amplitude of the Gaussian jump modeling the resurgence in cases\n",
    "                :param t_jump: Time where the Gaussian jump will reach its maximum value\n",
    "                :param std_normal: Standard Deviation of the Gaussian jump (~ time span of the resurgence in cases)\n",
    "                :param k3: Internal parameter 2 (used for initial conditions)\n",
    "                :return: predictions for all 16 states, which are the following\n",
    "                [0 S, 1 E, 2 I, 3 UR, 4 DHR, 5 DQR, 6 UD, 7 DHD, 8 DQD, 9 R, 10 D, 11 TH, 12 DVR,13 DVD, 14 DD, 15 DT]\n",
    "                \"\"\"\n",
    "                r_i = np.log(2) / IncubeD  # Rate of infection leaving incubation phase\n",
    "                r_d = np.log(2) / DetectD  # Rate of detection\n",
    "                r_ri = np.log(2) / RecoverID  # Rate of recovery not under infection\n",
    "                r_rh = np.log(2) / RecoverHD  # Rate of recovery under hospitalization\n",
    "                r_rv = np.log(2) / VentilatedD  # Rate of recovery under ventilation\n",
    "                gamma_t = (\n",
    "                    (2 / np.pi) * np.arctan(-(t - days) / 20 * r_s) + 1\n",
    "                    + jump * np.exp(-(t - t_jump) ** 2 / (2 * std_normal ** 2))\n",
    "                )\n",
    "                p_dth_mod = (2 / np.pi) * (p_dth - 0.001) * (np.arctan(-t / 20 * r_dthdecay) + np.pi / 2) + 0.001\n",
    "                assert (\n",
    "                    len(x) == 16\n",
    "                ), f\"Too many input variables, got {len(x)}, expected 16\"\n",
    "                S, E, I, AR, DHR, DQR, AD, DHD, DQD, R, D, TH, DVR, DVD, DD, DT = x\n",
    "                # Equations on main variables\n",
    "                dSdt = -alpha * gamma_t * S * I / N\n",
    "                dEdt = alpha * gamma_t * S * I / N - r_i * E\n",
    "                dIdt = r_i * E - r_d * I\n",
    "                dARdt = r_d * (1 - p_dth_mod) * (1 - p_d) * I - r_ri * AR\n",
    "                dDHRdt = r_d * (1 - p_dth_mod) * p_d * p_h * I - r_rh * DHR\n",
    "                dDQRdt = r_d * (1 - p_dth_mod) * p_d * (1 - p_h) * I - r_ri * DQR\n",
    "                dADdt = r_d * p_dth_mod * (1 - p_d) * I - r_dth * AD\n",
    "                dDHDdt = r_d * p_dth_mod * p_d * p_h * I - r_dth * DHD\n",
    "                dDQDdt = r_d * p_dth_mod * p_d * (1 - p_h) * I - r_dth * DQD\n",
    "                dRdt = r_ri * (AR + DQR) + r_rh * DHR\n",
    "                dDdt = r_dth * (AD + DQD + DHD)\n",
    "                # Helper states (usually important for some kind of output)\n",
    "                dTHdt = r_d * p_d * p_h * I\n",
    "                dDVRdt = r_d * (1 - p_dth_mod) * p_d * p_h * p_v * I - r_rv * DVR\n",
    "                dDVDdt = r_d * p_dth_mod * p_d * p_h * p_v * I - r_dth * DVD\n",
    "                dDDdt = r_dth * (DHD + DQD)\n",
    "                dDTdt = r_d * p_d * I\n",
    "                return [\n",
    "                    dSdt, dEdt, dIdt, dARdt, dDHRdt, dDQRdt, dADdt, dDHDdt,\n",
    "                    dDQDdt, dRdt, dDdt, dTHdt, dDVRdt, dDVDdt, dDDdt, dDTdt,\n",
    "                ]\n",
    "\n",
    "            def residuals_totalcases(params) -> float:\n",
    "                \"\"\"\n",
    "                Function that makes sure the parameters are in the right range during the fitting process and computes\n",
    "                the loss function depending on the optimizer that has been chosen for this run as a global variable\n",
    "                :param params: currently fitted values of the parameters during the fitting process\n",
    "                :return: the value of the loss function as a float that is optimized against (in our case, minimized)\n",
    "                \"\"\"\n",
    "                # Variables Initialization for the ODE system\n",
    "                alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = params\n",
    "                # Force params values to stay in a certain range during the optimization process with re-initializations\n",
    "                params = (\n",
    "                    max(alpha, dict_default_reinit_parameters[\"alpha\"]),\n",
    "                    days,\n",
    "                    max(r_s, dict_default_reinit_parameters[\"r_s\"]),\n",
    "                    max(min(r_dth, 1), dict_default_reinit_parameters[\"r_dth\"]),\n",
    "                    max(min(p_dth, 1), dict_default_reinit_parameters[\"p_dth\"]),\n",
    "                    max(r_dthdecay, dict_default_reinit_parameters[\"r_dthdecay\"]),\n",
    "                    max(k1, dict_default_reinit_parameters[\"k1\"]),\n",
    "                    max(k2, dict_default_reinit_parameters[\"k2\"]),\n",
    "                    max(jump, dict_default_reinit_parameters[\"jump\"]),\n",
    "                    max(t_jump, dict_default_reinit_parameters[\"t_jump\"]),\n",
    "                    max(std_normal, dict_default_reinit_parameters[\"std_normal\"]),\n",
    "                    max(k3, dict_default_reinit_lower_bounds[\"k3\"]),\n",
    "                )\n",
    "\n",
    "                x_0_cases = get_initial_conditions(\n",
    "                    params_fitted=params, global_params_fixed=GLOBAL_PARAMS_FIXED\n",
    "                )\n",
    "                x_sol_total = solve_ivp(\n",
    "                    fun=model_covid,\n",
    "                    y0=x_0_cases,\n",
    "                    t_span=[t_cases[0], t_cases[-1]],\n",
    "                    t_eval=t_cases,\n",
    "                    args=tuple(params),\n",
    "                )\n",
    "                x_sol = x_sol_total.y\n",
    "                weights = list(range(1, len(cases_data_fit) + 1))\n",
    "                # weights = [(x/len(cases_data_fit))**2 for x in weights]\n",
    "                if x_sol_total.status == 0:\n",
    "                    residuals_value = get_residuals_value(\n",
    "                        optimizer=OPTIMIZER,\n",
    "                        balance=balance,\n",
    "                        x_sol=x_sol,\n",
    "                        cases_data_fit=cases_data_fit,\n",
    "                        deaths_data_fit=deaths_data_fit,\n",
    "                        weights=weights\n",
    "                    )\n",
    "                else:\n",
    "                    residuals_value = 1e16\n",
    "                return residuals_value\n",
    "\n",
    "            if OPTIMIZER in [\"tnc\", \"trust-constr\"]:\n",
    "                output = minimize(\n",
    "                    residuals_totalcases,\n",
    "                    parameter_list,\n",
    "                    method=OPTIMIZER,\n",
    "                    bounds=bounds_params,\n",
    "                    options={\"maxiter\": max_iter},\n",
    "                )\n",
    "            elif OPTIMIZER == \"annealing\":\n",
    "                output = dual_annealing(\n",
    "                    residuals_totalcases, x0=parameter_list, bounds=bounds_params\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Optimizer not in 'tnc', 'trust-constr' or 'annealing' so not supported\")\n",
    "\n",
    "            if (OPTIMIZER in [\"tnc\", \"trust-constr\"]) or (OPTIMIZER == \"annealing\" and output.success):\n",
    "                best_params = output.x\n",
    "                t_predictions = [i for i in range(maxT)]\n",
    "    \n",
    "                def solve_best_params_and_predict(optimal_params):\n",
    "                    # Variables Initialization for the ODE system\n",
    "                    alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal, k3 = optimal_params\n",
    "                    optimal_params = [\n",
    "                        max(alpha, dict_default_reinit_parameters[\"alpha\"]),\n",
    "                        days,\n",
    "                        max(r_s, dict_default_reinit_parameters[\"r_s\"]),\n",
    "                        max(min(r_dth, 1), dict_default_reinit_parameters[\"r_dth\"]),\n",
    "                        max(min(p_dth, 1), dict_default_reinit_parameters[\"p_dth\"]),\n",
    "                        max(r_dthdecay, dict_default_reinit_parameters[\"r_dthdecay\"]),\n",
    "                        max(k1, dict_default_reinit_parameters[\"k1\"]),\n",
    "                        max(k2, dict_default_reinit_parameters[\"k2\"]),\n",
    "                        max(jump, dict_default_reinit_parameters[\"jump\"]),\n",
    "                        max(t_jump, dict_default_reinit_parameters[\"t_jump\"]),\n",
    "                        max(std_normal, dict_default_reinit_parameters[\"std_normal\"]),\n",
    "                        max(k3, dict_default_reinit_lower_bounds[\"k3\"]),\n",
    "                    ]\n",
    "                    x_0_cases = get_initial_conditions(\n",
    "                        params_fitted=optimal_params,\n",
    "                        global_params_fixed=GLOBAL_PARAMS_FIXED,\n",
    "                    )\n",
    "                    x_sol_best = solve_ivp(\n",
    "                        fun=model_covid,\n",
    "                        y0=x_0_cases,\n",
    "                        t_span=[t_predictions[0], t_predictions[-1]],\n",
    "                        t_eval=t_predictions,\n",
    "                        args=tuple(optimal_params),\n",
    "                    ).y\n",
    "                    return x_sol_best\n",
    "\n",
    "                x_sol_final = solve_best_params_and_predict(best_params)\n",
    "                data_creator = DELPHIDataCreator(\n",
    "                    x_sol_final=x_sol_final,\n",
    "                    date_day_since100=start_date,\n",
    "                    best_params=best_params,\n",
    "                    continent=continent,\n",
    "                    country=country,\n",
    "                    province=province,\n",
    "                    testing_data_included=False,\n",
    "                )\n",
    "                mape_data = get_mape_data_fitting(\n",
    "                    cases_data_fit=cases_data_fit, deaths_data_fit=deaths_data_fit, x_sol_final=x_sol_final\n",
    "                )\n",
    "                \n",
    "                logging.info(f\"In-Sample MAPE Last 15 Days {country, province}: {round(mape_data, 3)} %\")\n",
    "                logging.debug(f\"Best fitted parameters for {country, province}: {best_params}\")\n",
    "                df_parameters_area = data_creator.create_dataset_parameters(mape_data)\n",
    "                # Creating the datasets for predictions of this area\n",
    "                if GET_CONFIDENCE_INTERVALS:\n",
    "                   df_predictions_since_today_area, df_predictions_since_100_area = (\n",
    "                       data_creator.create_datasets_with_confidence_intervals(\n",
    "                           cases_data_fit, deaths_data_fit,\n",
    "                           past_prediction_file=PATH_TO_FOLDER_DANGER_MAP + f\"predicted/Global_V4_{past_prediction_date}.csv\",\n",
    "                           past_prediction_date=str(pd.to_datetime(past_prediction_date).date()))\n",
    "                   )\n",
    "                else:\n",
    "                    df_predictions_since_today_area, df_predictions_since_100_area = data_creator.create_datasets_predictions()\n",
    "                logging.info(\n",
    "                    f\"Finished predicting for Continent={continent}, Country={country} and Province={province} in \"\n",
    "                    + f\"{round(time.time() - time_entering, 2)} seconds\"\n",
    "                )\n",
    "                logging.info(\"--------------------------------------------------------------------------------------------\")\n",
    "                return (\n",
    "                    df_parameters_area,\n",
    "                    df_predictions_since_today_area,\n",
    "                    df_predictions_since_100_area,\n",
    "                    output,\n",
    "                )\n",
    "            else:\n",
    "                return None\n",
    "    else:  # file for that tuple (continent, country, province) doesn't exist in processed files\n",
    "        logging.info(\n",
    "            f\"Skipping Continent={continent}, Country={country} and Province={province} as no processed file available\"\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    assert USER_RUNNING in CONFIG_FILEPATHS[\"delphi_repo\"].keys(), f\"User {USER_RUNNING} not referenced in config.yml\"\n",
    "    if not os.path.exists(CONFIG_FILEPATHS[\"logs\"][USER_RUNNING] + \"model_fitting/\"):\n",
    "        os.mkdir(CONFIG_FILEPATHS[\"logs\"][USER_RUNNING] + \"model_fitting/\")\n",
    "\n",
    "    logger_filename = (\n",
    "            CONFIG_FILEPATHS[\"logs\"][USER_RUNNING] +\n",
    "            f\"model_fitting/delphi_model_V4_{yesterday_logs_filename}_{OPTIMIZER}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        filename=logger_filename,\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "        datefmt=\"%m-%d-%Y %I:%M:%S %p\",\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"The user is {USER_RUNNING}, the chosen optimizer for this run was {OPTIMIZER} and \" +\n",
    "        f\"generation of Confidence Intervals' flag is {GET_CONFIDENCE_INTERVALS}\"\n",
    "    )\n",
    "    popcountries = pd.read_csv(\n",
    "        PATH_TO_FOLDER_DANGER_MAP + f\"processed/Population_Global.csv\"\n",
    "    )\n",
    "    popcountries[\"tuple_area\"] = list(zip(popcountries.Continent, popcountries.Country, popcountries.Province))\n",
    "\n",
    "    if not os.path.exists(PATH_TO_DATA_SANDBOX + f\"predicted/raw_predictions/Predicted_model_state_V4_{fitting_start_date}.csv\"):\n",
    "        logging.error(f\"Initial model state file not found, can not train from {fitting_start_date}. Use model_V4 to train on entire data.\")\n",
    "        raise FileNotFoundError\n",
    "    df_initial_states = pd.read_csv(\n",
    "        PATH_TO_DATA_SANDBOX + f\"predicted/raw_predictions/Predicted_model_state_V4_{fitting_start_date}.csv\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        past_parameters = pd.read_csv(\n",
    "            PATH_TO_FOLDER_DANGER_MAP\n",
    "            + f\"predicted/Parameters_Global_V4_{yesterday}.csv\"\n",
    "        )\n",
    "        print(PATH_TO_FOLDER_DANGER_MAP+ f\"predicted/Parameters_Global_V4_{yesterday}.csv\")\n",
    "    except:\n",
    "        past_parameters = None\n",
    "\n",
    "    ### Fitting the Model ###\n",
    "    # Initalizing lists of the different dataframes that will be concatenated in the end\n",
    "    list_df_global_predictions_since_today = []\n",
    "    list_df_global_predictions_since_100_cases = []\n",
    "    list_df_global_parameters = []\n",
    "    obj_value = 0\n",
    "    solve_and_predict_area_partial = partial(\n",
    "        solve_and_predict_area,\n",
    "        yesterday_=yesterday,\n",
    "        past_parameters_=past_parameters,\n",
    "        popcountries=popcountries,\n",
    "        startT=fitting_start_date\n",
    "    )\n",
    "    n_cpu = psutil.cpu_count(logical = False) \n",
    "    logging.info(f\"Number of CPUs found and used in this run: {n_cpu}\")\n",
    "    list_tuples = [(\n",
    "        r.continent, \n",
    "        r.country, \n",
    "        r.province, \n",
    "        r.values[:16] if not pd.isna(r.S) else None\n",
    "        ) for _, r in df_initial_states.iterrows()]\n",
    "\n",
    "#    list_tuples = [t for t in list_tuples if t[2] in [\"Connecticut\"]]\n",
    "    # , \"Poland\", \"Belgium\", \"France\", \"Greece\"]]\n",
    "\n",
    "    logging.info(f\"Number of areas to be fitted in this run: {len(list_tuples)}\")\n",
    "    with mp.Pool(n_cpu) as pool:\n",
    "        for result_area in tqdm(\n",
    "            pool.map_async(solve_and_predict_area_partial, list_tuples).get(),\n",
    "            total=len(list_tuples),\n",
    "        ):\n",
    "            if result_area is not None:\n",
    "                (\n",
    "                    df_parameters_area,\n",
    "                    df_predictions_since_today_area,\n",
    "                    df_predictions_since_100_area,\n",
    "                    output,\n",
    "                ) = result_area\n",
    "                obj_value = obj_value + output.fun\n",
    "                # Then we add it to the list of df to be concatenated to update the tracking df\n",
    "                list_df_global_parameters.append(df_parameters_area)\n",
    "                list_df_global_predictions_since_today.append(df_predictions_since_today_area)\n",
    "                list_df_global_predictions_since_100_cases.append(df_predictions_since_100_area)\n",
    "            else:\n",
    "                continue\n",
    "        logging.info(\"Finished the Multiprocessing for all areas\")\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # Appending parameters, aggregations per country, per continent, and for the world\n",
    "    # for predictions today & since 100\n",
    "    today_date_str = \"\".join(str(datetime.now().date()).split(\"-\"))\n",
    "    df_global_parameters = pd.concat(list_df_global_parameters).sort_values(\n",
    "        [\"Country\", \"Province\"]\n",
    "    ).reset_index(drop=True)\n",
    "    df_global_predictions_since_today = pd.concat(list_df_global_predictions_since_today)\n",
    "    df_global_predictions_since_today = DELPHIAggregations.append_all_aggregations(\n",
    "        df_global_predictions_since_today\n",
    "    )\n",
    "    df_global_predictions_since_100_cases = pd.concat(list_df_global_predictions_since_100_cases)\n",
    "    if GET_CONFIDENCE_INTERVALS:\n",
    "        df_global_predictions_since_today, df_global_predictions_since_100_cases = DELPHIAggregations.append_all_aggregations_cf(\n",
    "            df_global_predictions_since_100_cases,\n",
    "            past_prediction_file=PATH_TO_FOLDER_DANGER_MAP + f\"predicted/Global_V4_{past_prediction_date}.csv\",\n",
    "            past_prediction_date=str(pd.to_datetime(past_prediction_date).date())\n",
    "        )\n",
    "    else:\n",
    "        df_global_predictions_since_100_cases = DELPHIAggregations.append_all_aggregations(\n",
    "            df_global_predictions_since_100_cases\n",
    "        )\n",
    "\n",
    "    logger = logging.getLogger(\"V4Logger\")\n",
    "    delphi_data_saver = DELPHIDataSaver(\n",
    "        path_to_folder_danger_map=PATH_TO_FOLDER_DANGER_MAP,\n",
    "        path_to_website_predicted=PATH_TO_WEBSITE_PREDICTED,\n",
    "        df_global_parameters=df_global_parameters,\n",
    "        df_global_predictions_since_today=df_global_predictions_since_today,\n",
    "        df_global_predictions_since_100_cases=df_global_predictions_since_100_cases,\n",
    "        logger=logger\n",
    "    )\n",
    "    delphi_data_saver.save_all_datasets(optimizer=OPTIMIZER, save_since_100_cases=SAVE_SINCE100_CASES, website=SAVE_TO_WEBSITE)\n",
    "    logging.info(\n",
    "        f\"Exported all 3 datasets to website & danger_map repositories, \"\n",
    "        + f\"total runtime was {round((time.time() - time_beginning)/60, 2)} minutes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
